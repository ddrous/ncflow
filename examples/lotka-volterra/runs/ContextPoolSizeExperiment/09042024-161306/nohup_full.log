
############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
No training. Loading data and results from: ./
WARNING: You did not provide a dataloader id. A new one has been generated: 091319
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 091319
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 308240 


WARNING: No key provided for the context initialization. Initializing at 0.

No training, loading model and results from ./ folder ...

WARNING: You did not provide a dataloader id. A new one has been generated: 091320
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 5.0034e-05


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./adapt/
 Seed: 6078


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt_test
 Savepath: ./adapt/
 Seed: 6078

WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.

==================== Sequential adaptation ====================


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.02511568
    Epoch:     1     LossContext: 0.01527279
    Epoch:     2     LossContext: 0.00751312
    Epoch:     3     LossContext: 0.00233107
    Epoch:   100     LossContext: 0.00002139
    Epoch:   200     LossContext: 0.00002022
    Epoch:   300     LossContext: 0.00001948
    Epoch:   400     LossContext: 0.00001876
    Epoch:   500     LossContext: 0.00001809
    Epoch:   600     LossContext: 0.00001776
    Epoch:   700     LossContext: 0.00001744
    Epoch:   800     LossContext: 0.00001712
    Epoch:   900     LossContext: 0.00001680
    Epoch:  1000     LossContext: 0.00001650
    Epoch:  1100     LossContext: 0.00001635
    Epoch:  1200     LossContext: 0.00001620
    Epoch:  1300     LossContext: 0.00001605
    Epoch:  1400     LossContext: 0.00001590
    Epoch:  1499     LossContext: 0.00001575

Gradient descent adaptation time: 0 hours 0 mins 34 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.10710991
    Epoch:     1     LossContext: 0.07890392
    Epoch:     2     LossContext: 0.05177121
    Epoch:     3     LossContext: 0.02813647
    Epoch:   100     LossContext: 0.00005351
    Epoch:   200     LossContext: 0.00004999
    Epoch:   300     LossContext: 0.00005409
    Epoch:   400     LossContext: 0.00005447
    Epoch:   500     LossContext: 0.00004855
    Epoch:   600     LossContext: 0.00004842
    Epoch:   700     LossContext: 0.00004635
    Epoch:   800     LossContext: 0.00004938
    Epoch:   900     LossContext: 0.00004734
    Epoch:  1000     LossContext: 0.00004917
    Epoch:  1100     LossContext: 0.00004810
    Epoch:  1200     LossContext: 0.00005004
    Epoch:  1300     LossContext: 0.00004790
    Epoch:  1400     LossContext: 0.00005348
    Epoch:  1499     LossContext: 0.00004546

Gradient descent adaptation time: 0 hours 0 mins 26 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.05347499
    Epoch:     1     LossContext: 0.03491210
    Epoch:     2     LossContext: 0.02167972
    Epoch:     3     LossContext: 0.01265834
    Epoch:   100     LossContext: 0.00004476
    Epoch:   200     LossContext: 0.00004131
    Epoch:   300     LossContext: 0.00003976
    Epoch:   400     LossContext: 0.00003865
    Epoch:   500     LossContext: 0.00003773
    Epoch:   600     LossContext: 0.00003761
    Epoch:   700     LossContext: 0.00003666
    Epoch:   800     LossContext: 0.00003641
    Epoch:   900     LossContext: 0.00003584
    Epoch:  1000     LossContext: 0.00003532
    Epoch:  1100     LossContext: 0.00003501
    Epoch:  1200     LossContext: 0.00003500
    Epoch:  1300     LossContext: 0.00003451
    Epoch:  1400     LossContext: 0.00003438
    Epoch:  1499     LossContext: 0.00003425

Gradient descent adaptation time: 0 hours 0 mins 27 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.02518685
    Epoch:     1     LossContext: 0.01440709
    Epoch:     2     LossContext: 0.00838274
    Epoch:     3     LossContext: 0.00551613
    Epoch:   100     LossContext: 0.00003687
    Epoch:   200     LossContext: 0.00003610
    Epoch:   300     LossContext: 0.00003557
    Epoch:   400     LossContext: 0.00003513
    Epoch:   500     LossContext: 0.00003470
    Epoch:   600     LossContext: 0.00003451
    Epoch:   700     LossContext: 0.00003435
    Epoch:   800     LossContext: 0.00003417
    Epoch:   900     LossContext: 0.00003400
    Epoch:  1000     LossContext: 0.00003380
    Epoch:  1100     LossContext: 0.00003374
    Epoch:  1200     LossContext: 0.00003365
    Epoch:  1300     LossContext: 0.00003358
    Epoch:  1400     LossContext: 0.00003348
    Epoch:  1499     LossContext: 0.00003338

Gradient descent adaptation time: 0 hours 0 mins 29 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 6.39028e-05


==================== Batch adaptation ====================

WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (4, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.05285278
    Epoch:     1     LossContext: 0.03596084
    Epoch:     2     LossContext: 0.02251782
    Epoch:     3     LossContext: 0.01236371
    Epoch:   100     LossContext: 0.00004354
    Epoch:   200     LossContext: 0.00006758
    Epoch:   300     LossContext: 0.00004821
    Epoch:   400     LossContext: 0.00005188
    Epoch:   500     LossContext: 0.00005142
    Epoch:   600     LossContext: 0.00004103
    Epoch:   700     LossContext: 0.00004049
    Epoch:   800     LossContext: 0.00004633
    Epoch:   900     LossContext: 0.00004255
    Epoch:  1000     LossContext: 0.00005009
    Epoch:  1100     LossContext: 0.00004412
    Epoch:  1200     LossContext: 0.00004055
    Epoch:  1300     LossContext: 0.00003844
    Epoch:  1400     LossContext: 0.00004005
    Epoch:  1499     LossContext: 0.00003831

Total gradient descent adaptation time: 0 hours 1 mins 8 secs
Environment weights at the end of the adaptation: [0.25 0.25 0.25 0.25]

Saving adaptation parameters into ./adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 7.0070906e-05


 ================================== ||||| DONE ||||| ==================================  

