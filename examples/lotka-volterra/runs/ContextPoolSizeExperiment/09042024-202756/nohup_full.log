
############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
No training. Loading data and results from: ./
WARNING: You did not provide a dataloader id. A new one has been generated: 093922
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 093922
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 308240 


WARNING: No key provided for the context initialization. Initializing at 0.

No training, loading model and results from ./ folder ...

WARNING: You did not provide a dataloader id. A new one has been generated: 093923
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 4.9214803e-05


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./adapt/
 Seed: 6078


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt_test
 Savepath: ./adapt/
 Seed: 6078

WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.

==================== Sequential adaptation ====================


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.02186902
    Epoch:     1     LossContext: 0.00958919
    Epoch:     2     LossContext: 0.00187158
    Epoch:     3     LossContext: 0.00059782
    Epoch:   100     LossContext: 0.00001639
    Epoch:   200     LossContext: 0.00001598
    Epoch:   300     LossContext: 0.00001561
    Epoch:   400     LossContext: 0.00001526
    Epoch:   500     LossContext: 0.00001494
    Epoch:   600     LossContext: 0.00001480
    Epoch:   700     LossContext: 0.00001468
    Epoch:   800     LossContext: 0.00001455
    Epoch:   900     LossContext: 0.00001443
    Epoch:  1000     LossContext: 0.00001432
    Epoch:  1100     LossContext: 0.00001426
    Epoch:  1200     LossContext: 0.00001420
    Epoch:  1300     LossContext: 0.00001415
    Epoch:  1400     LossContext: 0.00001409
    Epoch:  1499     LossContext: 0.00001404

Gradient descent adaptation time: 0 hours 0 mins 34 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.14153762
    Epoch:     1     LossContext: 0.10186788
    Epoch:     2     LossContext: 0.06542438
    Epoch:     3     LossContext: 0.03353461
    Epoch:   100     LossContext: 0.00005254
    Epoch:   200     LossContext: 0.00004702
    Epoch:   300     LossContext: 0.00004858
    Epoch:   400     LossContext: 0.00005296
    Epoch:   500     LossContext: 0.00004762
    Epoch:   600     LossContext: 0.00004676
    Epoch:   700     LossContext: 0.00004500
    Epoch:   800     LossContext: 0.00004718
    Epoch:   900     LossContext: 0.00004297
    Epoch:  1000     LossContext: 0.00004489
    Epoch:  1100     LossContext: 0.00005007
    Epoch:  1200     LossContext: 0.00004434
    Epoch:  1300     LossContext: 0.00004310
    Epoch:  1400     LossContext: 0.00004262
    Epoch:  1499     LossContext: 0.00004412

Gradient descent adaptation time: 0 hours 0 mins 26 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.03712648
    Epoch:     1     LossContext: 0.02110213
    Epoch:     2     LossContext: 0.01067370
    Epoch:     3     LossContext: 0.00447481
    Epoch:   100     LossContext: 0.00003218
    Epoch:   200     LossContext: 0.00003064
    Epoch:   300     LossContext: 0.00002962
    Epoch:   400     LossContext: 0.00002896
    Epoch:   500     LossContext: 0.00002859
    Epoch:   600     LossContext: 0.00002842
    Epoch:   700     LossContext: 0.00002819
    Epoch:   800     LossContext: 0.00002804
    Epoch:   900     LossContext: 0.00002752
    Epoch:  1000     LossContext: 0.00002749
    Epoch:  1100     LossContext: 0.00002740
    Epoch:  1200     LossContext: 0.00002747
    Epoch:  1300     LossContext: 0.00002739
    Epoch:  1400     LossContext: 0.00002724
    Epoch:  1499     LossContext: 0.00002729

Gradient descent adaptation time: 0 hours 0 mins 29 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.02666542
    Epoch:     1     LossContext: 0.01527019
    Epoch:     2     LossContext: 0.00827227
    Epoch:     3     LossContext: 0.00427835
    Epoch:   100     LossContext: 0.00003932
    Epoch:   200     LossContext: 0.00003665
    Epoch:   300     LossContext: 0.00003488
    Epoch:   400     LossContext: 0.00003335
    Epoch:   500     LossContext: 0.00003208
    Epoch:   600     LossContext: 0.00003151
    Epoch:   700     LossContext: 0.00003099
    Epoch:   800     LossContext: 0.00003049
    Epoch:   900     LossContext: 0.00003005
    Epoch:  1000     LossContext: 0.00002962
    Epoch:  1100     LossContext: 0.00002945
    Epoch:  1200     LossContext: 0.00002929
    Epoch:  1300     LossContext: 0.00002909
    Epoch:  1400     LossContext: 0.00002897
    Epoch:  1499     LossContext: 0.00002875

Gradient descent adaptation time: 0 hours 0 mins 28 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 6.705255e-05


==================== Batch adaptation ====================

WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (4, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.05692143
    Epoch:     1     LossContext: 0.03702382
    Epoch:     2     LossContext: 0.02164982
    Epoch:     3     LossContext: 0.01087208
    Epoch:   100     LossContext: 0.00005230
    Epoch:   200     LossContext: 0.00004837
    Epoch:   300     LossContext: 0.00004674
    Epoch:   400     LossContext: 0.00004436
    Epoch:   500     LossContext: 0.00004128
    Epoch:   600     LossContext: 0.00004906
    Epoch:   700     LossContext: 0.00004154
    Epoch:   800     LossContext: 0.00004073
    Epoch:   900     LossContext: 0.00004371
    Epoch:  1000     LossContext: 0.00005076
    Epoch:  1100     LossContext: 0.00004946
    Epoch:  1200     LossContext: 0.00003916
    Epoch:  1300     LossContext: 0.00004290
    Epoch:  1400     LossContext: 0.00003783
    Epoch:  1499     LossContext: 0.00003798

Total gradient descent adaptation time: 0 hours 1 mins 13 secs
Environment weights at the end of the adaptation: [0.25 0.25 0.25 0.25]

Saving adaptation parameters into ./adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 6.757589e-05


 ================================== ||||| DONE ||||| ==================================  

