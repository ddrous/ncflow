
############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
No training. Loading data and results from: ./
WARNING: You did not provide a dataloader id. A new one has been generated: 090206
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 090216
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 308240 


WARNING: No key provided for the context initialization. Initializing at 0.

No training, loading model and results from ./ folder ...

WARNING: You did not provide a dataloader id. A new one has been generated: 090217
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 4.7197453e-05


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./adapt/
 Seed: 6078


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt_test
 Savepath: ./adapt/
 Seed: 6078

WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.

==================== Sequential adaptation ====================


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.01916030
    Epoch:     1     LossContext: 0.00646360
    Epoch:     2     LossContext: 0.00018023
    Epoch:     3     LossContext: 0.00311941
    Epoch:   100     LossContext: 0.00001548
    Epoch:   200     LossContext: 0.00001457
    Epoch:   300     LossContext: 0.00001399
    Epoch:   400     LossContext: 0.00001350
    Epoch:   500     LossContext: 0.00001319
    Epoch:   600     LossContext: 0.00001307
    Epoch:   700     LossContext: 0.00001296
    Epoch:   800     LossContext: 0.00001286
    Epoch:   900     LossContext: 0.00001276
    Epoch:  1000     LossContext: 0.00001267
    Epoch:  1100     LossContext: 0.00001263
    Epoch:  1200     LossContext: 0.00001259
    Epoch:  1300     LossContext: 0.00001254
    Epoch:  1400     LossContext: 0.00001250
    Epoch:  1499     LossContext: 0.00001245

Gradient descent adaptation time: 0 hours 0 mins 34 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.13080747
    Epoch:     1     LossContext: 0.09091224
    Epoch:     2     LossContext: 0.05548321
    Epoch:     3     LossContext: 0.02608840
    Epoch:   100     LossContext: 0.00004612
    Epoch:   200     LossContext: 0.00004892
    Epoch:   300     LossContext: 0.00004399
    Epoch:   400     LossContext: 0.00004306
    Epoch:   500     LossContext: 0.00004813
    Epoch:   600     LossContext: 0.00004275
    Epoch:   700     LossContext: 0.00004176
    Epoch:   800     LossContext: 0.00004376
    Epoch:   900     LossContext: 0.00004574
    Epoch:  1000     LossContext: 0.00004312
    Epoch:  1100     LossContext: 0.00004322
    Epoch:  1200     LossContext: 0.00004306
    Epoch:  1300     LossContext: 0.00004245
    Epoch:  1400     LossContext: 0.00004261
    Epoch:  1499     LossContext: 0.00004106

Gradient descent adaptation time: 0 hours 0 mins 26 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.04396079
    Epoch:     1     LossContext: 0.02596580
    Epoch:     2     LossContext: 0.01398061
    Epoch:     3     LossContext: 0.00652869
    Epoch:   100     LossContext: 0.00003607
    Epoch:   200     LossContext: 0.00003322
    Epoch:   300     LossContext: 0.00003176
    Epoch:   400     LossContext: 0.00003107
    Epoch:   500     LossContext: 0.00002979
    Epoch:   600     LossContext: 0.00002948
    Epoch:   700     LossContext: 0.00002917
    Epoch:   800     LossContext: 0.00002914
    Epoch:   900     LossContext: 0.00002878
    Epoch:  1000     LossContext: 0.00002856
    Epoch:  1100     LossContext: 0.00002831
    Epoch:  1200     LossContext: 0.00002817
    Epoch:  1300     LossContext: 0.00002821
    Epoch:  1400     LossContext: 0.00002813
    Epoch:  1499     LossContext: 0.00002800

Gradient descent adaptation time: 0 hours 0 mins 29 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.02922846
    Epoch:     1     LossContext: 0.01560196
    Epoch:     2     LossContext: 0.00794122
    Epoch:     3     LossContext: 0.00386221
    Epoch:   100     LossContext: 0.00003759
    Epoch:   200     LossContext: 0.00003470
    Epoch:   300     LossContext: 0.00003324
    Epoch:   400     LossContext: 0.00003198
    Epoch:   500     LossContext: 0.00003095
    Epoch:   600     LossContext: 0.00003049
    Epoch:   700     LossContext: 0.00003007
    Epoch:   800     LossContext: 0.00002972
    Epoch:   900     LossContext: 0.00002941
    Epoch:  1000     LossContext: 0.00002914
    Epoch:  1100     LossContext: 0.00002900
    Epoch:  1200     LossContext: 0.00002883
    Epoch:  1300     LossContext: 0.00002873
    Epoch:  1400     LossContext: 0.00002863
    Epoch:  1499     LossContext: 0.00002851

Gradient descent adaptation time: 0 hours 0 mins 27 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 9.5398645e-05


==================== Batch adaptation ====================

WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (4, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.05593821
    Epoch:     1     LossContext: 0.03491934
    Epoch:     2     LossContext: 0.01956246
    Epoch:     3     LossContext: 0.01002170
    Epoch:   100     LossContext: 0.00005089
    Epoch:   200     LossContext: 0.00004494
    Epoch:   300     LossContext: 0.00004189
    Epoch:   400     LossContext: 0.00004297
    Epoch:   500     LossContext: 0.00004146
    Epoch:   600     LossContext: 0.00003932
    Epoch:   700     LossContext: 0.00003813
    Epoch:   800     LossContext: 0.00003752
    Epoch:   900     LossContext: 0.00004084
    Epoch:  1000     LossContext: 0.00003693
    Epoch:  1100     LossContext: 0.00003907
    Epoch:  1200     LossContext: 0.00003735
    Epoch:  1300     LossContext: 0.00003907
    Epoch:  1400     LossContext: 0.00003542
    Epoch:  1499     LossContext: 0.00003712

Total gradient descent adaptation time: 0 hours 1 mins 12 secs
Environment weights at the end of the adaptation: [0.25 0.25 0.25 0.25]

Saving adaptation parameters into ./adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 5.47559e-05


 ================================== ||||| DONE ||||| ==================================  

