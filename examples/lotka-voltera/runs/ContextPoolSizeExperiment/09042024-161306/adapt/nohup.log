
############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: train
 Savepath: ./09042024-152116/
 Seed: 2026


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: test
 Savepath: ./09042024-152116/
 Seed: 4052


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Run folder created successfuly: ./09042024-152116/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 152120
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 152120
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 308240 


WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed


=== Beginning training with proximal alternating minimization ... ===
    Number of examples in a batch: 4
    Maximum number of steps per inner minimization: 25
    Maximum number of outer minimizations: 1500
    Maximum total number of training steps: 37500

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Outer Step:     0      LossTrajs: 1.81636751     ContextsNorm: 0.00000000     ValIndCrit: 1.70389903
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.71e-06
        -DiffCxt:  inf
    Outer Step:     1      LossTrajs: 1.53356862     ContextsNorm: 0.00133134     ValIndCrit: 1.43725932
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.67e-06
        -DiffCxt:  5.62e-04
    Outer Step:     2      LossTrajs: 1.23032498     ContextsNorm: 0.00392259     ValIndCrit: 1.14924717
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.11e-05
        -DiffCxt:  7.08e-04
    Outer Step:     3      LossTrajs: 0.81609696     ContextsNorm: 0.00775006     ValIndCrit: 0.75058144
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.19e-05
        -DiffCxt:  3.82e-04
    Outer Step:   100      LossTrajs: 0.00342245     ContextsNorm: 0.01903399     ValIndCrit: 0.00542783
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.35e-07
        -DiffCxt:  4.22e-07
    Outer Step:   200      LossTrajs: 0.00056838     ContextsNorm: 0.02243277     ValIndCrit: 0.00095466
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.85e-08
        -DiffCxt:  1.33e-07
    Outer Step:   300      LossTrajs: 0.00015180     ContextsNorm: 0.02244991     ValIndCrit: 0.00025773
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.77e-08
        -DiffCxt:  1.98e-07
    Outer Step:   400      LossTrajs: 0.00009113     ContextsNorm: 0.02226462     ValIndCrit: 0.00015174
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.19e-07
        -DiffCxt:  1.27e-07
    Outer Step:   500      LossTrajs: 0.00006104     ContextsNorm: 0.02300437     ValIndCrit: 0.00014543
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.61e-07
        -DiffCxt:  3.07e-07
    Outer Step:   600      LossTrajs: 0.00008426     ContextsNorm: 0.02251776     ValIndCrit: 0.00011169
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.70e-07
        -DiffCxt:  2.56e-07
    Outer Step:   700      LossTrajs: 0.00005769     ContextsNorm: 0.02209646     ValIndCrit: 0.00008049
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.31e-08
        -DiffCxt:  4.55e-07
    Outer Step:   800      LossTrajs: 0.00004952     ContextsNorm: 0.02173536     ValIndCrit: 0.00006962
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.17e-08
        -DiffCxt:  3.31e-07
    Outer Step:   900      LossTrajs: 0.00007708     ContextsNorm: 0.02137241     ValIndCrit: 0.00006927
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.06e-07
        -DiffCxt:  3.40e-07
    Outer Step:  1000      LossTrajs: 0.00005137     ContextsNorm: 0.02045910     ValIndCrit: 0.00006781
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.17e-08
        -DiffCxt:  3.51e-07
    Outer Step:  1100      LossTrajs: 0.00004072     ContextsNorm: 0.01994253     ValIndCrit: 0.00007064
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.78e-07
        -DiffCxt:  6.34e-07
    Outer Step:  1200      LossTrajs: 0.00003383     ContextsNorm: 0.01968787     ValIndCrit: 0.00005699
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.41e-08
        -DiffCxt:  3.36e-07
    Outer Step:  1300      LossTrajs: 0.00004056     ContextsNorm: 0.01868158     ValIndCrit: 0.00005849
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.19e-08
        -DiffCxt:  1.39e-07
    Outer Step:  1400      LossTrajs: 0.00003901     ContextsNorm: 0.01816025     ValIndCrit: 0.00005225
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.63e-08
        -DiffCxt:  5.10e-07
    Outer Step:  1499      LossTrajs: 0.00003940     ContextsNorm: 0.01767140     ValIndCrit: 0.00007438
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.04e-07
        -DiffCxt:  4.16e-07

Total gradient descent training time: 0 hours 51 mins 21 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 161243
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 5.224747e-05


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./09042024-152116/adapt/
 Seed: 6078


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt_test
 Savepath: ./09042024-152116/adapt/
 Seed: 6078

WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.

==================== Sequential adaptation ====================


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 15
    Total number of training steps: 15

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.02327766
    Epoch:     1     LossContext: 0.01394633
    Epoch:     2     LossContext: 0.00699389
    Epoch:     3     LossContext: 0.00293882
    Epoch:    14     LossContext: 0.00098359

Gradient descent adaptation time: 0 hours 0 mins 7 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.11778260
    Epoch:     1     LossContext: 0.09682740
    Epoch:     2     LossContext: 0.07583591
    Epoch:     3     LossContext: 0.05561024
    Epoch:    14     LossContext: 0.00075876

Gradient descent adaptation time: 0 hours 0 mins 0 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.04777515
    Epoch:     1     LossContext: 0.03359828
    Epoch:     2     LossContext: 0.02310447
    Epoch:     3     LossContext: 0.01558673
    Epoch:    14     LossContext: 0.00138627

Gradient descent adaptation time: 0 hours 0 mins 0 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.02540202
    Epoch:     1     LossContext: 0.01563824
    Epoch:     2     LossContext: 0.00975475
    Epoch:     3     LossContext: 0.00649435
    Epoch:    14     LossContext: 0.00014183

Gradient descent adaptation time: 0 hours 0 mins 0 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./09042024-152116/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 0.0011486411


==================== Batch adaptation ====================

WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 15
    Total number of training steps: 15
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (4, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.05367055
    Epoch:     1     LossContext: 0.04017033
    Epoch:     2     LossContext: 0.02901018
    Epoch:     3     LossContext: 0.02031882
    Epoch:    14     LossContext: 0.00116162

Total gradient descent adaptation time: 0 hours 0 mins 6 secs
Environment weights at the end of the adaptation: [0.25 0.25 0.25 0.25]

Saving adaptation parameters into ./09042024-152116/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 0.0010941296


 ================================== ||||| DONE ||||| ==================================  


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: train
 Savepath: ./09042024-161306/
 Seed: 2026


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: test
 Savepath: ./09042024-161306/
 Seed: 4052


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Run folder created successfuly: ./09042024-161306/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 161310
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 161310
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 308240 


WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed


=== Beginning training with proximal alternating minimization ... ===
    Number of examples in a batch: 4
    Maximum number of steps per inner minimization: 25
    Maximum number of outer minimizations: 1500
    Maximum total number of training steps: 37500

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Outer Step:     0      LossTrajs: 1.81636643     ContextsNorm: 0.00000000     ValIndCrit: 1.70389879
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.71e-06
        -DiffCxt:  inf
    Outer Step:     1      LossTrajs: 1.53356576     ContextsNorm: 0.00133131     ValIndCrit: 1.43725991
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.67e-06
        -DiffCxt:  5.62e-04
    Outer Step:     2      LossTrajs: 1.23032022     ContextsNorm: 0.00392258     ValIndCrit: 1.14924872
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.11e-05
        -DiffCxt:  7.08e-04
    Outer Step:     3      LossTrajs: 0.81609875     ContextsNorm: 0.00774972     ValIndCrit: 0.75058174
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.19e-05
        -DiffCxt:  3.82e-04
    Outer Step:   100      LossTrajs: 0.00291993     ContextsNorm: 0.01678849     ValIndCrit: 0.00482500
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.76e-08
        -DiffCxt:  7.94e-08
    Outer Step:   200      LossTrajs: 0.00033110     ContextsNorm: 0.01818962     ValIndCrit: 0.00061097
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.42e-08
        -DiffCxt:  1.53e-07
    Outer Step:   300      LossTrajs: 0.00009350     ContextsNorm: 0.01790253     ValIndCrit: 0.00018790
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.69e-08
        -DiffCxt:  4.29e-07
    Outer Step:   400      LossTrajs: 0.00005981     ContextsNorm: 0.01774011     ValIndCrit: 0.00011190
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.63e-08
        -DiffCxt:  5.29e-07
    Outer Step:   500      LossTrajs: 0.00006807     ContextsNorm: 0.01785462     ValIndCrit: 0.00008488
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.14e-08
        -DiffCxt:  5.44e-07
    Outer Step:   600      LossTrajs: 0.00004178     ContextsNorm: 0.01739289     ValIndCrit: 0.00007149
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.75e-08
        -DiffCxt:  6.84e-07
    Outer Step:   700      LossTrajs: 0.00005203     ContextsNorm: 0.01678303     ValIndCrit: 0.00006314
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.64e-08
        -DiffCxt:  3.03e-07
    Outer Step:   800      LossTrajs: 0.00003322     ContextsNorm: 0.01653206     ValIndCrit: 0.00006059
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.97e-08
        -DiffCxt:  2.86e-07
    Outer Step:   900      LossTrajs: 0.00003218     ContextsNorm: 0.01617630     ValIndCrit: 0.00006689
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.22e-08
        -DiffCxt:  5.12e-07
    Outer Step:  1000      LossTrajs: 0.00003163     ContextsNorm: 0.01606894     ValIndCrit: 0.00005512
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.82e-08
        -DiffCxt:  2.18e-07
    Outer Step:  1100      LossTrajs: 0.00003385     ContextsNorm: 0.01563916     ValIndCrit: 0.00005727
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.91e-08
        -DiffCxt:  5.12e-07
    Outer Step:  1200      LossTrajs: 0.00003529     ContextsNorm: 0.01539731     ValIndCrit: 0.00005601
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.36e-08
        -DiffCxt:  3.08e-07
    Outer Step:  1300      LossTrajs: 0.00003281     ContextsNorm: 0.01532273     ValIndCrit: 0.00005003
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.11e-08
        -DiffCxt:  8.15e-07
    Outer Step:  1400      LossTrajs: 0.00002677     ContextsNorm: 0.01532568     ValIndCrit: 0.00005391
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.91e-08
        -DiffCxt:  2.42e-07
    Outer Step:  1499      LossTrajs: 0.00003759     ContextsNorm: 0.01546611     ValIndCrit: 0.00005553
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.05e-07
        -DiffCxt:  2.86e-07

Total gradient descent training time: 1 hours 13 mins 31 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 172642
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 5.0034e-05


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./09042024-161306/adapt/
 Seed: 6078


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt_test
 Savepath: ./09042024-161306/adapt/
 Seed: 6078

WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.

==================== Sequential adaptation ====================


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 15
    Total number of training steps: 15

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.02511568
    Epoch:     1     LossContext: 0.01527279
    Epoch:     2     LossContext: 0.00751312
    Epoch:     3     LossContext: 0.00233107
    Epoch:    14     LossContext: 0.00125326

Gradient descent adaptation time: 0 hours 0 mins 7 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.10710991
    Epoch:     1     LossContext: 0.07890392
    Epoch:     2     LossContext: 0.05177121
    Epoch:     3     LossContext: 0.02813647
    Epoch:    14     LossContext: 0.00531255

Gradient descent adaptation time: 0 hours 0 mins 0 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.05347499
    Epoch:     1     LossContext: 0.03491210
    Epoch:     2     LossContext: 0.02167972
    Epoch:     3     LossContext: 0.01265834
    Epoch:    14     LossContext: 0.00309000

Gradient descent adaptation time: 0 hours 0 mins 0 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.02518685
    Epoch:     1     LossContext: 0.01440709
    Epoch:     2     LossContext: 0.00838274
    Epoch:     3     LossContext: 0.00551613
    Epoch:    14     LossContext: 0.00032344

Gradient descent adaptation time: 0 hours 0 mins 0 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./09042024-161306/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 0.0036638207


==================== Batch adaptation ====================

WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 15
    Total number of training steps: 15
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (4, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.05285278
    Epoch:     1     LossContext: 0.03589608
    Epoch:     2     LossContext: 0.02248089
    Epoch:     3     LossContext: 0.01234152
    Epoch:    14     LossContext: 0.00247608

Total gradient descent adaptation time: 0 hours 0 mins 7 secs
Environment weights at the end of the adaptation: [0.25 0.25 0.25 0.25]

Saving adaptation parameters into ./09042024-161306/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 0.003807851

