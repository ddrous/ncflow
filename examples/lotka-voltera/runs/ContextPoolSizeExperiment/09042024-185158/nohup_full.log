
############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
No training. Loading data and results from: ./
WARNING: You did not provide a dataloader id. A new one has been generated: 092706
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 092706
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 308240 


WARNING: No key provided for the context initialization. Initializing at 0.

No training, loading model and results from ./ folder ...

WARNING: You did not provide a dataloader id. A new one has been generated: 092707
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 4.5672597e-05


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./adapt/
 Seed: 6078


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt_test
 Savepath: ./adapt/
 Seed: 6078

WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.

==================== Sequential adaptation ====================


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.02636820
    Epoch:     1     LossContext: 0.01247729
    Epoch:     2     LossContext: 0.00232348
    Epoch:     3     LossContext: 0.00064225
    Epoch:   100     LossContext: 0.00001796
    Epoch:   200     LossContext: 0.00001710
    Epoch:   300     LossContext: 0.00001633
    Epoch:   400     LossContext: 0.00001555
    Epoch:   500     LossContext: 0.00001488
    Epoch:   600     LossContext: 0.00001459
    Epoch:   700     LossContext: 0.00001432
    Epoch:   800     LossContext: 0.00001407
    Epoch:   900     LossContext: 0.00001383
    Epoch:  1000     LossContext: 0.00001361
    Epoch:  1100     LossContext: 0.00001350
    Epoch:  1200     LossContext: 0.00001340
    Epoch:  1300     LossContext: 0.00001330
    Epoch:  1400     LossContext: 0.00001320
    Epoch:  1499     LossContext: 0.00001311

Gradient descent adaptation time: 0 hours 0 mins 34 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.10948187
    Epoch:     1     LossContext: 0.07085209
    Epoch:     2     LossContext: 0.03625583
    Epoch:     3     LossContext: 0.01052806
    Epoch:   100     LossContext: 0.00004279
    Epoch:   200     LossContext: 0.00004125
    Epoch:   300     LossContext: 0.00004049
    Epoch:   400     LossContext: 0.00004309
    Epoch:   500     LossContext: 0.00004166
    Epoch:   600     LossContext: 0.00003927
    Epoch:   700     LossContext: 0.00003696
    Epoch:   800     LossContext: 0.00003800
    Epoch:   900     LossContext: 0.00004139
    Epoch:  1000     LossContext: 0.00003910
    Epoch:  1100     LossContext: 0.00004023
    Epoch:  1200     LossContext: 0.00004107
    Epoch:  1300     LossContext: 0.00003753
    Epoch:  1400     LossContext: 0.00004010
    Epoch:  1499     LossContext: 0.00003588

Gradient descent adaptation time: 0 hours 0 mins 26 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.05109036
    Epoch:     1     LossContext: 0.02903592
    Epoch:     2     LossContext: 0.01490503
    Epoch:     3     LossContext: 0.00657012
    Epoch:   100     LossContext: 0.00003474
    Epoch:   200     LossContext: 0.00003218
    Epoch:   300     LossContext: 0.00003098
    Epoch:   400     LossContext: 0.00002997
    Epoch:   500     LossContext: 0.00002877
    Epoch:   600     LossContext: 0.00002869
    Epoch:   700     LossContext: 0.00002812
    Epoch:   800     LossContext: 0.00002757
    Epoch:   900     LossContext: 0.00002722
    Epoch:  1000     LossContext: 0.00002704
    Epoch:  1100     LossContext: 0.00002695
    Epoch:  1200     LossContext: 0.00002661
    Epoch:  1300     LossContext: 0.00002642
    Epoch:  1400     LossContext: 0.00002636
    Epoch:  1499     LossContext: 0.00002619

Gradient descent adaptation time: 0 hours 0 mins 28 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.02351481
    Epoch:     1     LossContext: 0.01174576
    Epoch:     2     LossContext: 0.00624673
    Epoch:     3     LossContext: 0.00369690
    Epoch:   100     LossContext: 0.00003312
    Epoch:   200     LossContext: 0.00002983
    Epoch:   300     LossContext: 0.00002903
    Epoch:   400     LossContext: 0.00002841
    Epoch:   500     LossContext: 0.00002788
    Epoch:   600     LossContext: 0.00002762
    Epoch:   700     LossContext: 0.00002741
    Epoch:   800     LossContext: 0.00002716
    Epoch:   900     LossContext: 0.00002696
    Epoch:  1000     LossContext: 0.00002677
    Epoch:  1100     LossContext: 0.00002668
    Epoch:  1200     LossContext: 0.00002658
    Epoch:  1300     LossContext: 0.00002651
    Epoch:  1400     LossContext: 0.00002643
    Epoch:  1499     LossContext: 0.00002634

Gradient descent adaptation time: 0 hours 0 mins 27 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 5.9366954e-05


==================== Batch adaptation ====================

WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (4, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.05271857
    Epoch:     1     LossContext: 0.03110329
    Epoch:     2     LossContext: 0.01501834
    Epoch:     3     LossContext: 0.00542467
    Epoch:   100     LossContext: 0.00004106
    Epoch:   200     LossContext: 0.00004035
    Epoch:   300     LossContext: 0.00004229
    Epoch:   400     LossContext: 0.00003535
    Epoch:   500     LossContext: 0.00003446
    Epoch:   600     LossContext: 0.00003439
    Epoch:   700     LossContext: 0.00003456
    Epoch:   800     LossContext: 0.00003504
    Epoch:   900     LossContext: 0.00003234
    Epoch:  1000     LossContext: 0.00003254
    Epoch:  1100     LossContext: 0.00003226
    Epoch:  1200     LossContext: 0.00003255
    Epoch:  1300     LossContext: 0.00003085
    Epoch:  1400     LossContext: 0.00003179
    Epoch:  1499     LossContext: 0.00003202

Total gradient descent adaptation time: 0 hours 1 mins 12 secs
Environment weights at the end of the adaptation: [0.25 0.25 0.25 0.25]

Saving adaptation parameters into ./adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 5.397191e-05


 ================================== ||||| DONE ||||| ==================================  

