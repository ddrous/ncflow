Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./runs/12022024-223202/adapt/
 Seed: 3543


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
No training. Loading data and results from: ./runs/12022024-223202/
WARNING: You did not provide a dataloader id. A new one has been generated: 115629
WARNING: Note that this id used to distuinguish between adaptations to different environments.
Dataset shape: (9, 4, 20, 2)


Total number of parameters in the model: 25890 


WARNING: No key provided for the context initialization. Initializing at 0.

No training, loading model and results from ./runs/12022024-223202/ folder ...

No training. Loading and finetuning into: ./runs/12022024-223202/finetune_115629/


=== Beginning training ... ===
    Number of examples in a batch: 4
    Number of train steps per epoch: 1
    Number of training epochs: 25000
    Total number of training steps: 25000

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Epoch:     0      LossTrajs: 0.00001612     ContextsNorm: 0.13101359
    Epoch:     1      LossTrajs: 0.00001895     ContextsNorm: 0.13101359
    Epoch:     2      LossTrajs: 0.00001795     ContextsNorm: 0.13101357
    Epoch:     3      LossTrajs: 0.00002022     ContextsNorm: 0.13101356
    Epoch:  1000      LossTrajs: 0.00001606     ContextsNorm: 0.13101317
    Epoch:  2000      LossTrajs: 0.00001659     ContextsNorm: 0.13101286
    Epoch:  3000      LossTrajs: 0.00001625     ContextsNorm: 0.13101228
    Epoch:  4000      LossTrajs: 0.00001865     ContextsNorm: 0.13101232
    Epoch:  5000      LossTrajs: 0.00001569     ContextsNorm: 0.13101307
    Epoch:  6000      LossTrajs: 0.00001502     ContextsNorm: 0.13101299
    Epoch:  7000      LossTrajs: 0.00001650     ContextsNorm: 0.13101363
    Epoch:  8000      LossTrajs: 0.00001773     ContextsNorm: 0.13101374
    Epoch:  9000      LossTrajs: 0.00001665     ContextsNorm: 0.13101394
    Epoch: 10000      LossTrajs: 0.00001785     ContextsNorm: 0.13101318
    Epoch: 11000      LossTrajs: 0.00001559     ContextsNorm: 0.13101363
    Epoch: 12000      LossTrajs: 0.00001622     ContextsNorm: 0.13101345
    Epoch: 13000      LossTrajs: 0.00001601     ContextsNorm: 0.13101329
    Epoch: 14000      LossTrajs: 0.00001697     ContextsNorm: 0.13101381
    Epoch: 15000      LossTrajs: 0.00004177     ContextsNorm: 0.13101348
    Epoch: 16000      LossTrajs: 0.00001634     ContextsNorm: 0.13101332
    Epoch: 17000      LossTrajs: 0.00002024     ContextsNorm: 0.13101310
    Epoch: 18000      LossTrajs: 0.00001878     ContextsNorm: 0.13101298
    Epoch: 19000      LossTrajs: 0.00001726     ContextsNorm: 0.13101308
    Epoch: 20000      LossTrajs: 0.00002017     ContextsNorm: 0.13101271
    Epoch: 21000      LossTrajs: 0.00001748     ContextsNorm: 0.13101265
    Epoch: 22000      LossTrajs: 0.00001689     ContextsNorm: 0.13101278
    Epoch: 23000      LossTrajs: 0.00002152     ContextsNorm: 0.13101310
    Epoch: 24000      LossTrajs: 0.00001829     ContextsNorm: 0.13101326
    Epoch: 24999      LossTrajs: 0.00001624     ContextsNorm: 0.13101359

Total gradient descent training time: 0 hours 24 mins 40 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 122114
WARNING: Note that this id used to distuinguish between adaptations to different environments.
Dataset shape: (9, 32, 20, 2)
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 5.3568594e-05

==  Begining in-domain visualisation ... ==
    Environment id: 3
    Trajectory id: 27
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/12022024-223202/finetune_115629/results_in_domain.png
Dataset shape: (4, 1, 20, 2)
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 600
    Total number of training steps: 600

Compiling function "train_step" for context ...
Shapes of elements in a batch: (4, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.04264218
    Epoch:     1     LossContext: 0.05242208
    Epoch:     2     LossContext: 0.05249420
    Epoch:     3     LossContext: 0.05172911
    Epoch:   599     LossContext: 0.02104962
