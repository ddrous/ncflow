Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./runs/15022024-133847/adapt/
 Seed: 6078


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
No training. Loading data and results from: ./runs/15022024-133847/
WARNING: You did not provide a dataloader id. A new one has been generated: 215645
WARNING: Note that this id used to distuinguish between adaptations to different environments.


Total number of parameters in the model: 319506 


WARNING: No key provided for the context initialization. Initializing at 0.

No training, loading model and results from ./runs/15022024-133847/ folder ...

No training. Loading anctx_sd finetuning into: ./runs/15022024-133847/finetune_215645/


=== Beginning training ... ===
    Number of examples in a batch: 4
    Number of train steps per epoch: 1
    Number of training epochs: 24000
    Total number of training steps: 24000

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Epoch:     0      LossTrajs: 0.00000922     ContextsNorm: 0.16405478
    Epoch:     1      LossTrajs: 0.00000880     ContextsNorm: 0.16405478
    Epoch:     2      LossTrajs: 0.00000950     ContextsNorm: 0.16405478
    Epoch:     3      LossTrajs: 0.00000840     ContextsNorm: 0.16405475
    Epoch:  1000      LossTrajs: 0.00000996     ContextsNorm: 0.16405602
    Epoch:  2000      LossTrajs: 0.00001126     ContextsNorm: 0.16405590
    Epoch:  3000      LossTrajs: 0.00000844     ContextsNorm: 0.16405612
    Epoch:  4000      LossTrajs: 0.00001324     ContextsNorm: 0.16405629
    Epoch:  5000      LossTrajs: 0.00000891     ContextsNorm: 0.16405630
    Epoch:  6000      LossTrajs: 0.00000998     ContextsNorm: 0.16405666
    Epoch:  7000      LossTrajs: 0.00001731     ContextsNorm: 0.16405672
    Epoch:  8000      LossTrajs: 0.00001760     ContextsNorm: 0.16405693
    Epoch:  9000      LossTrajs: 0.00000867     ContextsNorm: 0.16405725
    Epoch: 10000      LossTrajs: 0.00000838     ContextsNorm: 0.16405736
    Epoch: 11000      LossTrajs: 0.00000863     ContextsNorm: 0.16405699
    Epoch: 12000      LossTrajs: 0.00000799     ContextsNorm: 0.16405673
    Epoch: 13000      LossTrajs: 0.00000863     ContextsNorm: 0.16405679
    Epoch: 14000      LossTrajs: 0.00000849     ContextsNorm: 0.16405691
    Epoch: 15000      LossTrajs: 0.00000907     ContextsNorm: 0.16405728
    Epoch: 16000      LossTrajs: 0.00000766     ContextsNorm: 0.16405697
    Epoch: 17000      LossTrajs: 0.00000958     ContextsNorm: 0.16405722
    Epoch: 18000      LossTrajs: 0.00000806     ContextsNorm: 0.16405706
    Epoch: 19000      LossTrajs: 0.00000845     ContextsNorm: 0.16405697
    Epoch: 20000      LossTrajs: 0.00001060     ContextsNorm: 0.16405718
    Epoch: 21000      LossTrajs: 0.00000866     ContextsNorm: 0.16405725
    Epoch: 22000      LossTrajs: 0.00000822     ContextsNorm: 0.16405690
    Epoch: 23000      LossTrajs: 0.00000880     ContextsNorm: 0.16405672
    Epoch: 23999      LossTrajs: 0.00000810     ContextsNorm: 0.16405697

Total gradient descent training time: 1 hours 0 mins 54 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 225745
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 6.203541e-05

==  Begining in-domain visualisation ... ==
    Environment id: 4
    Trajectory id: 15
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/15022024-133847/finetune_215645/results_in_domain.png
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 8000
    Total number of training steps: 8000
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (4, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 31.47297096
    Epoch:     1     LossContext: 30.47548676
    Epoch:     2     LossContext: 28.42967987
    Epoch:     3     LossContext: 26.45638657
    Epoch:  1000     LossContext: 0.01961431
    Epoch:  2000     LossContext: 0.01256344
    Epoch:  3000     LossContext: 0.00725055
    Epoch:  4000     LossContext: 0.00465204
    Epoch:  5000     LossContext: 0.00192386
    Epoch:  6000     LossContext: 0.00081179
    Epoch:  7000     LossContext: 0.00045173
    Epoch:  7999     LossContext: 0.00019164

Total gradient descent adaptation time: 0 hours 3 mins 53 secs
Environment weights at the end of the adaptation: [0.11115006 0.27309203 0.33072641 0.28503144]

Saving adaptation parameters into ./runs/15022024-133847/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 0.00023988122
