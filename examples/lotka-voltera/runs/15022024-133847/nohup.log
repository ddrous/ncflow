Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: train
 Savepath: ./runs/15022024-133847/
 Seed: 2026

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: test
 Savepath: ./runs/15022024-133847/
 Seed: 4052

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./runs/15022024-133847/adapt/
 Seed: 6078


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Data folder created successfuly: ./runs/15022024-133847/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 133849
WARNING: Note that this id used to distuinguish between adaptations to different environments.


Total number of parameters in the model: 319506 


WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning training ... ===
    Number of examples in a batch: 4
    Number of train steps per epoch: 1
    Number of training epochs: 72000
    Total number of training steps: 72000

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Epoch:     0      LossTrajs: 15.27994728     ContextsNorm: 0.00000000
    Epoch:     1      LossTrajs: 9.44130898     ContextsNorm: 0.00098262
    Epoch:     2      LossTrajs: 5.22011900     ContextsNorm: 0.00194847
    Epoch:     3      LossTrajs: 2.49045634     ContextsNorm: 0.00296064
    Epoch:  1000      LossTrajs: 0.00353450     ContextsNorm: 0.09619335
    Epoch:  2000      LossTrajs: 0.00053514     ContextsNorm: 0.09187843
    Epoch:  3000      LossTrajs: 0.00070998     ContextsNorm: 0.09005751
    Epoch:  4000      LossTrajs: 0.00032432     ContextsNorm: 0.08885302
    Epoch:  5000      LossTrajs: 0.00021070     ContextsNorm: 0.08715699
    Epoch:  6000      LossTrajs: 0.00015432     ContextsNorm: 0.08459190
    Epoch:  7000      LossTrajs: 0.00008765     ContextsNorm: 0.08308978
    Epoch:  8000      LossTrajs: 0.00061289     ContextsNorm: 0.08053003
    Epoch:  9000      LossTrajs: 0.00011174     ContextsNorm: 0.07713800
    Epoch: 10000      LossTrajs: 0.00038829     ContextsNorm: 0.07655071
    Epoch: 11000      LossTrajs: 0.00031137     ContextsNorm: 0.07478491
    Epoch: 12000      LossTrajs: 0.00013604     ContextsNorm: 0.07602439
    Epoch: 13000      LossTrajs: 0.00036939     ContextsNorm: 0.07273366
    Epoch: 14000      LossTrajs: 0.00004953     ContextsNorm: 0.07244734
    Epoch: 15000      LossTrajs: 0.00031045     ContextsNorm: 0.07146128
    Epoch: 16000      LossTrajs: 0.00068421     ContextsNorm: 0.07159667
    Epoch: 17000      LossTrajs: 0.00032016     ContextsNorm: 0.07153404
    Epoch: 18000      LossTrajs: 0.00039751     ContextsNorm: 0.07465392
    Epoch: 19000      LossTrajs: 0.00032686     ContextsNorm: 0.07146038
    Epoch: 20000      LossTrajs: 0.00013453     ContextsNorm: 0.07003424
    Epoch: 21000      LossTrajs: 0.00020684     ContextsNorm: 0.07081416
    Epoch: 22000      LossTrajs: 0.00022555     ContextsNorm: 0.06995538
    Epoch: 23000      LossTrajs: 0.00031753     ContextsNorm: 0.06885825
    Epoch: 24000      LossTrajs: 0.00015827     ContextsNorm: 0.11975172
    Epoch: 25000      LossTrajs: 0.00004657     ContextsNorm: 0.11970549
    Epoch: 26000      LossTrajs: 0.00004982     ContextsNorm: 0.11965854
    Epoch: 27000      LossTrajs: 0.00003537     ContextsNorm: 0.11960582
    Epoch: 28000      LossTrajs: 0.00002469     ContextsNorm: 0.11954161
    Epoch: 29000      LossTrajs: 0.00002210     ContextsNorm: 0.11945355
    Epoch: 30000      LossTrajs: 0.00002238     ContextsNorm: 0.11928810
    Epoch: 31000      LossTrajs: 0.00001841     ContextsNorm: 0.11917645
    Epoch: 32000      LossTrajs: 0.00001338     ContextsNorm: 0.11893087
    Epoch: 33000      LossTrajs: 0.00001950     ContextsNorm: 0.11837208
    Epoch: 34000      LossTrajs: 0.00003053     ContextsNorm: 0.13989162
    Epoch: 35000      LossTrajs: 0.00002080     ContextsNorm: 0.14013515
    Epoch: 36000      LossTrajs: 0.00047680     ContextsNorm: 0.14405012
    Epoch: 37000      LossTrajs: 0.00002050     ContextsNorm: 0.14928903
    Epoch: 38000      LossTrajs: 0.00001730     ContextsNorm: 0.14920001
    Epoch: 39000      LossTrajs: 0.00032976     ContextsNorm: 0.15600845
    Epoch: 40000      LossTrajs: 0.00001748     ContextsNorm: 0.15685187
    Epoch: 41000      LossTrajs: 0.00002170     ContextsNorm: 0.15650716
    Epoch: 42000      LossTrajs: 0.00001766     ContextsNorm: 0.15086365
    Epoch: 43000      LossTrajs: 0.00001737     ContextsNorm: 0.14262021
    Epoch: 44000      LossTrajs: 0.00011981     ContextsNorm: 0.16047518
    Epoch: 45000      LossTrajs: 0.00002959     ContextsNorm: 0.16046761
    Epoch: 46000      LossTrajs: 0.00001334     ContextsNorm: 0.16038646
    Epoch: 47000      LossTrajs: 0.00001244     ContextsNorm: 0.15937606
    Epoch: 48000      LossTrajs: 0.00002168     ContextsNorm: 0.16391884
    Epoch: 49000      LossTrajs: 0.00001902     ContextsNorm: 0.16391616
    Epoch: 50000      LossTrajs: 0.00001436     ContextsNorm: 0.16391730
    Epoch: 51000      LossTrajs: 0.00001518     ContextsNorm: 0.16391142
    Epoch: 52000      LossTrajs: 0.00001212     ContextsNorm: 0.16389282
    Epoch: 53000      LossTrajs: 0.00001288     ContextsNorm: 0.16389804
    Epoch: 54000      LossTrajs: 0.00001214     ContextsNorm: 0.16387863
    Epoch: 55000      LossTrajs: 0.00001299     ContextsNorm: 0.16383411
    Epoch: 56000      LossTrajs: 0.00001098     ContextsNorm: 0.16380250
    Epoch: 57000      LossTrajs: 0.00001100     ContextsNorm: 0.16381343
    Epoch: 58000      LossTrajs: 0.00001089     ContextsNorm: 0.16378599
    Epoch: 59000      LossTrajs: 0.00000991     ContextsNorm: 0.16387177
    Epoch: 60000      LossTrajs: 0.00001125     ContextsNorm: 0.16392589
    Epoch: 61000      LossTrajs: 0.00000857     ContextsNorm: 0.16389950
    Epoch: 62000      LossTrajs: 0.00001105     ContextsNorm: 0.16390724
    Epoch: 63000      LossTrajs: 0.00001031     ContextsNorm: 0.16388175
    Epoch: 64000      LossTrajs: 0.00000946     ContextsNorm: 0.16395856
    Epoch: 65000      LossTrajs: 0.00000906     ContextsNorm: 0.16396393
    Epoch: 66000      LossTrajs: 0.00000921     ContextsNorm: 0.16399039
    Epoch: 67000      LossTrajs: 0.00000964     ContextsNorm: 0.16405101
    Epoch: 68000      LossTrajs: 0.00000891     ContextsNorm: 0.16403770
    Epoch: 69000      LossTrajs: 0.00000967     ContextsNorm: 0.16406932
    Epoch: 70000      LossTrajs: 0.00000929     ContextsNorm: 0.16407162
    Epoch: 71000      LossTrajs: 0.00000843     ContextsNorm: 0.16407385
    Epoch: 71999      LossTrajs: 0.00000884     ContextsNorm: 0.16405465

Total gradient descent training time: 2 hours 57 mins 54 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 163655
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 6.1874736e-05

==  Begining in-domain visualisation ... ==
    Environment id: 1
    Trajectory id: 31
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/15022024-133847/results_in_domain.png
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 8000
    Total number of training steps: 8000
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (4, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 31.33651161
    Epoch:     1     LossContext: 30.43384171
    Epoch:     2     LossContext: 28.50204277
    Epoch:     3     LossContext: 26.58319283
    Epoch:  1000     LossContext: 0.02001849
    Epoch:  2000     LossContext: 0.01301582
    Epoch:  3000     LossContext: 0.00769586
    Epoch:  4000     LossContext: 0.00470556
    Epoch:  5000     LossContext: 0.00225763
    Epoch:  6000     LossContext: 0.00078187
    Epoch:  7000     LossContext: 0.00040965
    Epoch:  7999     LossContext: 0.00015874

Total gradient descent adaptation time: 0 hours 3 mins 53 secs
Environment weights at the end of the adaptation: [0.13177463 0.27123117 0.33327153 0.26372266]

Saving adaptation parameters into ./runs/15022024-133847/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 0.00018877487

