cp: 'main_T2_nearest.py' and './main_T2_nearest.py' are the same file
cp: 'dataset.py' and './dataset.py' are the same file

############# Neural Context Flow #############

Jax version: 0.4.28
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: train
 Savepath: ./
 Seed: 2026


############# Neural Context Flow #############

Jax version: 0.4.28
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: test
 Savepath: ./
 Seed: 4052


############# Neural Context Flow #############

Jax version: 0.4.28
Available devices: [cuda(id=0)]
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 135956
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 135956
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 308240 


WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed


=== Beginning training with proximal alternating minimization ... ===
    Number of examples in a batch: 4
    Maximum number of steps per inner minimization: 25
    Maximum number of outer minimizations: 500
    Maximum total number of training steps: 12500

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Outer Step:     0      LossTrajs: 1.81051505     ContextsNorm: 0.00000000     ValIndCrit: 1.60136437
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.70e-04
        -DiffCxt:  inf
    Outer Step:     1      LossTrajs: 0.29423621     ContextsNorm: 0.00415340     ValIndCrit: 0.32763696
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.71e-04
        -DiffCxt:  6.97e-04
    Outer Step:     2      LossTrajs: 0.22721601     ContextsNorm: 0.01074863     ValIndCrit: 0.24938825
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.74e-04
        -DiffCxt:  3.69e-04
    Outer Step:     3      LossTrajs: 0.15535925     ContextsNorm: 0.01019199     ValIndCrit: 0.16826114
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.09e-05
        -DiffCxt:  1.05e-04
    Outer Step:    10      LossTrajs: 0.00701442     ContextsNorm: 0.01204705     ValIndCrit: 0.01097145
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.82e-06
        -DiffCxt:  4.04e-06
    Outer Step:    20      LossTrajs: 0.00144108     ContextsNorm: 0.01299713     ValIndCrit: 0.00367082
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.52e-07
        -DiffCxt:  3.96e-07
    Outer Step:    30      LossTrajs: 0.00060705     ContextsNorm: 0.01302107     ValIndCrit: 0.00171400
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.65e-08
        -DiffCxt:  3.50e-07
    Outer Step:    40      LossTrajs: 0.00025952     ContextsNorm: 0.01311505     ValIndCrit: 0.00081219
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.48e-08
        -DiffCxt:  1.17e-07
    Outer Step:    50      LossTrajs: 0.00016752     ContextsNorm: 0.01335436     ValIndCrit: 0.00046287
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.45e-07
        -DiffCxt:  4.11e-06
    Outer Step:    60      LossTrajs: 0.00009681     ContextsNorm: 0.01325441     ValIndCrit: 0.00028162
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.42e-07
        -DiffCxt:  2.28e-06
    Outer Step:    70      LossTrajs: 0.00009125     ContextsNorm: 0.01337268     ValIndCrit: 0.00027647
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.60e-07
        -DiffCxt:  2.75e-06
    Outer Step:    80      LossTrajs: 0.00005816     ContextsNorm: 0.01329317     ValIndCrit: 0.00023081
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.50e-07
        -DiffCxt:  4.38e-06
    Outer Step:    90      LossTrajs: 0.00007349     ContextsNorm: 0.01336760     ValIndCrit: 0.00021277
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.45e-07
        -DiffCxt:  5.35e-06
    Outer Step:   100      LossTrajs: 0.00004340     ContextsNorm: 0.01336295     ValIndCrit: 0.00019381
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.53e-07
        -DiffCxt:  5.79e-07
    Outer Step:   110      LossTrajs: 0.00008630     ContextsNorm: 0.01400269     ValIndCrit: 0.00022248
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.68e-06
        -DiffCxt:  6.63e-06
    Outer Step:   120      LossTrajs: 0.00006209     ContextsNorm: 0.01399496     ValIndCrit: 0.00019173
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.17e-07
        -DiffCxt:  3.25e-06
    Outer Step:   130      LossTrajs: 0.00009572     ContextsNorm: 0.01410263     ValIndCrit: 0.00021688
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.21e-06
        -DiffCxt:  1.86e-05
    Outer Step:   140      LossTrajs: 0.00005523     ContextsNorm: 0.01369076     ValIndCrit: 0.00016354
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.39e-07
        -DiffCxt:  4.11e-06
    Outer Step:   150      LossTrajs: 0.00002948     ContextsNorm: 0.01412231     ValIndCrit: 0.00016770
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.06e-07
        -DiffCxt:  1.23e-06
    Outer Step:   160      LossTrajs: 0.00002762     ContextsNorm: 0.01359903     ValIndCrit: 0.00013724
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.86e-08
        -DiffCxt:  1.86e-06
    Outer Step:   170      LossTrajs: 0.00005225     ContextsNorm: 0.01339833     ValIndCrit: 0.00013458
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.06e-06
        -DiffCxt:  1.01e-05
    Outer Step:   180      LossTrajs: 0.00004107     ContextsNorm: 0.01331542     ValIndCrit: 0.00011030
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.44e-07
        -DiffCxt:  1.13e-05
    Outer Step:   190      LossTrajs: 0.00003014     ContextsNorm: 0.01282757     ValIndCrit: 0.00008368
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.25e-07
        -DiffCxt:  1.37e-06
    Outer Step:   200      LossTrajs: 0.00003264     ContextsNorm: 0.01270541     ValIndCrit: 0.00009378
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.06e-07
        -DiffCxt:  4.24e-06
    Outer Step:   210      LossTrajs: 0.00002973     ContextsNorm: 0.01235437     ValIndCrit: 0.00008892
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.01e-07
        -DiffCxt:  3.16e-06
    Outer Step:   220      LossTrajs: 0.00002079     ContextsNorm: 0.01228592     ValIndCrit: 0.00008252
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.12e-08
        -DiffCxt:  3.59e-07
    Outer Step:   230      LossTrajs: 0.00002370     ContextsNorm: 0.01193966     ValIndCrit: 0.00009237
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.89e-08
        -DiffCxt:  9.34e-07
    Outer Step:   240      LossTrajs: 0.00002723     ContextsNorm: 0.01171412     ValIndCrit: 0.00008428
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.80e-07
        -DiffCxt:  5.61e-06
    Outer Step:   250      LossTrajs: 0.00002727     ContextsNorm: 0.01156515     ValIndCrit: 0.00009022
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.18e-07
        -DiffCxt:  3.28e-06
    Outer Step:   260      LossTrajs: 0.00002010     ContextsNorm: 0.01147321     ValIndCrit: 0.00008085
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.39e-08
        -DiffCxt:  9.82e-07
    Outer Step:   270      LossTrajs: 0.00004862     ContextsNorm: 0.01127926     ValIndCrit: 0.00009719
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.27e-07
        -DiffCxt:  4.52e-06
    Outer Step:   280      LossTrajs: 0.00003233     ContextsNorm: 0.01124245     ValIndCrit: 0.00011133
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.56e-07
        -DiffCxt:  1.08e-05
    Outer Step:   290      LossTrajs: 0.00004987     ContextsNorm: 0.01145673     ValIndCrit: 0.00007199
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.27e-06
        -DiffCxt:  8.13e-06
    Outer Step:   300      LossTrajs: 0.00002184     ContextsNorm: 0.01166357     ValIndCrit: 0.00007820
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.02e-07
        -DiffCxt:  1.62e-06
    Outer Step:   310      LossTrajs: 0.00002120     ContextsNorm: 0.01132170     ValIndCrit: 0.00008075
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.90e-08
        -DiffCxt:  1.06e-06
    Outer Step:   320      LossTrajs: 0.00002147     ContextsNorm: 0.01101331     ValIndCrit: 0.00007308
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.13e-07
        -DiffCxt:  3.83e-06
    Outer Step:   330      LossTrajs: 0.00002538     ContextsNorm: 0.01092611     ValIndCrit: 0.00007134
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.63e-07
        -DiffCxt:  3.19e-06
    Outer Step:   340      LossTrajs: 0.00004779     ContextsNorm: 0.01081389     ValIndCrit: 0.00008377
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.87e-07
        -DiffCxt:  5.70e-06
    Outer Step:   350      LossTrajs: 0.00002832     ContextsNorm: 0.01083831     ValIndCrit: 0.00007455
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.70e-07
        -DiffCxt:  6.63e-06
    Outer Step:   360      LossTrajs: 0.00002668     ContextsNorm: 0.01077354     ValIndCrit: 0.00008007
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.01e-06
        -DiffCxt:  1.49e-05
    Outer Step:   370      LossTrajs: 0.00001559     ContextsNorm: 0.01068501     ValIndCrit: 0.00007327
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.76e-09
        -DiffCxt:  2.53e-07
    Outer Step:   380      LossTrajs: 0.00002727     ContextsNorm: 0.01054827     ValIndCrit: 0.00008234
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.71e-07
        -DiffCxt:  6.70e-06
    Outer Step:   390      LossTrajs: 0.00002291     ContextsNorm: 0.01046409     ValIndCrit: 0.00009262
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.81e-07
        -DiffCxt:  2.06e-06
    Outer Step:   400      LossTrajs: 0.00002129     ContextsNorm: 0.01038992     ValIndCrit: 0.00008569
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.83e-07
        -DiffCxt:  2.46e-06
    Outer Step:   410      LossTrajs: 0.00002945     ContextsNorm: 0.01030841     ValIndCrit: 0.00008410
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.50e-07
        -DiffCxt:  1.91e-06
    Outer Step:   420      LossTrajs: 0.00002014     ContextsNorm: 0.01026183     ValIndCrit: 0.00007988
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.74e-08
        -DiffCxt:  8.35e-07
    Outer Step:   430      LossTrajs: 0.00003095     ContextsNorm: 0.01020211     ValIndCrit: 0.00007353
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.86e-07
        -DiffCxt:  5.56e-06
    Outer Step:   440      LossTrajs: 0.00003097     ContextsNorm: 0.00988983     ValIndCrit: 0.00011587
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.15e-06
        -DiffCxt:  5.86e-06
    Outer Step:   450      LossTrajs: 0.00001496     ContextsNorm: 0.00986929     ValIndCrit: 0.00007935
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.31e-09
        -DiffCxt:  1.28e-07
    Outer Step:   460      LossTrajs: 0.00001814     ContextsNorm: 0.00987383     ValIndCrit: 0.00008043
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.03e-07
        -DiffCxt:  3.55e-06
    Outer Step:   470      LossTrajs: 0.00002223     ContextsNorm: 0.00987696     ValIndCrit: 0.00007171
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.14e-07
        -DiffCxt:  7.68e-06
    Outer Step:   480      LossTrajs: 0.00002226     ContextsNorm: 0.00999899     ValIndCrit: 0.00007961
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.18e-07
        -DiffCxt:  2.90e-06
    Outer Step:   490      LossTrajs: 0.00001840     ContextsNorm: 0.00988628     ValIndCrit: 0.00007370
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.32e-08
        -DiffCxt:  1.55e-06
    Outer Step:   499      LossTrajs: 0.00002172     ContextsNorm: 0.00984015     ValIndCrit: 0.00008058
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.41e-07
        -DiffCxt:  2.31e-06

Total gradient descent training time: 2 hours 1 mins 40 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 160138
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 7.1343784e-05


############# Neural Context Flow #############

Jax version: 0.4.28
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./adapt/
 Seed: 6078


############# Neural Context Flow #############

Jax version: 0.4.28
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt_test
 Savepath: ./adapt/
 Seed: 6078

==  Begining in-domain visualisation ... ==
    Environment id: 8
    Trajectory id: 21
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./results_in_domain.png
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.05940069
    Epoch:     1     LossContext: 0.04293373
    Epoch:     2     LossContext: 0.02869913
    Epoch:     3     LossContext: 0.01498161
    Epoch:    10     LossContext: 0.00432042
    Epoch:    20     LossContext: 0.00189049
    Epoch:    30     LossContext: 0.00024126
    Epoch:    40     LossContext: 0.00002937
    Epoch:    50     LossContext: 0.00009116
    Epoch:    60     LossContext: 0.00007345
    Epoch:    70     LossContext: 0.00004026
    Epoch:    80     LossContext: 0.00002314
    Epoch:    90     LossContext: 0.00002558
    Epoch:   100     LossContext: 0.00002352
    Epoch:   110     LossContext: 0.00002334
    Epoch:   120     LossContext: 0.00002309
    Epoch:   130     LossContext: 0.00002304
    Epoch:   140     LossContext: 0.00002298
    Epoch:   150     LossContext: 0.00002296
    Epoch:   160     LossContext: 0.00002293
    Epoch:   170     LossContext: 0.00002291
    Epoch:   180     LossContext: 0.00002288
    Epoch:   190     LossContext: 0.00002286
    Epoch:   200     LossContext: 0.00002284
    Epoch:   210     LossContext: 0.00002282
    Epoch:   220     LossContext: 0.00002279
    Epoch:   230     LossContext: 0.00002277
    Epoch:   240     LossContext: 0.00002275
    Epoch:   250     LossContext: 0.00002273
    Epoch:   260     LossContext: 0.00002271
    Epoch:   270     LossContext: 0.00002269
    Epoch:   280     LossContext: 0.00002267
    Epoch:   290     LossContext: 0.00002265
    Epoch:   300     LossContext: 0.00002262
    Epoch:   310     LossContext: 0.00002260
    Epoch:   320     LossContext: 0.00002258
    Epoch:   330     LossContext: 0.00002256
    Epoch:   340     LossContext: 0.00002253
    Epoch:   350     LossContext: 0.00002251
    Epoch:   360     LossContext: 0.00002249
    Epoch:   370     LossContext: 0.00002247
    Epoch:   380     LossContext: 0.00002245
    Epoch:   390     LossContext: 0.00002242
    Epoch:   400     LossContext: 0.00002240
    Epoch:   410     LossContext: 0.00002238
    Epoch:   420     LossContext: 0.00002236
    Epoch:   430     LossContext: 0.00002234
    Epoch:   440     LossContext: 0.00002232
    Epoch:   450     LossContext: 0.00002230
    Epoch:   460     LossContext: 0.00002228
    Epoch:   470     LossContext: 0.00002226
    Epoch:   480     LossContext: 0.00002224
    Epoch:   490     LossContext: 0.00002222
    Epoch:   500     LossContext: 0.00002220
    Epoch:   510     LossContext: 0.00002218
    Epoch:   520     LossContext: 0.00002216
    Epoch:   530     LossContext: 0.00002214
    Epoch:   540     LossContext: 0.00002212
    Epoch:   550     LossContext: 0.00002210
    Epoch:   560     LossContext: 0.00002208
    Epoch:   570     LossContext: 0.00002206
    Epoch:   580     LossContext: 0.00002204
    Epoch:   590     LossContext: 0.00002202
    Epoch:   600     LossContext: 0.00002200
    Epoch:   610     LossContext: 0.00002198
    Epoch:   620     LossContext: 0.00002196
    Epoch:   630     LossContext: 0.00002194
    Epoch:   640     LossContext: 0.00002192
    Epoch:   650     LossContext: 0.00002191
    Epoch:   660     LossContext: 0.00002189
    Epoch:   670     LossContext: 0.00002187
    Epoch:   680     LossContext: 0.00002185
    Epoch:   690     LossContext: 0.00002183
    Epoch:   700     LossContext: 0.00002182
    Epoch:   710     LossContext: 0.00002180
    Epoch:   720     LossContext: 0.00002178
    Epoch:   730     LossContext: 0.00002176
    Epoch:   740     LossContext: 0.00002174
    Epoch:   750     LossContext: 0.00002173
    Epoch:   760     LossContext: 0.00002171
    Epoch:   770     LossContext: 0.00002169
    Epoch:   780     LossContext: 0.00002167
    Epoch:   790     LossContext: 0.00002166
    Epoch:   800     LossContext: 0.00002164
    Epoch:   810     LossContext: 0.00002162
    Epoch:   820     LossContext: 0.00002160
    Epoch:   830     LossContext: 0.00002159
    Epoch:   840     LossContext: 0.00002157
    Epoch:   850     LossContext: 0.00002155
    Epoch:   860     LossContext: 0.00002154
    Epoch:   870     LossContext: 0.00002152
    Epoch:   880     LossContext: 0.00002150
    Epoch:   890     LossContext: 0.00002148
    Epoch:   900     LossContext: 0.00002147
    Epoch:   910     LossContext: 0.00002145
    Epoch:   920     LossContext: 0.00002143
    Epoch:   930     LossContext: 0.00002142
    Epoch:   940     LossContext: 0.00002140
    Epoch:   950     LossContext: 0.00002138
    Epoch:   960     LossContext: 0.00002137
    Epoch:   970     LossContext: 0.00002135
    Epoch:   980     LossContext: 0.00002133
    Epoch:   990     LossContext: 0.00002132
    Epoch:  1000     LossContext: 0.00002130
    Epoch:  1010     LossContext: 0.00002128
    Epoch:  1020     LossContext: 0.00002127
    Epoch:  1030     LossContext: 0.00002125
    Epoch:  1040     LossContext: 0.00002124
    Epoch:  1050     LossContext: 0.00002122
    Epoch:  1060     LossContext: 0.00002120
    Epoch:  1070     LossContext: 0.00002119
    Epoch:  1080     LossContext: 0.00002117
    Epoch:  1090     LossContext: 0.00002116
    Epoch:  1100     LossContext: 0.00002114
    Epoch:  1110     LossContext: 0.00002112
    Epoch:  1120     LossContext: 0.00002111
    Epoch:  1130     LossContext: 0.00002109
    Epoch:  1140     LossContext: 0.00002108
    Epoch:  1150     LossContext: 0.00002106
    Epoch:  1160     LossContext: 0.00002105
    Epoch:  1170     LossContext: 0.00002103
    Epoch:  1180     LossContext: 0.00002102
    Epoch:  1190     LossContext: 0.00002100
    Epoch:  1200     LossContext: 0.00002099
    Epoch:  1210     LossContext: 0.00002097
    Epoch:  1220     LossContext: 0.00002095
    Epoch:  1230     LossContext: 0.00002094
    Epoch:  1240     LossContext: 0.00002092
    Epoch:  1250     LossContext: 0.00002091
    Epoch:  1260     LossContext: 0.00002089
    Epoch:  1270     LossContext: 0.00002088
    Epoch:  1280     LossContext: 0.00002086
    Epoch:  1290     LossContext: 0.00002084
    Epoch:  1300     LossContext: 0.00002083
    Epoch:  1310     LossContext: 0.00002081
    Epoch:  1320     LossContext: 0.00002079
    Epoch:  1330     LossContext: 0.00002078
    Epoch:  1340     LossContext: 0.00002078
    Epoch:  1350     LossContext: 0.00002102
    Epoch:  1360     LossContext: 0.00002595
    Epoch:  1370     LossContext: 0.00013377
    Epoch:  1380     LossContext: 0.00005266
    Epoch:  1390     LossContext: 0.00003957
    Epoch:  1400     LossContext: 0.00002806
    Epoch:  1410     LossContext: 0.00002358
    Epoch:  1420     LossContext: 0.00002187
    Epoch:  1430     LossContext: 0.00002075
    Epoch:  1440     LossContext: 0.00002097
    Epoch:  1450     LossContext: 0.00002076
    Epoch:  1460     LossContext: 0.00002156
    Epoch:  1470     LossContext: 0.00004430
    Epoch:  1480     LossContext: 0.00009884
    Epoch:  1490     LossContext: 0.00003562
    Epoch:  1499     LossContext: 0.00002548

Gradient descent adaptation time: 0 hours 1 mins 41 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.16399102
    Epoch:     1     LossContext: 0.09194272
    Epoch:     2     LossContext: 0.02789814
    Epoch:     3     LossContext: 0.00056292
    Epoch:    10     LossContext: 0.01508930
    Epoch:    20     LossContext: 0.00990452
    Epoch:    30     LossContext: 0.00179639
    Epoch:    40     LossContext: 0.00061180
    Epoch:    50     LossContext: 0.00033736
    Epoch:    60     LossContext: 0.00013285
    Epoch:    70     LossContext: 0.00003644
    Epoch:    80     LossContext: 0.00005104
    Epoch:    90     LossContext: 0.00003603
    Epoch:   100     LossContext: 0.00002965
    Epoch:   110     LossContext: 0.00002895
    Epoch:   120     LossContext: 0.00002887
    Epoch:   130     LossContext: 0.00002882
    Epoch:   140     LossContext: 0.00002877
    Epoch:   150     LossContext: 0.00002873
    Epoch:   160     LossContext: 0.00002868
    Epoch:   170     LossContext: 0.00002864
    Epoch:   180     LossContext: 0.00002860
    Epoch:   190     LossContext: 0.00002855
    Epoch:   200     LossContext: 0.00002851
    Epoch:   210     LossContext: 0.00002846
    Epoch:   220     LossContext: 0.00002841
    Epoch:   230     LossContext: 0.00002836
    Epoch:   240     LossContext: 0.00002832
    Epoch:   250     LossContext: 0.00002827
    Epoch:   260     LossContext: 0.00002822
    Epoch:   270     LossContext: 0.00002817
    Epoch:   280     LossContext: 0.00002812
    Epoch:   290     LossContext: 0.00002807
    Epoch:   300     LossContext: 0.00002802
    Epoch:   310     LossContext: 0.00002796
    Epoch:   320     LossContext: 0.00002791
    Epoch:   330     LossContext: 0.00002786
    Epoch:   340     LossContext: 0.00002781
    Epoch:   350     LossContext: 0.00002775
    Epoch:   360     LossContext: 0.00002770
    Epoch:   370     LossContext: 0.00002764
    Epoch:   380     LossContext: 0.00002759
    Epoch:   390     LossContext: 0.00002753
    Epoch:   400     LossContext: 0.00002747
    Epoch:   410     LossContext: 0.00002741
    Epoch:   420     LossContext: 0.00002736
    Epoch:   430     LossContext: 0.00002730
    Epoch:   440     LossContext: 0.00002724
    Epoch:   450     LossContext: 0.00002718
    Epoch:   460     LossContext: 0.00002712
    Epoch:   470     LossContext: 0.00002707
    Epoch:   480     LossContext: 0.00002701
    Epoch:   490     LossContext: 0.00002695
    Epoch:   500     LossContext: 0.00002689
    Epoch:   510     LossContext: 0.00002684
    Epoch:   520     LossContext: 0.00002679
    Epoch:   530     LossContext: 0.00002673
    Epoch:   540     LossContext: 0.00002668
    Epoch:   550     LossContext: 0.00002662
    Epoch:   560     LossContext: 0.00002657
    Epoch:   570     LossContext: 0.00002652
    Epoch:   580     LossContext: 0.00002646
    Epoch:   590     LossContext: 0.00002641
    Epoch:   600     LossContext: 0.00002636
    Epoch:   610     LossContext: 0.00002631
    Epoch:   620     LossContext: 0.00002626
    Epoch:   630     LossContext: 0.00002620
    Epoch:   640     LossContext: 0.00002615
    Epoch:   650     LossContext: 0.00002610
    Epoch:   660     LossContext: 0.00002605
    Epoch:   670     LossContext: 0.00002599
    Epoch:   680     LossContext: 0.00002594
    Epoch:   690     LossContext: 0.00002589
    Epoch:   700     LossContext: 0.00002583
    Epoch:   710     LossContext: 0.00002578
    Epoch:   720     LossContext: 0.00002573
    Epoch:   730     LossContext: 0.00002567
    Epoch:   740     LossContext: 0.00002562
    Epoch:   750     LossContext: 0.00002557
    Epoch:   760     LossContext: 0.00002552
    Epoch:   770     LossContext: 0.00002547
    Epoch:   780     LossContext: 0.00002542
    Epoch:   790     LossContext: 0.00002537
    Epoch:   800     LossContext: 0.00002532
    Epoch:   810     LossContext: 0.00002527
    Epoch:   820     LossContext: 0.00002522
    Epoch:   830     LossContext: 0.00002517
    Epoch:   840     LossContext: 0.00002512
    Epoch:   850     LossContext: 0.00002508
    Epoch:   860     LossContext: 0.00002503
    Epoch:   870     LossContext: 0.00002498
    Epoch:   880     LossContext: 0.00002494
    Epoch:   890     LossContext: 0.00002489
    Epoch:   900     LossContext: 0.00002485
    Epoch:   910     LossContext: 0.00002481
    Epoch:   920     LossContext: 0.00002476
    Epoch:   930     LossContext: 0.00002472
    Epoch:   940     LossContext: 0.00002468
    Epoch:   950     LossContext: 0.00002464
    Epoch:   960     LossContext: 0.00002460
    Epoch:   970     LossContext: 0.00002455
    Epoch:   980     LossContext: 0.00002451
    Epoch:   990     LossContext: 0.00002447
    Epoch:  1000     LossContext: 0.00002443
    Epoch:  1010     LossContext: 0.00002439
    Epoch:  1020     LossContext: 0.00002435
    Epoch:  1030     LossContext: 0.00002431
    Epoch:  1040     LossContext: 0.00002427
    Epoch:  1050     LossContext: 0.00002423
    Epoch:  1060     LossContext: 0.00002420
    Epoch:  1070     LossContext: 0.00002416
    Epoch:  1080     LossContext: 0.00002412
    Epoch:  1090     LossContext: 0.00002408
    Epoch:  1100     LossContext: 0.00002405
    Epoch:  1110     LossContext: 0.00002401
    Epoch:  1120     LossContext: 0.00002398
    Epoch:  1130     LossContext: 0.00002394
    Epoch:  1140     LossContext: 0.00002391
    Epoch:  1150     LossContext: 0.00002387
    Epoch:  1160     LossContext: 0.00002384
    Epoch:  1170     LossContext: 0.00002381
    Epoch:  1180     LossContext: 0.00002378
    Epoch:  1190     LossContext: 0.00002374
    Epoch:  1200     LossContext: 0.00002371
    Epoch:  1210     LossContext: 0.00002368
    Epoch:  1220     LossContext: 0.00002365
    Epoch:  1230     LossContext: 0.00002362
    Epoch:  1240     LossContext: 0.00002359
    Epoch:  1250     LossContext: 0.00002356
    Epoch:  1260     LossContext: 0.00002353
    Epoch:  1270     LossContext: 0.00002350
    Epoch:  1280     LossContext: 0.00002347
    Epoch:  1290     LossContext: 0.00002344
    Epoch:  1300     LossContext: 0.00002341
    Epoch:  1310     LossContext: 0.00002338
    Epoch:  1320     LossContext: 0.00002335
    Epoch:  1330     LossContext: 0.00002332
    Epoch:  1340     LossContext: 0.00002329
    Epoch:  1350     LossContext: 0.00002326
    Epoch:  1360     LossContext: 0.00002324
    Epoch:  1370     LossContext: 0.00002321
    Epoch:  1380     LossContext: 0.00002318
    Epoch:  1390     LossContext: 0.00002316
    Epoch:  1400     LossContext: 0.00002313
    Epoch:  1410     LossContext: 0.00002311
    Epoch:  1420     LossContext: 0.00002308
    Epoch:  1430     LossContext: 0.00002306
    Epoch:  1440     LossContext: 0.00002303
    Epoch:  1450     LossContext: 0.00002300
    Epoch:  1460     LossContext: 0.00002298
    Epoch:  1470     LossContext: 0.00002295
    Epoch:  1480     LossContext: 0.00002293
    Epoch:  1490     LossContext: 0.00002290
    Epoch:  1499     LossContext: 0.00002288

Gradient descent adaptation time: 0 hours 1 mins 26 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.02009044
    Epoch:     1     LossContext: 0.00336532
    Epoch:     2     LossContext: 0.00007740
    Epoch:     3     LossContext: 0.00247590
    Epoch:    10     LossContext: 0.00123038
    Epoch:    20     LossContext: 0.00069132
    Epoch:    30     LossContext: 0.00049799
    Epoch:    40     LossContext: 0.00018288
    Epoch:    50     LossContext: 0.00004676
    Epoch:    60     LossContext: 0.00004265
    Epoch:    70     LossContext: 0.00002181
    Epoch:    80     LossContext: 0.00002138
    Epoch:    90     LossContext: 0.00001932
    Epoch:   100     LossContext: 0.00001954
    Epoch:   110     LossContext: 0.00001920
    Epoch:   120     LossContext: 0.00001900
    Epoch:   130     LossContext: 0.00001893
    Epoch:   140     LossContext: 0.00001884
    Epoch:   150     LossContext: 0.00001874
    Epoch:   160     LossContext: 0.00001865
    Epoch:   170     LossContext: 0.00001857
    Epoch:   180     LossContext: 0.00001848
    Epoch:   190     LossContext: 0.00001839
    Epoch:   200     LossContext: 0.00001829
    Epoch:   210     LossContext: 0.00001820
    Epoch:   220     LossContext: 0.00001811
    Epoch:   230     LossContext: 0.00001801
    Epoch:   240     LossContext: 0.00001792
    Epoch:   250     LossContext: 0.00001782
    Epoch:   260     LossContext: 0.00001772
    Epoch:   270     LossContext: 0.00001763
    Epoch:   280     LossContext: 0.00001753
    Epoch:   290     LossContext: 0.00001744
    Epoch:   300     LossContext: 0.00001734
    Epoch:   310     LossContext: 0.00001725
    Epoch:   320     LossContext: 0.00001715
    Epoch:   330     LossContext: 0.00001706
    Epoch:   340     LossContext: 0.00001697
    Epoch:   350     LossContext: 0.00001688
    Epoch:   360     LossContext: 0.00001679
    Epoch:   370     LossContext: 0.00001671
    Epoch:   380     LossContext: 0.00001662
    Epoch:   390     LossContext: 0.00001654
    Epoch:   400     LossContext: 0.00001646
    Epoch:   410     LossContext: 0.00001638
    Epoch:   420     LossContext: 0.00001630
    Epoch:   430     LossContext: 0.00001623
    Epoch:   440     LossContext: 0.00001615
    Epoch:   450     LossContext: 0.00001608
    Epoch:   460     LossContext: 0.00001601
    Epoch:   470     LossContext: 0.00001594
    Epoch:   480     LossContext: 0.00001587
    Epoch:   490     LossContext: 0.00001580
    Epoch:   500     LossContext: 0.00001574
    Epoch:   510     LossContext: 0.00001568
    Epoch:   520     LossContext: 0.00001562
    Epoch:   530     LossContext: 0.00001556
    Epoch:   540     LossContext: 0.00001551
    Epoch:   550     LossContext: 0.00001546
    Epoch:   560     LossContext: 0.00001541
    Epoch:   570     LossContext: 0.00001536
    Epoch:   580     LossContext: 0.00001532
    Epoch:   590     LossContext: 0.00001528
    Epoch:   600     LossContext: 0.00001524
    Epoch:   610     LossContext: 0.00001520
    Epoch:   620     LossContext: 0.00001516
    Epoch:   630     LossContext: 0.00001513
    Epoch:   640     LossContext: 0.00001509
    Epoch:   650     LossContext: 0.00001506
    Epoch:   660     LossContext: 0.00001503
    Epoch:   670     LossContext: 0.00001500
    Epoch:   680     LossContext: 0.00001497
    Epoch:   690     LossContext: 0.00001494
    Epoch:   700     LossContext: 0.00001491
    Epoch:   710     LossContext: 0.00001488
    Epoch:   720     LossContext: 0.00001486
    Epoch:   730     LossContext: 0.00001483
    Epoch:   740     LossContext: 0.00001480
    Epoch:   750     LossContext: 0.00001478
    Epoch:   760     LossContext: 0.00001476
    Epoch:   770     LossContext: 0.00001473
    Epoch:   780     LossContext: 0.00001471
    Epoch:   790     LossContext: 0.00001469
    Epoch:   800     LossContext: 0.00001466
    Epoch:   810     LossContext: 0.00001464
    Epoch:   820     LossContext: 0.00001462
    Epoch:   830     LossContext: 0.00001460
    Epoch:   840     LossContext: 0.00001458
    Epoch:   850     LossContext: 0.00001456
    Epoch:   860     LossContext: 0.00001454
    Epoch:   870     LossContext: 0.00001452
    Epoch:   880     LossContext: 0.00001450
    Epoch:   890     LossContext: 0.00001448
    Epoch:   900     LossContext: 0.00001446
    Epoch:   910     LossContext: 0.00001444
    Epoch:   920     LossContext: 0.00001442
    Epoch:   930     LossContext: 0.00001440
    Epoch:   940     LossContext: 0.00001438
    Epoch:   950     LossContext: 0.00001436
    Epoch:   960     LossContext: 0.00001434
    Epoch:   970     LossContext: 0.00001433
    Epoch:   980     LossContext: 0.00001431
    Epoch:   990     LossContext: 0.00001429
    Epoch:  1000     LossContext: 0.00001427
    Epoch:  1010     LossContext: 0.00001426
    Epoch:  1020     LossContext: 0.00001424
    Epoch:  1030     LossContext: 0.00001422
    Epoch:  1040     LossContext: 0.00001421
    Epoch:  1050     LossContext: 0.00001419
    Epoch:  1060     LossContext: 0.00001418
    Epoch:  1070     LossContext: 0.00001416
    Epoch:  1080     LossContext: 0.00001415
    Epoch:  1090     LossContext: 0.00001413
    Epoch:  1100     LossContext: 0.00001412
    Epoch:  1110     LossContext: 0.00001410
    Epoch:  1120     LossContext: 0.00001409
    Epoch:  1130     LossContext: 0.00001407
    Epoch:  1140     LossContext: 0.00001406
    Epoch:  1150     LossContext: 0.00001404
    Epoch:  1160     LossContext: 0.00001403
    Epoch:  1170     LossContext: 0.00001402
    Epoch:  1180     LossContext: 0.00001400
    Epoch:  1190     LossContext: 0.00001399
    Epoch:  1200     LossContext: 0.00001398
    Epoch:  1210     LossContext: 0.00001396
    Epoch:  1220     LossContext: 0.00001395
    Epoch:  1230     LossContext: 0.00001394
    Epoch:  1240     LossContext: 0.00001392
    Epoch:  1250     LossContext: 0.00001391
    Epoch:  1260     LossContext: 0.00001390
    Epoch:  1270     LossContext: 0.00001389
    Epoch:  1280     LossContext: 0.00001388
    Epoch:  1290     LossContext: 0.00001386
    Epoch:  1300     LossContext: 0.00001385
    Epoch:  1310     LossContext: 0.00001384
    Epoch:  1320     LossContext: 0.00001383
    Epoch:  1330     LossContext: 0.00001382
    Epoch:  1340     LossContext: 0.00001381
    Epoch:  1350     LossContext: 0.00001379
    Epoch:  1360     LossContext: 0.00001378
    Epoch:  1370     LossContext: 0.00001377
    Epoch:  1380     LossContext: 0.00001376
    Epoch:  1390     LossContext: 0.00001375
    Epoch:  1400     LossContext: 0.00001374
    Epoch:  1410     LossContext: 0.00001373
    Epoch:  1420     LossContext: 0.00001372
    Epoch:  1430     LossContext: 0.00001371
    Epoch:  1440     LossContext: 0.00001370
    Epoch:  1450     LossContext: 0.00001368
    Epoch:  1460     LossContext: 0.00001367
    Epoch:  1470     LossContext: 0.00001366
    Epoch:  1480     LossContext: 0.00001365
    Epoch:  1490     LossContext: 0.00001364
    Epoch:  1499     LossContext: 0.00001363

Gradient descent adaptation time: 0 hours 1 mins 26 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.00656954
    Epoch:     1     LossContext: 0.00480142
    Epoch:     2     LossContext: 0.00172350
    Epoch:     3     LossContext: 0.00091034
    Epoch:    10     LossContext: 0.00052933
    Epoch:    20     LossContext: 0.00034121
    Epoch:    30     LossContext: 0.00010218
    Epoch:    40     LossContext: 0.00003958
    Epoch:    50     LossContext: 0.00004413
    Epoch:    60     LossContext: 0.00002063
    Epoch:    70     LossContext: 0.00002145
    Epoch:    80     LossContext: 0.00002081
    Epoch:    90     LossContext: 0.00001991
    Epoch:   100     LossContext: 0.00001980
    Epoch:   110     LossContext: 0.00001976
    Epoch:   120     LossContext: 0.00001966
    Epoch:   130     LossContext: 0.00001958
    Epoch:   140     LossContext: 0.00001950
    Epoch:   150     LossContext: 0.00001943
    Epoch:   160     LossContext: 0.00001936
    Epoch:   170     LossContext: 0.00001930
    Epoch:   180     LossContext: 0.00001925
    Epoch:   190     LossContext: 0.00001920
    Epoch:   200     LossContext: 0.00001915
    Epoch:   210     LossContext: 0.00001910
    Epoch:   220     LossContext: 0.00001906
    Epoch:   230     LossContext: 0.00001901
    Epoch:   240     LossContext: 0.00001897
    Epoch:   250     LossContext: 0.00001892
    Epoch:   260     LossContext: 0.00001888
    Epoch:   270     LossContext: 0.00001883
    Epoch:   280     LossContext: 0.00001879
    Epoch:   290     LossContext: 0.00001875
    Epoch:   300     LossContext: 0.00001870
    Epoch:   310     LossContext: 0.00001866
    Epoch:   320     LossContext: 0.00001862
    Epoch:   330     LossContext: 0.00001858
    Epoch:   340     LossContext: 0.00001853
    Epoch:   350     LossContext: 0.00001849
    Epoch:   360     LossContext: 0.00001845
    Epoch:   370     LossContext: 0.00001841
    Epoch:   380     LossContext: 0.00001836
    Epoch:   390     LossContext: 0.00001832
    Epoch:   400     LossContext: 0.00001828
    Epoch:   410     LossContext: 0.00001824
    Epoch:   420     LossContext: 0.00001820
    Epoch:   430     LossContext: 0.00001816
    Epoch:   440     LossContext: 0.00001812
    Epoch:   450     LossContext: 0.00001808
    Epoch:   460     LossContext: 0.00001804
    Epoch:   470     LossContext: 0.00001800
    Epoch:   480     LossContext: 0.00001796
    Epoch:   490     LossContext: 0.00001793
    Epoch:   500     LossContext: 0.00001789
    Epoch:   510     LossContext: 0.00001785
    Epoch:   520     LossContext: 0.00001782
    Epoch:   530     LossContext: 0.00001779
    Epoch:   540     LossContext: 0.00001775
    Epoch:   550     LossContext: 0.00001772
    Epoch:   560     LossContext: 0.00001768
    Epoch:   570     LossContext: 0.00001765
    Epoch:   580     LossContext: 0.00001762
    Epoch:   590     LossContext: 0.00001759
    Epoch:   600     LossContext: 0.00001756
    Epoch:   610     LossContext: 0.00001752
    Epoch:   620     LossContext: 0.00001749
    Epoch:   630     LossContext: 0.00001747
    Epoch:   640     LossContext: 0.00001744
    Epoch:   650     LossContext: 0.00001741
    Epoch:   660     LossContext: 0.00001738
    Epoch:   670     LossContext: 0.00001735
    Epoch:   680     LossContext: 0.00001732
    Epoch:   690     LossContext: 0.00001729
    Epoch:   700     LossContext: 0.00001726
    Epoch:   710     LossContext: 0.00001724
    Epoch:   720     LossContext: 0.00001721
    Epoch:   730     LossContext: 0.00001718
    Epoch:   740     LossContext: 0.00001716
    Epoch:   750     LossContext: 0.00001713
    Epoch:   760     LossContext: 0.00001710
    Epoch:   770     LossContext: 0.00001708
    Epoch:   780     LossContext: 0.00001705
    Epoch:   790     LossContext: 0.00001703
    Epoch:   800     LossContext: 0.00001700
    Epoch:   810     LossContext: 0.00001698
    Epoch:   820     LossContext: 0.00001696
    Epoch:   830     LossContext: 0.00001693
    Epoch:   840     LossContext: 0.00001691
    Epoch:   850     LossContext: 0.00001689
    Epoch:   860     LossContext: 0.00001686
    Epoch:   870     LossContext: 0.00001684
    Epoch:   880     LossContext: 0.00001682
    Epoch:   890     LossContext: 0.00001680
    Epoch:   900     LossContext: 0.00001678
    Epoch:   910     LossContext: 0.00001676
    Epoch:   920     LossContext: 0.00001673
    Epoch:   930     LossContext: 0.00001671
    Epoch:   940     LossContext: 0.00001669
    Epoch:   950     LossContext: 0.00001667
    Epoch:   960     LossContext: 0.00001666
    Epoch:   970     LossContext: 0.00001664
    Epoch:   980     LossContext: 0.00001662
    Epoch:   990     LossContext: 0.00001660
    Epoch:  1000     LossContext: 0.00001658
    Epoch:  1010     LossContext: 0.00001656
    Epoch:  1020     LossContext: 0.00001654
    Epoch:  1030     LossContext: 0.00001653
    Epoch:  1040     LossContext: 0.00001652
    Epoch:  1050     LossContext: 0.00001658
    Epoch:  1060     LossContext: 0.00001684
    Epoch:  1070     LossContext: 0.00001850
    Epoch:  1080     LossContext: 0.00002888
    Epoch:  1090     LossContext: 0.00001692
    Epoch:  1100     LossContext: 0.00002069
    Epoch:  1110     LossContext: 0.00001656
    Epoch:  1120     LossContext: 0.00001723
    Epoch:  1130     LossContext: 0.00003112
    Epoch:  1140     LossContext: 0.00001719
    Epoch:  1150     LossContext: 0.00001877
    Epoch:  1160     LossContext: 0.00001903
    Epoch:  1170     LossContext: 0.00001631
    Epoch:  1180     LossContext: 0.00001797
    Epoch:  1190     LossContext: 0.00002405
    Epoch:  1200     LossContext: 0.00001956
    Epoch:  1210     LossContext: 0.00001755
    Epoch:  1220     LossContext: 0.00002412
    Epoch:  1230     LossContext: 0.00001981
    Epoch:  1240     LossContext: 0.00001722
    Epoch:  1250     LossContext: 0.00002250
    Epoch:  1260     LossContext: 0.00002283
    Epoch:  1270     LossContext: 0.00001666
    Epoch:  1280     LossContext: 0.00002115
    Epoch:  1290     LossContext: 0.00002275
    Epoch:  1300     LossContext: 0.00001626
    Epoch:  1310     LossContext: 0.00001769
    Epoch:  1320     LossContext: 0.00003389
    Epoch:  1330     LossContext: 0.00001612
    Epoch:  1340     LossContext: 0.00001781
    Epoch:  1350     LossContext: 0.00001851
    Epoch:  1360     LossContext: 0.00001620
    Epoch:  1370     LossContext: 0.00001641
    Epoch:  1380     LossContext: 0.00002455
    Epoch:  1390     LossContext: 0.00002640
    Epoch:  1400     LossContext: 0.00002192
    Epoch:  1410     LossContext: 0.00001669
    Epoch:  1420     LossContext: 0.00001775
    Epoch:  1430     LossContext: 0.00001614
    Epoch:  1440     LossContext: 0.00001651
    Epoch:  1450     LossContext: 0.00003340
    Epoch:  1460     LossContext: 0.00001660
    Epoch:  1470     LossContext: 0.00001646
    Epoch:  1480     LossContext: 0.00001674
    Epoch:  1490     LossContext: 0.00001735
    Epoch:  1499     LossContext: 0.00001601

Gradient descent adaptation time: 0 hours 1 mins 27 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 4.62755e-05

