
############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

========= STARTING =========
Working with a context size of 2^1 = 2
Run folder created successfuly: ./05052024-092750/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 092807
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 092808
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 46608 


WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed


=== Beginning training with proximal alternating minimization ... ===
    Number of examples in a batch: 4
    Maximum number of steps per inner minimization: 25
    Maximum number of outer minimizations: 500
    Maximum total number of training steps: 12500

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Outer Step:     0      LossTrajs: 1.81044996     ContextsNorm: 0.00000000     ValIndCrit: 1.69829845
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.49e-06
        -DiffCxt:  inf
    Outer Step:     1      LossTrajs: 1.50677943     ContextsNorm: 0.00012276     ValIndCrit: 1.41141903
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.46e-06
        -DiffCxt:  3.88e-03
    Outer Step:     2      LossTrajs: 1.14373624     ContextsNorm: 0.00031573     ValIndCrit: 1.06682456
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.25e-06
        -DiffCxt:  2.56e-03
    Outer Step:     3      LossTrajs: 0.59868580     ContextsNorm: 0.00066125     ValIndCrit: 0.56421119
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.90e-06
        -DiffCxt:  7.78e-04
    Outer Step:    10      LossTrajs: 0.25790417     ContextsNorm: 0.00133178     ValIndCrit: 0.30458218
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.26e-07
        -DiffCxt:  5.22e-05
    Outer Step:    20      LossTrajs: 0.16468894     ContextsNorm: 0.00239894     ValIndCrit: 0.17729028
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.07e-07
        -DiffCxt:  2.82e-05
    Outer Step:    30      LossTrajs: 0.08489937     ContextsNorm: 0.00409630     ValIndCrit: 0.08879507
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.17e-08
        -DiffCxt:  7.26e-06
    Outer Step:    40      LossTrajs: 0.07555290     ContextsNorm: 0.00562518     ValIndCrit: 0.07920280
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.22e-09
        -DiffCxt:  4.28e-06
    Outer Step:    50      LossTrajs: 0.07451831     ContextsNorm: 0.00781399     ValIndCrit: 0.07816203
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.48e-09
        -DiffCxt:  5.11e-06
    Outer Step:    60      LossTrajs: 0.07335620     ContextsNorm: 0.01165273     ValIndCrit: 0.07697055
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.73e-08
        -DiffCxt:  6.05e-06
    Outer Step:    70      LossTrajs: 0.06688280     ContextsNorm: 0.02409797     ValIndCrit: 0.06959999
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.93e-07
        -DiffCxt:  3.54e-06
    Outer Step:    80      LossTrajs: 0.03909003     ContextsNorm: 0.03499429     ValIndCrit: 0.04393270
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.81e-08
        -DiffCxt:  1.36e-07
    Outer Step:    90      LossTrajs: 0.03122975     ContextsNorm: 0.03768751     ValIndCrit: 0.03761768
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.72e-08
        -DiffCxt:  1.54e-07
    Outer Step:   100      LossTrajs: 0.02904842     ContextsNorm: 0.03898310     ValIndCrit: 0.03616863
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.76e-09
        -DiffCxt:  4.47e-08
    Outer Step:   110      LossTrajs: 0.02817778     ContextsNorm: 0.03939323     ValIndCrit: 0.03647576
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.02e-09
        -DiffCxt:  1.31e-07
    Outer Step:   120      LossTrajs: 0.02743174     ContextsNorm: 0.03910310     ValIndCrit: 0.03696220
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.76e-08
        -DiffCxt:  1.13e-07
    Outer Step:   130      LossTrajs: 0.02640934     ContextsNorm: 0.04044576     ValIndCrit: 0.03746767
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.10e-08
        -DiffCxt:  3.18e-07
    Outer Step:   140      LossTrajs: 0.02317197     ContextsNorm: 0.04312186     ValIndCrit: 0.03576146
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.57e-07
        -DiffCxt:  5.05e-07
    Outer Step:   150      LossTrajs: 0.00645207     ContextsNorm: 0.04913782     ValIndCrit: 0.01263865
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.96e-08
        -DiffCxt:  1.08e-07
    Outer Step:   160      LossTrajs: 0.00431281     ContextsNorm: 0.04945314     ValIndCrit: 0.00724582
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.71e-08
        -DiffCxt:  5.44e-08
    Outer Step:   170      LossTrajs: 0.00159951     ContextsNorm: 0.04901071     ValIndCrit: 0.00227597
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.33e-08
        -DiffCxt:  7.65e-08
    Outer Step:   180      LossTrajs: 0.00089688     ContextsNorm: 0.04874317     ValIndCrit: 0.00191422
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.79e-09
        -DiffCxt:  8.72e-08
    Outer Step:   190      LossTrajs: 0.00068645     ContextsNorm: 0.04900306     ValIndCrit: 0.00138373
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.94e-08
        -DiffCxt:  1.27e-07
    Outer Step:   200      LossTrajs: 0.00057647     ContextsNorm: 0.04902410     ValIndCrit: 0.00118417
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.62e-09
        -DiffCxt:  1.20e-08
    Outer Step:   210      LossTrajs: 0.00049002     ContextsNorm: 0.04875042     ValIndCrit: 0.00092099
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.41e-09
        -DiffCxt:  3.61e-08
    Outer Step:   220      LossTrajs: 0.00044297     ContextsNorm: 0.04879423     ValIndCrit: 0.00092900
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.90e-09
        -DiffCxt:  1.09e-07
    Outer Step:   230      LossTrajs: 0.00037311     ContextsNorm: 0.04866502     ValIndCrit: 0.00074383
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   12
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.62e-10
        -DiffCxt:  9.98e-09
    Outer Step:   240      LossTrajs: 0.00035597     ContextsNorm: 0.04873635     ValIndCrit: 0.00067592
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   13
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.21e-08
        -DiffCxt:  9.77e-09
    Outer Step:   250      LossTrajs: 0.00031833     ContextsNorm: 0.04841272     ValIndCrit: 0.00061209
        Saving best model so far ...
        -NbInnerStepsNode:    4
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.83e-10
        -DiffCxt:  1.51e-08
    Outer Step:   260      LossTrajs: 0.00030662     ContextsNorm: 0.04833229     ValIndCrit: 0.00052955
        Saving best model so far ...
        -NbInnerStepsNode:   14
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.91e-10
        -DiffCxt:  1.59e-08
    Outer Step:   270      LossTrajs: 0.00029965     ContextsNorm: 0.04822287     ValIndCrit: 0.00054660
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   11
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.91e-10
        -DiffCxt:  5.47e-09
    Outer Step:   280      LossTrajs: 0.00030076     ContextsNorm: 0.04822177     ValIndCrit: 0.00050042
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.12e-08
        -DiffCxt:  2.27e-08
    Outer Step:   290      LossTrajs: 0.00027214     ContextsNorm: 0.04794296     ValIndCrit: 0.00049300
        Saving best model so far ...
        -NbInnerStepsNode:    2
        -NbInnerStepsCxt:   22
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.18e-10
        -DiffCxt:  9.63e-09
    Outer Step:   300      LossTrajs: 0.00025019     ContextsNorm: 0.04803377     ValIndCrit: 0.00043577
        Saving best model so far ...
        -NbInnerStepsNode:   17
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.33e-10
        -DiffCxt:  1.06e-08
    Outer Step:   310      LossTrajs: 0.00023189     ContextsNorm: 0.04801112     ValIndCrit: 0.00042103
        Saving best model so far ...
        -NbInnerStepsNode:   21
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.04e-10
        -DiffCxt:  1.31e-08
    Outer Step:   320      LossTrajs: 0.00023181     ContextsNorm: 0.04783260     ValIndCrit: 0.00039928
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.96e-09
        -DiffCxt:  3.84e-08
    Outer Step:   330      LossTrajs: 0.00022380     ContextsNorm: 0.04773809     ValIndCrit: 0.00038860
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.25e-10
        -DiffCxt:  9.77e-09
    Outer Step:   340      LossTrajs: 0.00022147     ContextsNorm: 0.04770055     ValIndCrit: 0.00035530
        Saving best model so far ...
        -NbInnerStepsNode:   19
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.22e-10
        -DiffCxt:  3.07e-08
    Outer Step:   350      LossTrajs: 0.00023278     ContextsNorm: 0.04811838     ValIndCrit: 0.00035818
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.61e-08
        -DiffCxt:  2.81e-08
    Outer Step:   360      LossTrajs: 0.00019958     ContextsNorm: 0.04808743     ValIndCrit: 0.00032330
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   21
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.16e-09
        -DiffCxt:  7.59e-09
    Outer Step:   370      LossTrajs: 0.00020059     ContextsNorm: 0.04800534     ValIndCrit: 0.00032409
        -NbInnerStepsNode:    2
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.20e-10
        -DiffCxt:  7.82e-09
    Outer Step:   380      LossTrajs: 0.00019171     ContextsNorm: 0.04773479     ValIndCrit: 0.00032134
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   23
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.58e-10
        -DiffCxt:  9.11e-09
    Outer Step:   390      LossTrajs: 0.00018528     ContextsNorm: 0.04791500     ValIndCrit: 0.00030539
        Saving best model so far ...
        -NbInnerStepsNode:   19
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.65e-10
        -DiffCxt:  6.14e-08
    Outer Step:   400      LossTrajs: 0.00017323     ContextsNorm: 0.04770062     ValIndCrit: 0.00029606
        Saving best model so far ...
        -NbInnerStepsNode:    2
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.91e-10
        -DiffCxt:  1.90e-08
    Outer Step:   410      LossTrajs: 0.00017670     ContextsNorm: 0.04773903     ValIndCrit: 0.00030002
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.99e-09
        -DiffCxt:  6.39e-08
    Outer Step:   420      LossTrajs: 0.00017221     ContextsNorm: 0.04776692     ValIndCrit: 0.00029049
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.94e-10
        -DiffCxt:  3.40e-08
    Outer Step:   430      LossTrajs: 0.00017293     ContextsNorm: 0.04792991     ValIndCrit: 0.00026488
        Saving best model so far ...
        -NbInnerStepsNode:   15
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.72e-10
        -DiffCxt:  6.16e-08
    Outer Step:   440      LossTrajs: 0.00014952     ContextsNorm: 0.04806184     ValIndCrit: 0.00023202
        Saving best model so far ...
        -NbInnerStepsNode:    3
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.12e-10
        -DiffCxt:  1.94e-08
    Outer Step:   450      LossTrajs: 0.00016362     ContextsNorm: 0.04786354     ValIndCrit: 0.00023735
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.38e-09
        -DiffCxt:  1.52e-08
    Outer Step:   460      LossTrajs: 0.00015874     ContextsNorm: 0.04795495     ValIndCrit: 0.00023333
        -NbInnerStepsNode:    9
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.27e-10
        -DiffCxt:  1.74e-08
    Outer Step:   470      LossTrajs: 0.00015666     ContextsNorm: 0.04803444     ValIndCrit: 0.00023048
        Saving best model so far ...
        -NbInnerStepsNode:    6
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.39e-10
        -DiffCxt:  2.39e-08
    Outer Step:   480      LossTrajs: 0.00014695     ContextsNorm: 0.04792618     ValIndCrit: 0.00022213
        Saving best model so far ...
        -NbInnerStepsNode:    2
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.51e-10
        -DiffCxt:  9.28e-09
    Outer Step:   490      LossTrajs: 0.00014976     ContextsNorm: 0.04834351     ValIndCrit: 0.00021246
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.80e-09
        -DiffCxt:  3.38e-08
    Outer Step:   499      LossTrajs: 0.00014572     ContextsNorm: 0.04845441     ValIndCrit: 0.00021431
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.51e-10
        -DiffCxt:  1.93e-08

Total gradient descent training time: 1 hours 34 mins 28 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 110237
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 0.00021246144


############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]
==  Begining in-domain visualisation ... ==
    Environment id: 0
    Trajectory id: 17
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./05052024-092750/results_in_domain.png
Testing finished. Figure saved in: ./05052024-092750/results_in_domain_uq.pdf
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.02089248
    Epoch:     1     LossContext: 0.01652092
    Epoch:     2     LossContext: 0.01244161
    Epoch:     3     LossContext: 0.00897422
    Epoch:    10     LossContext: 0.00064107
    Epoch:    20     LossContext: 0.00045005
    Epoch:    30     LossContext: 0.00058595
    Epoch:    40     LossContext: 0.00014889
    Epoch:    50     LossContext: 0.00011752
    Epoch:    60     LossContext: 0.00012547
    Epoch:    70     LossContext: 0.00011250
    Epoch:    80     LossContext: 0.00010559
    Epoch:    90     LossContext: 0.00010393
    Epoch:   100     LossContext: 0.00010349
    Epoch:   110     LossContext: 0.00010349
    Epoch:   120     LossContext: 0.00010348
    Epoch:   130     LossContext: 0.00010348
    Epoch:   140     LossContext: 0.00010347
    Epoch:   150     LossContext: 0.00010347
    Epoch:   160     LossContext: 0.00010347
    Epoch:   170     LossContext: 0.00010347
    Epoch:   180     LossContext: 0.00010347
    Epoch:   190     LossContext: 0.00010347
    Epoch:   200     LossContext: 0.00010347
    Epoch:   210     LossContext: 0.00010347
    Epoch:   220     LossContext: 0.00010347
    Epoch:   230     LossContext: 0.00010347
    Epoch:   240     LossContext: 0.00010347
    Epoch:   250     LossContext: 0.00010347
    Epoch:   260     LossContext: 0.00010347
    Epoch:   270     LossContext: 0.00010347
    Epoch:   280     LossContext: 0.00010347
    Epoch:   290     LossContext: 0.00010347
    Epoch:   300     LossContext: 0.00010347
    Epoch:   310     LossContext: 0.00010347
    Epoch:   320     LossContext: 0.00010347
    Epoch:   330     LossContext: 0.00010347
    Epoch:   340     LossContext: 0.00010347
    Epoch:   350     LossContext: 0.00010347
    Epoch:   360     LossContext: 0.00010347
    Epoch:   370     LossContext: 0.00010347
    Epoch:   380     LossContext: 0.00010347
    Epoch:   390     LossContext: 0.00010347
    Epoch:   400     LossContext: 0.00010347
    Epoch:   410     LossContext: 0.00010347
    Epoch:   420     LossContext: 0.00010347
    Epoch:   430     LossContext: 0.00010347
    Epoch:   440     LossContext: 0.00010347
    Epoch:   450     LossContext: 0.00010347
    Epoch:   460     LossContext: 0.00010347
    Epoch:   470     LossContext: 0.00010347
    Epoch:   480     LossContext: 0.00010347
    Epoch:   490     LossContext: 0.00010347
    Epoch:   500     LossContext: 0.00010347
    Epoch:   510     LossContext: 0.00010347
    Epoch:   520     LossContext: 0.00010347
    Epoch:   530     LossContext: 0.00010347
    Epoch:   540     LossContext: 0.00010347
    Epoch:   550     LossContext: 0.00010347
    Epoch:   560     LossContext: 0.00010347
    Epoch:   570     LossContext: 0.00010347
    Epoch:   580     LossContext: 0.00010347
    Epoch:   590     LossContext: 0.00010347
    Epoch:   600     LossContext: 0.00010347
    Epoch:   610     LossContext: 0.00010347
    Epoch:   620     LossContext: 0.00010347
    Epoch:   630     LossContext: 0.00010347
    Epoch:   640     LossContext: 0.00010347
    Epoch:   650     LossContext: 0.00010347
    Epoch:   660     LossContext: 0.00010347
    Epoch:   670     LossContext: 0.00010347
    Epoch:   680     LossContext: 0.00010347
    Epoch:   690     LossContext: 0.00010347
    Epoch:   700     LossContext: 0.00010347
    Epoch:   710     LossContext: 0.00010347
    Epoch:   720     LossContext: 0.00010347
    Epoch:   730     LossContext: 0.00010347
    Epoch:   740     LossContext: 0.00010347
    Epoch:   750     LossContext: 0.00010347
    Epoch:   760     LossContext: 0.00010347
    Epoch:   770     LossContext: 0.00010347
    Epoch:   780     LossContext: 0.00010347
    Epoch:   790     LossContext: 0.00010347
    Epoch:   800     LossContext: 0.00010347
    Epoch:   810     LossContext: 0.00010347
    Epoch:   820     LossContext: 0.00010347
    Epoch:   830     LossContext: 0.00010347
    Epoch:   840     LossContext: 0.00010347
    Epoch:   850     LossContext: 0.00010347
    Epoch:   860     LossContext: 0.00010347
    Epoch:   870     LossContext: 0.00010347
    Epoch:   880     LossContext: 0.00010347
    Epoch:   890     LossContext: 0.00010347
    Epoch:   900     LossContext: 0.00010347
    Epoch:   910     LossContext: 0.00010347
    Epoch:   920     LossContext: 0.00010347
    Epoch:   930     LossContext: 0.00010347
    Epoch:   940     LossContext: 0.00010347
    Epoch:   950     LossContext: 0.00010347
    Epoch:   960     LossContext: 0.00010347
    Epoch:   970     LossContext: 0.00010347
    Epoch:   980     LossContext: 0.00010347
    Epoch:   990     LossContext: 0.00010347
    Epoch:  1000     LossContext: 0.00010347
    Epoch:  1010     LossContext: 0.00010347
    Epoch:  1020     LossContext: 0.00010347
    Epoch:  1030     LossContext: 0.00010347
    Epoch:  1040     LossContext: 0.00010347
    Epoch:  1050     LossContext: 0.00010347
    Epoch:  1060     LossContext: 0.00010347
    Epoch:  1070     LossContext: 0.00010347
    Epoch:  1080     LossContext: 0.00010347
    Epoch:  1090     LossContext: 0.00010347
    Epoch:  1100     LossContext: 0.00010347
    Epoch:  1110     LossContext: 0.00010347
    Epoch:  1120     LossContext: 0.00010347
    Epoch:  1130     LossContext: 0.00010347
    Epoch:  1140     LossContext: 0.00010347
    Epoch:  1150     LossContext: 0.00010347
    Epoch:  1160     LossContext: 0.00010347
    Epoch:  1170     LossContext: 0.00010347
    Epoch:  1180     LossContext: 0.00010347
    Epoch:  1190     LossContext: 0.00010347
    Epoch:  1200     LossContext: 0.00010347
    Epoch:  1210     LossContext: 0.00010347
    Epoch:  1220     LossContext: 0.00010347
    Epoch:  1230     LossContext: 0.00010347
    Epoch:  1240     LossContext: 0.00010347
    Epoch:  1250     LossContext: 0.00010347
    Epoch:  1260     LossContext: 0.00010347
    Epoch:  1270     LossContext: 0.00010347
    Epoch:  1280     LossContext: 0.00010347
    Epoch:  1290     LossContext: 0.00010347
    Epoch:  1300     LossContext: 0.00010347
    Epoch:  1310     LossContext: 0.00010347
    Epoch:  1320     LossContext: 0.00010347
    Epoch:  1330     LossContext: 0.00010347
    Epoch:  1340     LossContext: 0.00010347
    Epoch:  1350     LossContext: 0.00010347
    Epoch:  1360     LossContext: 0.00010347
    Epoch:  1370     LossContext: 0.00010347
    Epoch:  1380     LossContext: 0.00010347
    Epoch:  1390     LossContext: 0.00010347
    Epoch:  1400     LossContext: 0.00010347
    Epoch:  1410     LossContext: 0.00010347
    Epoch:  1420     LossContext: 0.00010347
    Epoch:  1430     LossContext: 0.00010347
    Epoch:  1440     LossContext: 0.00010347
    Epoch:  1450     LossContext: 0.00010347
    Epoch:  1460     LossContext: 0.00010347
    Epoch:  1470     LossContext: 0.00010347
    Epoch:  1480     LossContext: 0.00010347
    Epoch:  1490     LossContext: 0.00010347
    Epoch:  1499     LossContext: 0.00010347

Gradient descent adaptation time: 0 hours 1 mins 47 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.12934403
    Epoch:     1     LossContext: 0.11780426
    Epoch:     2     LossContext: 0.10555889
    Epoch:     3     LossContext: 0.09268703
    Epoch:    10     LossContext: 0.02202835
    Epoch:    20     LossContext: 0.00224022
    Epoch:    30     LossContext: 0.00129920
    Epoch:    40     LossContext: 0.00079673
    Epoch:    50     LossContext: 0.00052119
    Epoch:    60     LossContext: 0.00039112
    Epoch:    70     LossContext: 0.00028016
    Epoch:    80     LossContext: 0.00023065
    Epoch:    90     LossContext: 0.00022134
    Epoch:   100     LossContext: 0.00022064
    Epoch:   110     LossContext: 0.00021942
    Epoch:   120     LossContext: 0.00021855
    Epoch:   130     LossContext: 0.00021836
    Epoch:   140     LossContext: 0.00021836
    Epoch:   150     LossContext: 0.00021835
    Epoch:   160     LossContext: 0.00021834
    Epoch:   170     LossContext: 0.00021834
    Epoch:   180     LossContext: 0.00021834
    Epoch:   190     LossContext: 0.00021834
    Epoch:   200     LossContext: 0.00021834
    Epoch:   210     LossContext: 0.00021834
    Epoch:   220     LossContext: 0.00021834
    Epoch:   230     LossContext: 0.00021834
    Epoch:   240     LossContext: 0.00021834
    Epoch:   250     LossContext: 0.00021834
    Epoch:   260     LossContext: 0.00021834
    Epoch:   270     LossContext: 0.00021834
    Epoch:   280     LossContext: 0.00021834
    Epoch:   290     LossContext: 0.00021834
    Epoch:   300     LossContext: 0.00021834
    Epoch:   310     LossContext: 0.00021834
    Epoch:   320     LossContext: 0.00021834
    Epoch:   330     LossContext: 0.00021834
    Epoch:   340     LossContext: 0.00021834
    Epoch:   350     LossContext: 0.00021834
    Epoch:   360     LossContext: 0.00021834
    Epoch:   370     LossContext: 0.00021834
    Epoch:   380     LossContext: 0.00021834
    Epoch:   390     LossContext: 0.00021834
    Epoch:   400     LossContext: 0.00021834
    Epoch:   410     LossContext: 0.00021834
    Epoch:   420     LossContext: 0.00021834
    Epoch:   430     LossContext: 0.00021834
    Epoch:   440     LossContext: 0.00021834
    Epoch:   450     LossContext: 0.00021834
    Epoch:   460     LossContext: 0.00021834
    Epoch:   470     LossContext: 0.00021834
    Epoch:   480     LossContext: 0.00021834
    Epoch:   490     LossContext: 0.00021834
    Epoch:   500     LossContext: 0.00021834
    Epoch:   510     LossContext: 0.00021834
    Epoch:   520     LossContext: 0.00021834
    Epoch:   530     LossContext: 0.00021834
    Epoch:   540     LossContext: 0.00021834
    Epoch:   550     LossContext: 0.00021834
    Epoch:   560     LossContext: 0.00021834
    Epoch:   570     LossContext: 0.00021834
    Epoch:   580     LossContext: 0.00021834
    Epoch:   590     LossContext: 0.00021834
    Epoch:   600     LossContext: 0.00021834
    Epoch:   610     LossContext: 0.00021834
    Epoch:   620     LossContext: 0.00021834
    Epoch:   630     LossContext: 0.00021834
    Epoch:   640     LossContext: 0.00021834
    Epoch:   650     LossContext: 0.00021834
    Epoch:   660     LossContext: 0.00021834
    Epoch:   670     LossContext: 0.00021834
    Epoch:   680     LossContext: 0.00021834
    Epoch:   690     LossContext: 0.00021834
    Epoch:   700     LossContext: 0.00021834
    Epoch:   710     LossContext: 0.00021834
    Epoch:   720     LossContext: 0.00021834
    Epoch:   730     LossContext: 0.00021834
    Epoch:   740     LossContext: 0.00021834
    Epoch:   750     LossContext: 0.00021834
    Epoch:   760     LossContext: 0.00021834
    Epoch:   770     LossContext: 0.00021834
    Epoch:   780     LossContext: 0.00021834
    Epoch:   790     LossContext: 0.00021834
    Epoch:   800     LossContext: 0.00021834
    Epoch:   810     LossContext: 0.00021834
    Epoch:   820     LossContext: 0.00021834
    Epoch:   830     LossContext: 0.00021834
    Epoch:   840     LossContext: 0.00021834
    Epoch:   850     LossContext: 0.00021834
    Epoch:   860     LossContext: 0.00021834
    Epoch:   870     LossContext: 0.00021834
    Epoch:   880     LossContext: 0.00021834
    Epoch:   890     LossContext: 0.00021834
    Epoch:   900     LossContext: 0.00021834
    Epoch:   910     LossContext: 0.00021834
    Epoch:   920     LossContext: 0.00021834
    Epoch:   930     LossContext: 0.00021834
    Epoch:   940     LossContext: 0.00021834
    Epoch:   950     LossContext: 0.00021834
    Epoch:   960     LossContext: 0.00021834
    Epoch:   970     LossContext: 0.00021834
    Epoch:   980     LossContext: 0.00021834
    Epoch:   990     LossContext: 0.00021834
    Epoch:  1000     LossContext: 0.00021834
    Epoch:  1010     LossContext: 0.00021834
    Epoch:  1020     LossContext: 0.00021834
    Epoch:  1030     LossContext: 0.00021834
    Epoch:  1040     LossContext: 0.00021834
    Epoch:  1050     LossContext: 0.00021834
    Epoch:  1060     LossContext: 0.00021834
    Epoch:  1070     LossContext: 0.00021834
    Epoch:  1080     LossContext: 0.00021834
    Epoch:  1090     LossContext: 0.00021834
    Epoch:  1100     LossContext: 0.00021834
    Epoch:  1110     LossContext: 0.00021834
    Epoch:  1120     LossContext: 0.00021834
    Epoch:  1130     LossContext: 0.00021834
    Epoch:  1140     LossContext: 0.00021834
    Epoch:  1150     LossContext: 0.00021834
    Epoch:  1160     LossContext: 0.00021834
    Epoch:  1170     LossContext: 0.00021834
    Epoch:  1180     LossContext: 0.00021834
    Epoch:  1190     LossContext: 0.00021834
    Epoch:  1200     LossContext: 0.00021834
    Epoch:  1210     LossContext: 0.00021834
    Epoch:  1220     LossContext: 0.00021834
    Epoch:  1230     LossContext: 0.00021834
    Epoch:  1240     LossContext: 0.00021834
    Epoch:  1250     LossContext: 0.00021834
    Epoch:  1260     LossContext: 0.00021834
    Epoch:  1270     LossContext: 0.00021834
    Epoch:  1280     LossContext: 0.00021834
    Epoch:  1290     LossContext: 0.00021834
    Epoch:  1300     LossContext: 0.00021834
    Epoch:  1310     LossContext: 0.00021834
    Epoch:  1320     LossContext: 0.00021834
    Epoch:  1330     LossContext: 0.00021834
    Epoch:  1340     LossContext: 0.00021834
    Epoch:  1350     LossContext: 0.00021834
    Epoch:  1360     LossContext: 0.00021834
    Epoch:  1370     LossContext: 0.00021834
    Epoch:  1380     LossContext: 0.00021834
    Epoch:  1390     LossContext: 0.00021834
    Epoch:  1400     LossContext: 0.00021834
    Epoch:  1410     LossContext: 0.00021834
    Epoch:  1420     LossContext: 0.00021834
    Epoch:  1430     LossContext: 0.00021834
    Epoch:  1440     LossContext: 0.00021834
    Epoch:  1450     LossContext: 0.00021834
    Epoch:  1460     LossContext: 0.00021834
    Epoch:  1470     LossContext: 0.00021834
    Epoch:  1480     LossContext: 0.00021834
    Epoch:  1490     LossContext: 0.00021834
    Epoch:  1499     LossContext: 0.00021834

Gradient descent adaptation time: 0 hours 1 mins 33 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.04386384
    Epoch:     1     LossContext: 0.03689199
    Epoch:     2     LossContext: 0.03063339
    Epoch:     3     LossContext: 0.02509645
    Epoch:    10     LossContext: 0.00408063
    Epoch:    20     LossContext: 0.00028091
    Epoch:    30     LossContext: 0.00069987
    Epoch:    40     LossContext: 0.00064221
    Epoch:    50     LossContext: 0.00033090
    Epoch:    60     LossContext: 0.00019335
    Epoch:    70     LossContext: 0.00019146
    Epoch:    80     LossContext: 0.00019351
    Epoch:    90     LossContext: 0.00018641
    Epoch:   100     LossContext: 0.00018523
    Epoch:   110     LossContext: 0.00018549
    Epoch:   120     LossContext: 0.00018521
    Epoch:   130     LossContext: 0.00018515
    Epoch:   140     LossContext: 0.00018516
    Epoch:   150     LossContext: 0.00018514
    Epoch:   160     LossContext: 0.00018514
    Epoch:   170     LossContext: 0.00018514
    Epoch:   180     LossContext: 0.00018514
    Epoch:   190     LossContext: 0.00018514
    Epoch:   200     LossContext: 0.00018514
    Epoch:   210     LossContext: 0.00018514
    Epoch:   220     LossContext: 0.00018514
    Epoch:   230     LossContext: 0.00018514
    Epoch:   240     LossContext: 0.00018514
    Epoch:   250     LossContext: 0.00018514
    Epoch:   260     LossContext: 0.00018514
    Epoch:   270     LossContext: 0.00018514
    Epoch:   280     LossContext: 0.00018514
    Epoch:   290     LossContext: 0.00018514
    Epoch:   300     LossContext: 0.00018514
    Epoch:   310     LossContext: 0.00018514
    Epoch:   320     LossContext: 0.00018514
    Epoch:   330     LossContext: 0.00018514
    Epoch:   340     LossContext: 0.00018514
    Epoch:   350     LossContext: 0.00018514
    Epoch:   360     LossContext: 0.00018514
    Epoch:   370     LossContext: 0.00018514
    Epoch:   380     LossContext: 0.00018514
    Epoch:   390     LossContext: 0.00018514
    Epoch:   400     LossContext: 0.00018514
    Epoch:   410     LossContext: 0.00018514
    Epoch:   420     LossContext: 0.00018514
    Epoch:   430     LossContext: 0.00018514
    Epoch:   440     LossContext: 0.00018514
    Epoch:   450     LossContext: 0.00018514
    Epoch:   460     LossContext: 0.00018514
    Epoch:   470     LossContext: 0.00018514
    Epoch:   480     LossContext: 0.00018514
    Epoch:   490     LossContext: 0.00018514
    Epoch:   500     LossContext: 0.00018514
    Epoch:   510     LossContext: 0.00018514
    Epoch:   520     LossContext: 0.00018514
    Epoch:   530     LossContext: 0.00018514
    Epoch:   540     LossContext: 0.00018514
    Epoch:   550     LossContext: 0.00018514
    Epoch:   560     LossContext: 0.00018514
    Epoch:   570     LossContext: 0.00018514
    Epoch:   580     LossContext: 0.00018514
    Epoch:   590     LossContext: 0.00018514
    Epoch:   600     LossContext: 0.00018514
    Epoch:   610     LossContext: 0.00018514
    Epoch:   620     LossContext: 0.00018514
    Epoch:   630     LossContext: 0.00018514
    Epoch:   640     LossContext: 0.00018514
    Epoch:   650     LossContext: 0.00018514
    Epoch:   660     LossContext: 0.00018514
    Epoch:   670     LossContext: 0.00018514
    Epoch:   680     LossContext: 0.00018514
    Epoch:   690     LossContext: 0.00018514
    Epoch:   700     LossContext: 0.00018514
    Epoch:   710     LossContext: 0.00018514
    Epoch:   720     LossContext: 0.00018514
    Epoch:   730     LossContext: 0.00018514
    Epoch:   740     LossContext: 0.00018514
    Epoch:   750     LossContext: 0.00018514
    Epoch:   760     LossContext: 0.00018514
    Epoch:   770     LossContext: 0.00018514
    Epoch:   780     LossContext: 0.00018514
    Epoch:   790     LossContext: 0.00018514
    Epoch:   800     LossContext: 0.00018514
    Epoch:   810     LossContext: 0.00018514
    Epoch:   820     LossContext: 0.00018514
    Epoch:   830     LossContext: 0.00018514
    Epoch:   840     LossContext: 0.00018514
    Epoch:   850     LossContext: 0.00018514
    Epoch:   860     LossContext: 0.00018514
    Epoch:   870     LossContext: 0.00018514
    Epoch:   880     LossContext: 0.00018514
    Epoch:   890     LossContext: 0.00018514
    Epoch:   900     LossContext: 0.00018514
    Epoch:   910     LossContext: 0.00018514
    Epoch:   920     LossContext: 0.00018514
    Epoch:   930     LossContext: 0.00018514
    Epoch:   940     LossContext: 0.00018514
    Epoch:   950     LossContext: 0.00018514
    Epoch:   960     LossContext: 0.00018514
    Epoch:   970     LossContext: 0.00018514
    Epoch:   980     LossContext: 0.00018514
    Epoch:   990     LossContext: 0.00018514
    Epoch:  1000     LossContext: 0.00018514
    Epoch:  1010     LossContext: 0.00018514
    Epoch:  1020     LossContext: 0.00018514
    Epoch:  1030     LossContext: 0.00018514
    Epoch:  1040     LossContext: 0.00018514
    Epoch:  1050     LossContext: 0.00018514
    Epoch:  1060     LossContext: 0.00018514
    Epoch:  1070     LossContext: 0.00018514
    Epoch:  1080     LossContext: 0.00018514
    Epoch:  1090     LossContext: 0.00018514
    Epoch:  1100     LossContext: 0.00018514
    Epoch:  1110     LossContext: 0.00018514
    Epoch:  1120     LossContext: 0.00018514
    Epoch:  1130     LossContext: 0.00018514
    Epoch:  1140     LossContext: 0.00018514
    Epoch:  1150     LossContext: 0.00018514
    Epoch:  1160     LossContext: 0.00018514
    Epoch:  1170     LossContext: 0.00018514
    Epoch:  1180     LossContext: 0.00018514
    Epoch:  1190     LossContext: 0.00018514
    Epoch:  1200     LossContext: 0.00018514
    Epoch:  1210     LossContext: 0.00018514
    Epoch:  1220     LossContext: 0.00018514
    Epoch:  1230     LossContext: 0.00018514
    Epoch:  1240     LossContext: 0.00018514
    Epoch:  1250     LossContext: 0.00018514
    Epoch:  1260     LossContext: 0.00018514
    Epoch:  1270     LossContext: 0.00018514
    Epoch:  1280     LossContext: 0.00018514
    Epoch:  1290     LossContext: 0.00018514
    Epoch:  1300     LossContext: 0.00018514
    Epoch:  1310     LossContext: 0.00018514
    Epoch:  1320     LossContext: 0.00018514
    Epoch:  1330     LossContext: 0.00018514
    Epoch:  1340     LossContext: 0.00018514
    Epoch:  1350     LossContext: 0.00018514
    Epoch:  1360     LossContext: 0.00018514
    Epoch:  1370     LossContext: 0.00018514
    Epoch:  1380     LossContext: 0.00018514
    Epoch:  1390     LossContext: 0.00018514
    Epoch:  1400     LossContext: 0.00018514
    Epoch:  1410     LossContext: 0.00018514
    Epoch:  1420     LossContext: 0.00018514
    Epoch:  1430     LossContext: 0.00018514
    Epoch:  1440     LossContext: 0.00018514
    Epoch:  1450     LossContext: 0.00018514
    Epoch:  1460     LossContext: 0.00018514
    Epoch:  1470     LossContext: 0.00018514
    Epoch:  1480     LossContext: 0.00018514
    Epoch:  1490     LossContext: 0.00018514
    Epoch:  1499     LossContext: 0.00018514

Gradient descent adaptation time: 0 hours 1 mins 33 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.02771924
    Epoch:     1     LossContext: 0.02319386
    Epoch:     2     LossContext: 0.01947586
    Epoch:     3     LossContext: 0.01643076
    Epoch:    10     LossContext: 0.00373353
    Epoch:    20     LossContext: 0.00056790
    Epoch:    30     LossContext: 0.00049099
    Epoch:    40     LossContext: 0.00055242
    Epoch:    50     LossContext: 0.00050947
    Epoch:    60     LossContext: 0.00044054
    Epoch:    70     LossContext: 0.00040539
    Epoch:    80     LossContext: 0.00039901
    Epoch:    90     LossContext: 0.00039978
    Epoch:   100     LossContext: 0.00039953
    Epoch:   110     LossContext: 0.00039897
    Epoch:   120     LossContext: 0.00039883
    Epoch:   130     LossContext: 0.00039885
    Epoch:   140     LossContext: 0.00039884
    Epoch:   150     LossContext: 0.00039883
    Epoch:   160     LossContext: 0.00039883
    Epoch:   170     LossContext: 0.00039883
    Epoch:   180     LossContext: 0.00039883
    Epoch:   190     LossContext: 0.00039883
    Epoch:   200     LossContext: 0.00039883
    Epoch:   210     LossContext: 0.00039883
    Epoch:   220     LossContext: 0.00039883
    Epoch:   230     LossContext: 0.00039883
    Epoch:   240     LossContext: 0.00039883
    Epoch:   250     LossContext: 0.00039883
    Epoch:   260     LossContext: 0.00039883
    Epoch:   270     LossContext: 0.00039883
    Epoch:   280     LossContext: 0.00039883
    Epoch:   290     LossContext: 0.00039883
    Epoch:   300     LossContext: 0.00039883
    Epoch:   310     LossContext: 0.00039883
    Epoch:   320     LossContext: 0.00039883
    Epoch:   330     LossContext: 0.00039883
    Epoch:   340     LossContext: 0.00039883
    Epoch:   350     LossContext: 0.00039883
    Epoch:   360     LossContext: 0.00039883
    Epoch:   370     LossContext: 0.00039883
    Epoch:   380     LossContext: 0.00039883
    Epoch:   390     LossContext: 0.00039883
    Epoch:   400     LossContext: 0.00039883
    Epoch:   410     LossContext: 0.00039883
    Epoch:   420     LossContext: 0.00039883
    Epoch:   430     LossContext: 0.00039883
    Epoch:   440     LossContext: 0.00039883
    Epoch:   450     LossContext: 0.00039883
    Epoch:   460     LossContext: 0.00039883
    Epoch:   470     LossContext: 0.00039883
    Epoch:   480     LossContext: 0.00039883
    Epoch:   490     LossContext: 0.00039883
    Epoch:   500     LossContext: 0.00039883
    Epoch:   510     LossContext: 0.00039883
    Epoch:   520     LossContext: 0.00039883
    Epoch:   530     LossContext: 0.00039883
    Epoch:   540     LossContext: 0.00039883
    Epoch:   550     LossContext: 0.00039883
    Epoch:   560     LossContext: 0.00039883
    Epoch:   570     LossContext: 0.00039883
    Epoch:   580     LossContext: 0.00039883
    Epoch:   590     LossContext: 0.00039883
    Epoch:   600     LossContext: 0.00039883
    Epoch:   610     LossContext: 0.00039883
    Epoch:   620     LossContext: 0.00039883
    Epoch:   630     LossContext: 0.00039883
    Epoch:   640     LossContext: 0.00039883
    Epoch:   650     LossContext: 0.00039883
    Epoch:   660     LossContext: 0.00039883
    Epoch:   670     LossContext: 0.00039883
    Epoch:   680     LossContext: 0.00039883
    Epoch:   690     LossContext: 0.00039883
    Epoch:   700     LossContext: 0.00039883
    Epoch:   710     LossContext: 0.00039883
    Epoch:   720     LossContext: 0.00039883
    Epoch:   730     LossContext: 0.00039883
    Epoch:   740     LossContext: 0.00039883
    Epoch:   750     LossContext: 0.00039883
    Epoch:   760     LossContext: 0.00039883
    Epoch:   770     LossContext: 0.00039883
    Epoch:   780     LossContext: 0.00039883
    Epoch:   790     LossContext: 0.00039883
    Epoch:   800     LossContext: 0.00039883
    Epoch:   810     LossContext: 0.00039883
    Epoch:   820     LossContext: 0.00039883
    Epoch:   830     LossContext: 0.00039883
    Epoch:   840     LossContext: 0.00039883
    Epoch:   850     LossContext: 0.00039883
    Epoch:   860     LossContext: 0.00039883
    Epoch:   870     LossContext: 0.00039883
    Epoch:   880     LossContext: 0.00039883
    Epoch:   890     LossContext: 0.00039883
    Epoch:   900     LossContext: 0.00039883
    Epoch:   910     LossContext: 0.00039883
    Epoch:   920     LossContext: 0.00039883
    Epoch:   930     LossContext: 0.00039883
    Epoch:   940     LossContext: 0.00039883
    Epoch:   950     LossContext: 0.00039883
    Epoch:   960     LossContext: 0.00039883
    Epoch:   970     LossContext: 0.00039883
    Epoch:   980     LossContext: 0.00039883
    Epoch:   990     LossContext: 0.00039883
    Epoch:  1000     LossContext: 0.00039883
    Epoch:  1010     LossContext: 0.00039883
    Epoch:  1020     LossContext: 0.00039883
    Epoch:  1030     LossContext: 0.00039883
    Epoch:  1040     LossContext: 0.00039883
    Epoch:  1050     LossContext: 0.00039883
    Epoch:  1060     LossContext: 0.00039883
    Epoch:  1070     LossContext: 0.00039883
    Epoch:  1080     LossContext: 0.00039883
    Epoch:  1090     LossContext: 0.00039883
    Epoch:  1100     LossContext: 0.00039883
    Epoch:  1110     LossContext: 0.00039883
    Epoch:  1120     LossContext: 0.00039883
    Epoch:  1130     LossContext: 0.00039883
    Epoch:  1140     LossContext: 0.00039883
    Epoch:  1150     LossContext: 0.00039883
    Epoch:  1160     LossContext: 0.00039883
    Epoch:  1170     LossContext: 0.00039883
    Epoch:  1180     LossContext: 0.00039883
    Epoch:  1190     LossContext: 0.00039883
    Epoch:  1200     LossContext: 0.00039883
    Epoch:  1210     LossContext: 0.00039883
    Epoch:  1220     LossContext: 0.00039883
    Epoch:  1230     LossContext: 0.00039883
    Epoch:  1240     LossContext: 0.00039883
    Epoch:  1250     LossContext: 0.00039883
    Epoch:  1260     LossContext: 0.00039883
    Epoch:  1270     LossContext: 0.00039883
    Epoch:  1280     LossContext: 0.00039883
    Epoch:  1290     LossContext: 0.00039883
    Epoch:  1300     LossContext: 0.00039883
    Epoch:  1310     LossContext: 0.00039883
    Epoch:  1320     LossContext: 0.00039883
    Epoch:  1330     LossContext: 0.00039883
    Epoch:  1340     LossContext: 0.00039883
    Epoch:  1350     LossContext: 0.00039883
    Epoch:  1360     LossContext: 0.00039883
    Epoch:  1370     LossContext: 0.00039883
    Epoch:  1380     LossContext: 0.00039883
    Epoch:  1390     LossContext: 0.00039883
    Epoch:  1400     LossContext: 0.00039883
    Epoch:  1410     LossContext: 0.00039883
    Epoch:  1420     LossContext: 0.00039883
    Epoch:  1430     LossContext: 0.00039883
    Epoch:  1440     LossContext: 0.00039883
    Epoch:  1450     LossContext: 0.00039883
    Epoch:  1460     LossContext: 0.00039883
    Epoch:  1470     LossContext: 0.00039883
    Epoch:  1480     LossContext: 0.00039883
    Epoch:  1490     LossContext: 0.00039883
    Epoch:  1499     LossContext: 0.00039883

Gradient descent adaptation time: 0 hours 1 mins 33 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./05052024-092750/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 0.00026362634

==  Begining out-of-distribution visualisation ... ==
    Environment id: 0
    Trajectory id: 0
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./05052024-092750/adapt/results_ood.png
Testing finished. Figure saved in: ./05052024-092750/adapt/results_ood_uq.pdf

======================= DONE ======================== 

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

========= STARTING =========
Working with a context size of 2^2 = 4
Run folder created successfuly: ./05052024-110926/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 110943
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 110943
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 47120 


WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed


=== Beginning training with proximal alternating minimization ... ===
    Number of examples in a batch: 4
    Maximum number of steps per inner minimization: 25
    Maximum number of outer minimizations: 500
    Maximum total number of training steps: 12500

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Outer Step:     0      LossTrajs: 1.81043029     ContextsNorm: 0.00000000     ValIndCrit: 1.69849944
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.59e-06
        -DiffCxt:  inf
    Outer Step:     1      LossTrajs: 1.51415193     ContextsNorm: 0.00010805     ValIndCrit: 1.41881227
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.54e-06
        -DiffCxt:  3.24e-03
    Outer Step:     2      LossTrajs: 1.17332292     ContextsNorm: 0.00014515     ValIndCrit: 1.09586036
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.38e-06
        -DiffCxt:  2.21e-03
    Outer Step:     3      LossTrajs: 0.67210549     ContextsNorm: 0.00047194     ValIndCrit: 0.63062692
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.22e-06
        -DiffCxt:  1.08e-03
    Outer Step:    10      LossTrajs: 0.25470787     ContextsNorm: 0.00165054     ValIndCrit: 0.30099317
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.19e-07
        -DiffCxt:  2.34e-05
    Outer Step:    20      LossTrajs: 0.17007416     ContextsNorm: 0.00310088     ValIndCrit: 0.18600045
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.25e-07
        -DiffCxt:  1.19e-05
    Outer Step:    30      LossTrajs: 0.08711986     ContextsNorm: 0.00612873     ValIndCrit: 0.09109799
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.89e-08
        -DiffCxt:  5.52e-06
    Outer Step:    40      LossTrajs: 0.07495676     ContextsNorm: 0.01044829     ValIndCrit: 0.07856613
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.88e-08
        -DiffCxt:  3.68e-06
    Outer Step:    50      LossTrajs: 0.06770040     ContextsNorm: 0.02246431     ValIndCrit: 0.07045320
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.15e-07
        -DiffCxt:  4.17e-06
    Outer Step:    60      LossTrajs: 0.04613422     ContextsNorm: 0.03416202     ValIndCrit: 0.04962738
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.14e-08
        -DiffCxt:  3.18e-07
    Outer Step:    70      LossTrajs: 0.03598613     ContextsNorm: 0.03524419     ValIndCrit: 0.04105024
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.64e-08
        -DiffCxt:  1.40e-07
    Outer Step:    80      LossTrajs: 0.03056845     ContextsNorm: 0.03762168     ValIndCrit: 0.03637799
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.40e-08
        -DiffCxt:  1.39e-07
    Outer Step:    90      LossTrajs: 0.02930985     ContextsNorm: 0.03857268     ValIndCrit: 0.03578998
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.73e-09
        -DiffCxt:  6.22e-08
    Outer Step:   100      LossTrajs: 0.02822630     ContextsNorm: 0.03848614     ValIndCrit: 0.03578777
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.87e-08
        -DiffCxt:  9.30e-08
    Outer Step:   110      LossTrajs: 0.02668450     ContextsNorm: 0.04041416     ValIndCrit: 0.03519382
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.18e-08
        -DiffCxt:  1.71e-07
    Outer Step:   120      LossTrajs: 0.02467854     ContextsNorm: 0.04226590     ValIndCrit: 0.03312362
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.21e-08
        -DiffCxt:  2.33e-07
    Outer Step:   130      LossTrajs: 0.01860185     ContextsNorm: 0.04811957     ValIndCrit: 0.02708156
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.61e-07
        -DiffCxt:  1.02e-06
    Outer Step:   140      LossTrajs: 0.00651908     ContextsNorm: 0.04852538     ValIndCrit: 0.01017032
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.06e-08
        -DiffCxt:  1.85e-07
    Outer Step:   150      LossTrajs: 0.00355140     ContextsNorm: 0.04815024     ValIndCrit: 0.00530740
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.44e-08
        -DiffCxt:  1.40e-07
    Outer Step:   160      LossTrajs: 0.00140958     ContextsNorm: 0.04832740     ValIndCrit: 0.00226832
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.19e-08
        -DiffCxt:  3.17e-08
    Outer Step:   170      LossTrajs: 0.00099838     ContextsNorm: 0.04813069     ValIndCrit: 0.00170708
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.43e-09
        -DiffCxt:  1.71e-08
    Outer Step:   180      LossTrajs: 0.00078489     ContextsNorm: 0.04801989     ValIndCrit: 0.00135783
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.42e-09
        -DiffCxt:  3.09e-08
    Outer Step:   190      LossTrajs: 0.00064253     ContextsNorm: 0.04741718     ValIndCrit: 0.00111989
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.30e-09
        -DiffCxt:  4.26e-08
    Outer Step:   200      LossTrajs: 0.00050789     ContextsNorm: 0.04718906     ValIndCrit: 0.00089642
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.26e-09
        -DiffCxt:  1.58e-08
    Outer Step:   210      LossTrajs: 0.00041881     ContextsNorm: 0.04714472     ValIndCrit: 0.00070195
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   24
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.08e-09
        -DiffCxt:  9.90e-09
    Outer Step:   220      LossTrajs: 0.00036677     ContextsNorm: 0.04695657     ValIndCrit: 0.00059807
        Saving best model so far ...
        -NbInnerStepsNode:    4
        -NbInnerStepsCxt:   12
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.38e-10
        -DiffCxt:  7.33e-09
    Outer Step:   230      LossTrajs: 0.00031377     ContextsNorm: 0.04693515     ValIndCrit: 0.00050127
        Saving best model so far ...
        -NbInnerStepsNode:   24
        -NbInnerStepsCxt:   21
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.35e-10
        -DiffCxt:  8.83e-09
    Outer Step:   240      LossTrajs: 0.00029765     ContextsNorm: 0.04688539     ValIndCrit: 0.00045630
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.58e-08
        -DiffCxt:  2.47e-08
    Outer Step:   250      LossTrajs: 0.00023804     ContextsNorm: 0.04691922     ValIndCrit: 0.00038818
        Saving best model so far ...
        -NbInnerStepsNode:   17
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.58e-10
        -DiffCxt:  4.48e-08
    Outer Step:   260      LossTrajs: 0.00022646     ContextsNorm: 0.04677820     ValIndCrit: 0.00035186
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.64e-09
        -DiffCxt:  2.08e-08
    Outer Step:   270      LossTrajs: 0.00021497     ContextsNorm: 0.04675507     ValIndCrit: 0.00032427
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.46e-09
        -DiffCxt:  2.22e-08
    Outer Step:   280      LossTrajs: 0.00020829     ContextsNorm: 0.04677726     ValIndCrit: 0.00028976
        Saving best model so far ...
        -NbInnerStepsNode:   14
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.14e-10
        -DiffCxt:  5.21e-08
    Outer Step:   290      LossTrajs: 0.00017515     ContextsNorm: 0.04663479     ValIndCrit: 0.00028023
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.74e-09
        -DiffCxt:  6.81e-08
    Outer Step:   300      LossTrajs: 0.00015613     ContextsNorm: 0.04647801     ValIndCrit: 0.00024642
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.63e-09
        -DiffCxt:  5.27e-08
    Outer Step:   310      LossTrajs: 0.00015650     ContextsNorm: 0.04629156     ValIndCrit: 0.00022768
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   14
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.46e-10
        -DiffCxt:  9.07e-09
    Outer Step:   320      LossTrajs: 0.00015224     ContextsNorm: 0.04624905     ValIndCrit: 0.00021646
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.23e-08
        -DiffCxt:  8.61e-08
    Outer Step:   330      LossTrajs: 0.00014650     ContextsNorm: 0.04628234     ValIndCrit: 0.00019919
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.86e-09
        -DiffCxt:  3.73e-08
    Outer Step:   340      LossTrajs: 0.00013587     ContextsNorm: 0.04615128     ValIndCrit: 0.00018035
        Saving best model so far ...
        -NbInnerStepsNode:    2
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.23e-10
        -DiffCxt:  9.45e-09
    Outer Step:   350      LossTrajs: 0.00014156     ContextsNorm: 0.04617110     ValIndCrit: 0.00017420
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.16e-09
        -DiffCxt:  3.80e-08
    Outer Step:   360      LossTrajs: 0.00011687     ContextsNorm: 0.04612114     ValIndCrit: 0.00015962
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    7
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.73e-10
        -DiffCxt:  9.21e-09
    Outer Step:   370      LossTrajs: 0.00014666     ContextsNorm: 0.04611736     ValIndCrit: 0.00015671
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.34e-08
        -DiffCxt:  3.66e-08
    Outer Step:   380      LossTrajs: 0.00011805     ContextsNorm: 0.04623451     ValIndCrit: 0.00015751
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.51e-09
        -DiffCxt:  3.93e-08
    Outer Step:   390      LossTrajs: 0.00010851     ContextsNorm: 0.04611554     ValIndCrit: 0.00013872
        Saving best model so far ...
        -NbInnerStepsNode:    3
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.62e-10
        -DiffCxt:  1.10e-08
    Outer Step:   400      LossTrajs: 0.00011226     ContextsNorm: 0.04588396     ValIndCrit: 0.00014309
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.39e-09
        -DiffCxt:  5.63e-08
    Outer Step:   410      LossTrajs: 0.00011459     ContextsNorm: 0.04583319     ValIndCrit: 0.00013145
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.99e-09
        -DiffCxt:  3.34e-08
    Outer Step:   420      LossTrajs: 0.00010748     ContextsNorm: 0.04584833     ValIndCrit: 0.00013183
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.87e-09
        -DiffCxt:  4.75e-08
    Outer Step:   430      LossTrajs: 0.00010167     ContextsNorm: 0.04584850     ValIndCrit: 0.00012048
        Saving best model so far ...
        -NbInnerStepsNode:    2
        -NbInnerStepsCxt:   23
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.87e-10
        -DiffCxt:  7.94e-09
    Outer Step:   440      LossTrajs: 0.00011122     ContextsNorm: 0.04570092     ValIndCrit: 0.00011763
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.43e-08
        -DiffCxt:  2.42e-08
    Outer Step:   450      LossTrajs: 0.00010513     ContextsNorm: 0.04545989     ValIndCrit: 0.00011864
        -NbInnerStepsNode:    6
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.53e-10
        -DiffCxt:  4.83e-08
    Outer Step:   460      LossTrajs: 0.00009671     ContextsNorm: 0.04533369     ValIndCrit: 0.00011331
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   23
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.12e-10
        -DiffCxt:  6.49e-09
    Outer Step:   470      LossTrajs: 0.00010871     ContextsNorm: 0.04534143     ValIndCrit: 0.00011296
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   23
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.24e-09
        -DiffCxt:  7.74e-09
    Outer Step:   480      LossTrajs: 0.00010503     ContextsNorm: 0.04522581     ValIndCrit: 0.00010524
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.41e-08
        -DiffCxt:  1.42e-08
    Outer Step:   490      LossTrajs: 0.00009015     ContextsNorm: 0.04519952     ValIndCrit: 0.00010137
        Saving best model so far ...
        -NbInnerStepsNode:   15
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.44e-10
        -DiffCxt:  1.06e-08
    Outer Step:   499      LossTrajs: 0.00009051     ContextsNorm: 0.04508351     ValIndCrit: 0.00009778
        Saving best model so far ...
        -NbInnerStepsNode:    3
        -NbInnerStepsCxt:   23
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.12e-10
        -DiffCxt:  9.70e-09

Total gradient descent training time: 1 hours 45 mins 52 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 125536
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 9.777795e-05


############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]
==  Begining in-domain visualisation ... ==
    Environment id: 0
    Trajectory id: 5
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./05052024-110926/results_in_domain.png
Testing finished. Figure saved in: ./05052024-110926/results_in_domain_uq.pdf
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.01996643
    Epoch:     1     LossContext: 0.01545664
    Epoch:     2     LossContext: 0.01107727
    Epoch:     3     LossContext: 0.00705447
    Epoch:    10     LossContext: 0.00294975
    Epoch:    20     LossContext: 0.00025552
    Epoch:    30     LossContext: 0.00040792
    Epoch:    40     LossContext: 0.00025819
    Epoch:    50     LossContext: 0.00011011
    Epoch:    60     LossContext: 0.00008050
    Epoch:    70     LossContext: 0.00008046
    Epoch:    80     LossContext: 0.00008047
    Epoch:    90     LossContext: 0.00007960
    Epoch:   100     LossContext: 0.00007885
    Epoch:   110     LossContext: 0.00007847
    Epoch:   120     LossContext: 0.00007825
    Epoch:   130     LossContext: 0.00007807
    Epoch:   140     LossContext: 0.00007791
    Epoch:   150     LossContext: 0.00007774
    Epoch:   160     LossContext: 0.00007758
    Epoch:   170     LossContext: 0.00007745
    Epoch:   180     LossContext: 0.00007736
    Epoch:   190     LossContext: 0.00007727
    Epoch:   200     LossContext: 0.00007718
    Epoch:   210     LossContext: 0.00007708
    Epoch:   220     LossContext: 0.00007699
    Epoch:   230     LossContext: 0.00007689
    Epoch:   240     LossContext: 0.00007678
    Epoch:   250     LossContext: 0.00007668
    Epoch:   260     LossContext: 0.00007657
    Epoch:   270     LossContext: 0.00007647
    Epoch:   280     LossContext: 0.00007635
    Epoch:   290     LossContext: 0.00007624
    Epoch:   300     LossContext: 0.00007612
    Epoch:   310     LossContext: 0.00007601
    Epoch:   320     LossContext: 0.00007589
    Epoch:   330     LossContext: 0.00007576
    Epoch:   340     LossContext: 0.00007568
    Epoch:   350     LossContext: 0.00007562
    Epoch:   360     LossContext: 0.00007556
    Epoch:   370     LossContext: 0.00007549
    Epoch:   380     LossContext: 0.00007542
    Epoch:   390     LossContext: 0.00007536
    Epoch:   400     LossContext: 0.00007529
    Epoch:   410     LossContext: 0.00007522
    Epoch:   420     LossContext: 0.00007515
    Epoch:   430     LossContext: 0.00007508
    Epoch:   440     LossContext: 0.00007500
    Epoch:   450     LossContext: 0.00007493
    Epoch:   460     LossContext: 0.00007485
    Epoch:   470     LossContext: 0.00007478
    Epoch:   480     LossContext: 0.00007470
    Epoch:   490     LossContext: 0.00007462
    Epoch:   500     LossContext: 0.00007455
    Epoch:   510     LossContext: 0.00007447
    Epoch:   520     LossContext: 0.00007439
    Epoch:   530     LossContext: 0.00007430
    Epoch:   540     LossContext: 0.00007422
    Epoch:   550     LossContext: 0.00007414
    Epoch:   560     LossContext: 0.00007405
    Epoch:   570     LossContext: 0.00007397
    Epoch:   580     LossContext: 0.00007388
    Epoch:   590     LossContext: 0.00007379
    Epoch:   600     LossContext: 0.00007370
    Epoch:   610     LossContext: 0.00007362
    Epoch:   620     LossContext: 0.00007353
    Epoch:   630     LossContext: 0.00007343
    Epoch:   640     LossContext: 0.00007334
    Epoch:   650     LossContext: 0.00007325
    Epoch:   660     LossContext: 0.00007315
    Epoch:   670     LossContext: 0.00007306
    Epoch:   680     LossContext: 0.00007296
    Epoch:   690     LossContext: 0.00007287
    Epoch:   700     LossContext: 0.00007277
    Epoch:   710     LossContext: 0.00007267
    Epoch:   720     LossContext: 0.00007257
    Epoch:   730     LossContext: 0.00007247
    Epoch:   740     LossContext: 0.00007237
    Epoch:   750     LossContext: 0.00007226
    Epoch:   760     LossContext: 0.00007216
    Epoch:   770     LossContext: 0.00007205
    Epoch:   780     LossContext: 0.00007195
    Epoch:   790     LossContext: 0.00007184
    Epoch:   800     LossContext: 0.00007173
    Epoch:   810     LossContext: 0.00007163
    Epoch:   820     LossContext: 0.00007152
    Epoch:   830     LossContext: 0.00007141
    Epoch:   840     LossContext: 0.00007129
    Epoch:   850     LossContext: 0.00007118
    Epoch:   860     LossContext: 0.00007107
    Epoch:   870     LossContext: 0.00007095
    Epoch:   880     LossContext: 0.00007084
    Epoch:   890     LossContext: 0.00007072
    Epoch:   900     LossContext: 0.00007061
    Epoch:   910     LossContext: 0.00007049
    Epoch:   920     LossContext: 0.00007037
    Epoch:   930     LossContext: 0.00007025
    Epoch:   940     LossContext: 0.00007013
    Epoch:   950     LossContext: 0.00007001
    Epoch:   960     LossContext: 0.00006988
    Epoch:   970     LossContext: 0.00006976
    Epoch:   980     LossContext: 0.00006963
    Epoch:   990     LossContext: 0.00006951
    Epoch:  1000     LossContext: 0.00006938
    Epoch:  1010     LossContext: 0.00006925
    Epoch:  1020     LossContext: 0.00006913
    Epoch:  1030     LossContext: 0.00006900
    Epoch:  1040     LossContext: 0.00006887
    Epoch:  1050     LossContext: 0.00006873
    Epoch:  1060     LossContext: 0.00006860
    Epoch:  1070     LossContext: 0.00006847
    Epoch:  1080     LossContext: 0.00006833
    Epoch:  1090     LossContext: 0.00006820
    Epoch:  1100     LossContext: 0.00006806
    Epoch:  1110     LossContext: 0.00006792
    Epoch:  1120     LossContext: 0.00006779
    Epoch:  1130     LossContext: 0.00006765
    Epoch:  1140     LossContext: 0.00006751
    Epoch:  1150     LossContext: 0.00006736
    Epoch:  1160     LossContext: 0.00006722
    Epoch:  1170     LossContext: 0.00006708
    Epoch:  1180     LossContext: 0.00006693
    Epoch:  1190     LossContext: 0.00006687
    Epoch:  1200     LossContext: 0.00006685
    Epoch:  1210     LossContext: 0.00006685
    Epoch:  1220     LossContext: 0.00006685
    Epoch:  1230     LossContext: 0.00006685
    Epoch:  1240     LossContext: 0.00006686
    Epoch:  1250     LossContext: 0.00006685
    Epoch:  1260     LossContext: 0.00006685
    Epoch:  1270     LossContext: 0.00006684
    Epoch:  1280     LossContext: 0.00006684
    Epoch:  1290     LossContext: 0.00006685
    Epoch:  1300     LossContext: 0.00006684
    Epoch:  1310     LossContext: 0.00006684
    Epoch:  1320     LossContext: 0.00006684
    Epoch:  1330     LossContext: 0.00006684
    Epoch:  1340     LossContext: 0.00006684
    Epoch:  1350     LossContext: 0.00006684
    Epoch:  1360     LossContext: 0.00006684
    Epoch:  1370     LossContext: 0.00006684
    Epoch:  1380     LossContext: 0.00006684
    Epoch:  1390     LossContext: 0.00006684
    Epoch:  1400     LossContext: 0.00006684
    Epoch:  1410     LossContext: 0.00006684
    Epoch:  1420     LossContext: 0.00006684
    Epoch:  1430     LossContext: 0.00006684
    Epoch:  1440     LossContext: 0.00006684
    Epoch:  1450     LossContext: 0.00006684
    Epoch:  1460     LossContext: 0.00006683
    Epoch:  1470     LossContext: 0.00006683
    Epoch:  1480     LossContext: 0.00006683
    Epoch:  1490     LossContext: 0.00006683
    Epoch:  1499     LossContext: 0.00006683

Gradient descent adaptation time: 0 hours 1 mins 47 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.12451039
    Epoch:     1     LossContext: 0.11383460
    Epoch:     2     LossContext: 0.10256419
    Epoch:     3     LossContext: 0.09077326
    Epoch:    10     LossContext: 0.02086947
    Epoch:    20     LossContext: 0.00219301
    Epoch:    30     LossContext: 0.00133425
    Epoch:    40     LossContext: 0.00081293
    Epoch:    50     LossContext: 0.00062849
    Epoch:    60     LossContext: 0.00040777
    Epoch:    70     LossContext: 0.00024750
    Epoch:    80     LossContext: 0.00020700
    Epoch:    90     LossContext: 0.00020681
    Epoch:   100     LossContext: 0.00020527
    Epoch:   110     LossContext: 0.00020293
    Epoch:   120     LossContext: 0.00020240
    Epoch:   130     LossContext: 0.00020243
    Epoch:   140     LossContext: 0.00020235
    Epoch:   150     LossContext: 0.00020226
    Epoch:   160     LossContext: 0.00020221
    Epoch:   170     LossContext: 0.00020217
    Epoch:   180     LossContext: 0.00020214
    Epoch:   190     LossContext: 0.00020211
    Epoch:   200     LossContext: 0.00020209
    Epoch:   210     LossContext: 0.00020206
    Epoch:   220     LossContext: 0.00020203
    Epoch:   230     LossContext: 0.00020200
    Epoch:   240     LossContext: 0.00020197
    Epoch:   250     LossContext: 0.00020194
    Epoch:   260     LossContext: 0.00020191
    Epoch:   270     LossContext: 0.00020187
    Epoch:   280     LossContext: 0.00020184
    Epoch:   290     LossContext: 0.00020181
    Epoch:   300     LossContext: 0.00020177
    Epoch:   310     LossContext: 0.00020174
    Epoch:   320     LossContext: 0.00020170
    Epoch:   330     LossContext: 0.00020166
    Epoch:   340     LossContext: 0.00020164
    Epoch:   350     LossContext: 0.00020162
    Epoch:   360     LossContext: 0.00020160
    Epoch:   370     LossContext: 0.00020158
    Epoch:   380     LossContext: 0.00020156
    Epoch:   390     LossContext: 0.00020154
    Epoch:   400     LossContext: 0.00020152
    Epoch:   410     LossContext: 0.00020150
    Epoch:   420     LossContext: 0.00020148
    Epoch:   430     LossContext: 0.00020146
    Epoch:   440     LossContext: 0.00020143
    Epoch:   450     LossContext: 0.00020141
    Epoch:   460     LossContext: 0.00020139
    Epoch:   470     LossContext: 0.00020137
    Epoch:   480     LossContext: 0.00020134
    Epoch:   490     LossContext: 0.00020132
    Epoch:   500     LossContext: 0.00020130
    Epoch:   510     LossContext: 0.00020127
    Epoch:   520     LossContext: 0.00020125
    Epoch:   530     LossContext: 0.00020122
    Epoch:   540     LossContext: 0.00020120
    Epoch:   550     LossContext: 0.00020117
    Epoch:   560     LossContext: 0.00020115
    Epoch:   570     LossContext: 0.00020112
    Epoch:   580     LossContext: 0.00020110
    Epoch:   590     LossContext: 0.00020107
    Epoch:   600     LossContext: 0.00020104
    Epoch:   610     LossContext: 0.00020102
    Epoch:   620     LossContext: 0.00020099
    Epoch:   630     LossContext: 0.00020096
    Epoch:   640     LossContext: 0.00020093
    Epoch:   650     LossContext: 0.00020090
    Epoch:   660     LossContext: 0.00020088
    Epoch:   670     LossContext: 0.00020085
    Epoch:   680     LossContext: 0.00020082
    Epoch:   690     LossContext: 0.00020079
    Epoch:   700     LossContext: 0.00020076
    Epoch:   710     LossContext: 0.00020073
    Epoch:   720     LossContext: 0.00020070
    Epoch:   730     LossContext: 0.00020067
    Epoch:   740     LossContext: 0.00020064
    Epoch:   750     LossContext: 0.00020061
    Epoch:   760     LossContext: 0.00020058
    Epoch:   770     LossContext: 0.00020054
    Epoch:   780     LossContext: 0.00020051
    Epoch:   790     LossContext: 0.00020048
    Epoch:   800     LossContext: 0.00020045
    Epoch:   810     LossContext: 0.00020041
    Epoch:   820     LossContext: 0.00020038
    Epoch:   830     LossContext: 0.00020035
    Epoch:   840     LossContext: 0.00020031
    Epoch:   850     LossContext: 0.00020028
    Epoch:   860     LossContext: 0.00020024
    Epoch:   870     LossContext: 0.00020021
    Epoch:   880     LossContext: 0.00020017
    Epoch:   890     LossContext: 0.00020014
    Epoch:   900     LossContext: 0.00020011
    Epoch:   910     LossContext: 0.00020007
    Epoch:   920     LossContext: 0.00020003
    Epoch:   930     LossContext: 0.00020000
    Epoch:   940     LossContext: 0.00019996
    Epoch:   950     LossContext: 0.00019992
    Epoch:   960     LossContext: 0.00019988
    Epoch:   970     LossContext: 0.00019985
    Epoch:   980     LossContext: 0.00019981
    Epoch:   990     LossContext: 0.00019977
    Epoch:  1000     LossContext: 0.00019973
    Epoch:  1010     LossContext: 0.00019969
    Epoch:  1020     LossContext: 0.00019965
    Epoch:  1030     LossContext: 0.00019962
    Epoch:  1040     LossContext: 0.00019958
    Epoch:  1050     LossContext: 0.00019953
    Epoch:  1060     LossContext: 0.00019950
    Epoch:  1070     LossContext: 0.00019945
    Epoch:  1080     LossContext: 0.00019941
    Epoch:  1090     LossContext: 0.00019937
    Epoch:  1100     LossContext: 0.00019933
    Epoch:  1110     LossContext: 0.00019929
    Epoch:  1120     LossContext: 0.00019924
    Epoch:  1130     LossContext: 0.00019920
    Epoch:  1140     LossContext: 0.00019916
    Epoch:  1150     LossContext: 0.00019912
    Epoch:  1160     LossContext: 0.00019907
    Epoch:  1170     LossContext: 0.00019903
    Epoch:  1180     LossContext: 0.00019898
    Epoch:  1190     LossContext: 0.00019894
    Epoch:  1200     LossContext: 0.00019889
    Epoch:  1210     LossContext: 0.00019885
    Epoch:  1220     LossContext: 0.00019881
    Epoch:  1230     LossContext: 0.00019876
    Epoch:  1240     LossContext: 0.00019871
    Epoch:  1250     LossContext: 0.00019867
    Epoch:  1260     LossContext: 0.00019862
    Epoch:  1270     LossContext: 0.00019857
    Epoch:  1280     LossContext: 0.00019852
    Epoch:  1290     LossContext: 0.00019848
    Epoch:  1300     LossContext: 0.00019843
    Epoch:  1310     LossContext: 0.00019838
    Epoch:  1320     LossContext: 0.00019833
    Epoch:  1330     LossContext: 0.00019828
    Epoch:  1340     LossContext: 0.00019823
    Epoch:  1350     LossContext: 0.00019818
    Epoch:  1360     LossContext: 0.00019813
    Epoch:  1370     LossContext: 0.00019808
    Epoch:  1380     LossContext: 0.00019803
    Epoch:  1390     LossContext: 0.00019798
    Epoch:  1400     LossContext: 0.00019793
    Epoch:  1410     LossContext: 0.00019788
    Epoch:  1420     LossContext: 0.00019783
    Epoch:  1430     LossContext: 0.00019777
    Epoch:  1440     LossContext: 0.00019772
    Epoch:  1450     LossContext: 0.00019767
    Epoch:  1460     LossContext: 0.00019761
    Epoch:  1470     LossContext: 0.00019756
    Epoch:  1480     LossContext: 0.00019751
    Epoch:  1490     LossContext: 0.00019745
    Epoch:  1499     LossContext: 0.00019740

Gradient descent adaptation time: 0 hours 1 mins 33 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.04711356
    Epoch:     1     LossContext: 0.04031361
    Epoch:     2     LossContext: 0.03412138
    Epoch:     3     LossContext: 0.02855806
    Epoch:    10     LossContext: 0.00596342
    Epoch:    20     LossContext: 0.00021523
    Epoch:    30     LossContext: 0.00035690
    Epoch:    40     LossContext: 0.00049598
    Epoch:    50     LossContext: 0.00034010
    Epoch:    60     LossContext: 0.00018500
    Epoch:    70     LossContext: 0.00013605
    Epoch:    80     LossContext: 0.00013689
    Epoch:    90     LossContext: 0.00013764
    Epoch:   100     LossContext: 0.00013521
    Epoch:   110     LossContext: 0.00013439
    Epoch:   120     LossContext: 0.00013444
    Epoch:   130     LossContext: 0.00013439
    Epoch:   140     LossContext: 0.00013432
    Epoch:   150     LossContext: 0.00013429
    Epoch:   160     LossContext: 0.00013426
    Epoch:   170     LossContext: 0.00013423
    Epoch:   180     LossContext: 0.00013422
    Epoch:   190     LossContext: 0.00013420
    Epoch:   200     LossContext: 0.00013418
    Epoch:   210     LossContext: 0.00013417
    Epoch:   220     LossContext: 0.00013415
    Epoch:   230     LossContext: 0.00013413
    Epoch:   240     LossContext: 0.00013411
    Epoch:   250     LossContext: 0.00013409
    Epoch:   260     LossContext: 0.00013407
    Epoch:   270     LossContext: 0.00013406
    Epoch:   280     LossContext: 0.00013403
    Epoch:   290     LossContext: 0.00013401
    Epoch:   300     LossContext: 0.00013399
    Epoch:   310     LossContext: 0.00013397
    Epoch:   320     LossContext: 0.00013395
    Epoch:   330     LossContext: 0.00013393
    Epoch:   340     LossContext: 0.00013391
    Epoch:   350     LossContext: 0.00013390
    Epoch:   360     LossContext: 0.00013389
    Epoch:   370     LossContext: 0.00013388
    Epoch:   380     LossContext: 0.00013387
    Epoch:   390     LossContext: 0.00013386
    Epoch:   400     LossContext: 0.00013384
    Epoch:   410     LossContext: 0.00013383
    Epoch:   420     LossContext: 0.00013382
    Epoch:   430     LossContext: 0.00013381
    Epoch:   440     LossContext: 0.00013379
    Epoch:   450     LossContext: 0.00013378
    Epoch:   460     LossContext: 0.00013377
    Epoch:   470     LossContext: 0.00013375
    Epoch:   480     LossContext: 0.00013374
    Epoch:   490     LossContext: 0.00013372
    Epoch:   500     LossContext: 0.00013371
    Epoch:   510     LossContext: 0.00013370
    Epoch:   520     LossContext: 0.00013368
    Epoch:   530     LossContext: 0.00013367
    Epoch:   540     LossContext: 0.00013365
    Epoch:   550     LossContext: 0.00013364
    Epoch:   560     LossContext: 0.00013362
    Epoch:   570     LossContext: 0.00013361
    Epoch:   580     LossContext: 0.00013359
    Epoch:   590     LossContext: 0.00013357
    Epoch:   600     LossContext: 0.00013356
    Epoch:   610     LossContext: 0.00013354
    Epoch:   620     LossContext: 0.00013353
    Epoch:   630     LossContext: 0.00013351
    Epoch:   640     LossContext: 0.00013349
    Epoch:   650     LossContext: 0.00013348
    Epoch:   660     LossContext: 0.00013346
    Epoch:   670     LossContext: 0.00013344
    Epoch:   680     LossContext: 0.00013342
    Epoch:   690     LossContext: 0.00013341
    Epoch:   700     LossContext: 0.00013339
    Epoch:   710     LossContext: 0.00013337
    Epoch:   720     LossContext: 0.00013335
    Epoch:   730     LossContext: 0.00013334
    Epoch:   740     LossContext: 0.00013332
    Epoch:   750     LossContext: 0.00013330
    Epoch:   760     LossContext: 0.00013328
    Epoch:   770     LossContext: 0.00013326
    Epoch:   780     LossContext: 0.00013324
    Epoch:   790     LossContext: 0.00013322
    Epoch:   800     LossContext: 0.00013320
    Epoch:   810     LossContext: 0.00013318
    Epoch:   820     LossContext: 0.00013316
    Epoch:   830     LossContext: 0.00013314
    Epoch:   840     LossContext: 0.00013312
    Epoch:   850     LossContext: 0.00013310
    Epoch:   860     LossContext: 0.00013308
    Epoch:   870     LossContext: 0.00013306
    Epoch:   880     LossContext: 0.00013304
    Epoch:   890     LossContext: 0.00013302
    Epoch:   900     LossContext: 0.00013300
    Epoch:   910     LossContext: 0.00013298
    Epoch:   920     LossContext: 0.00013296
    Epoch:   930     LossContext: 0.00013293
    Epoch:   940     LossContext: 0.00013291
    Epoch:   950     LossContext: 0.00013289
    Epoch:   960     LossContext: 0.00013287
    Epoch:   970     LossContext: 0.00013284
    Epoch:   980     LossContext: 0.00013282
    Epoch:   990     LossContext: 0.00013280
    Epoch:  1000     LossContext: 0.00013278
    Epoch:  1010     LossContext: 0.00013275
    Epoch:  1020     LossContext: 0.00013273
    Epoch:  1030     LossContext: 0.00013271
    Epoch:  1040     LossContext: 0.00013268
    Epoch:  1050     LossContext: 0.00013266
    Epoch:  1060     LossContext: 0.00013263
    Epoch:  1070     LossContext: 0.00013261
    Epoch:  1080     LossContext: 0.00013259
    Epoch:  1090     LossContext: 0.00013256
    Epoch:  1100     LossContext: 0.00013254
    Epoch:  1110     LossContext: 0.00013251
    Epoch:  1120     LossContext: 0.00013249
    Epoch:  1130     LossContext: 0.00013246
    Epoch:  1140     LossContext: 0.00013243
    Epoch:  1150     LossContext: 0.00013241
    Epoch:  1160     LossContext: 0.00013238
    Epoch:  1170     LossContext: 0.00013236
    Epoch:  1180     LossContext: 0.00013233
    Epoch:  1190     LossContext: 0.00013230
    Epoch:  1200     LossContext: 0.00013228
    Epoch:  1210     LossContext: 0.00013225
    Epoch:  1220     LossContext: 0.00013222
    Epoch:  1230     LossContext: 0.00013220
    Epoch:  1240     LossContext: 0.00013217
    Epoch:  1250     LossContext: 0.00013214
    Epoch:  1260     LossContext: 0.00013211
    Epoch:  1270     LossContext: 0.00013208
    Epoch:  1280     LossContext: 0.00013206
    Epoch:  1290     LossContext: 0.00013203
    Epoch:  1300     LossContext: 0.00013200
    Epoch:  1310     LossContext: 0.00013197
    Epoch:  1320     LossContext: 0.00013194
    Epoch:  1330     LossContext: 0.00013191
    Epoch:  1340     LossContext: 0.00013188
    Epoch:  1350     LossContext: 0.00013185
    Epoch:  1360     LossContext: 0.00013182
    Epoch:  1370     LossContext: 0.00013179
    Epoch:  1380     LossContext: 0.00013176
    Epoch:  1390     LossContext: 0.00013173
    Epoch:  1400     LossContext: 0.00013170
    Epoch:  1410     LossContext: 0.00013167
    Epoch:  1420     LossContext: 0.00013164
    Epoch:  1430     LossContext: 0.00013161
    Epoch:  1440     LossContext: 0.00013158
    Epoch:  1450     LossContext: 0.00013155
    Epoch:  1460     LossContext: 0.00013151
    Epoch:  1470     LossContext: 0.00013148
    Epoch:  1480     LossContext: 0.00013145
    Epoch:  1490     LossContext: 0.00013142
    Epoch:  1499     LossContext: 0.00013139

Gradient descent adaptation time: 0 hours 1 mins 34 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.02918768
    Epoch:     1     LossContext: 0.02425969
    Epoch:     2     LossContext: 0.01993799
    Epoch:     3     LossContext: 0.01621814
    Epoch:    10     LossContext: 0.00312637
    Epoch:    20     LossContext: 0.00032015
    Epoch:    30     LossContext: 0.00022351
    Epoch:    40     LossContext: 0.00028255
    Epoch:    50     LossContext: 0.00027475
    Epoch:    60     LossContext: 0.00023953
    Epoch:    70     LossContext: 0.00021069
    Epoch:    80     LossContext: 0.00019809
    Epoch:    90     LossContext: 0.00019503
    Epoch:   100     LossContext: 0.00019487
    Epoch:   110     LossContext: 0.00019484
    Epoch:   120     LossContext: 0.00019462
    Epoch:   130     LossContext: 0.00019441
    Epoch:   140     LossContext: 0.00019425
    Epoch:   150     LossContext: 0.00019410
    Epoch:   160     LossContext: 0.00019395
    Epoch:   170     LossContext: 0.00019382
    Epoch:   180     LossContext: 0.00019374
    Epoch:   190     LossContext: 0.00019366
    Epoch:   200     LossContext: 0.00019357
    Epoch:   210     LossContext: 0.00019348
    Epoch:   220     LossContext: 0.00019339
    Epoch:   230     LossContext: 0.00019330
    Epoch:   240     LossContext: 0.00019320
    Epoch:   250     LossContext: 0.00019310
    Epoch:   260     LossContext: 0.00019300
    Epoch:   270     LossContext: 0.00019290
    Epoch:   280     LossContext: 0.00019280
    Epoch:   290     LossContext: 0.00019269
    Epoch:   300     LossContext: 0.00019258
    Epoch:   310     LossContext: 0.00019247
    Epoch:   320     LossContext: 0.00019236
    Epoch:   330     LossContext: 0.00019224
    Epoch:   340     LossContext: 0.00019217
    Epoch:   350     LossContext: 0.00019211
    Epoch:   360     LossContext: 0.00019205
    Epoch:   370     LossContext: 0.00019199
    Epoch:   380     LossContext: 0.00019193
    Epoch:   390     LossContext: 0.00019186
    Epoch:   400     LossContext: 0.00019180
    Epoch:   410     LossContext: 0.00019173
    Epoch:   420     LossContext: 0.00019167
    Epoch:   430     LossContext: 0.00019160
    Epoch:   440     LossContext: 0.00019153
    Epoch:   450     LossContext: 0.00019146
    Epoch:   460     LossContext: 0.00019139
    Epoch:   470     LossContext: 0.00019132
    Epoch:   480     LossContext: 0.00019125
    Epoch:   490     LossContext: 0.00019118
    Epoch:   500     LossContext: 0.00019110
    Epoch:   510     LossContext: 0.00019103
    Epoch:   520     LossContext: 0.00019095
    Epoch:   530     LossContext: 0.00019088
    Epoch:   540     LossContext: 0.00019080
    Epoch:   550     LossContext: 0.00019072
    Epoch:   560     LossContext: 0.00019064
    Epoch:   570     LossContext: 0.00019056
    Epoch:   580     LossContext: 0.00019048
    Epoch:   590     LossContext: 0.00019040
    Epoch:   600     LossContext: 0.00019031
    Epoch:   610     LossContext: 0.00019023
    Epoch:   620     LossContext: 0.00019015
    Epoch:   630     LossContext: 0.00019006
    Epoch:   640     LossContext: 0.00018997
    Epoch:   650     LossContext: 0.00018989
    Epoch:   660     LossContext: 0.00018980
    Epoch:   670     LossContext: 0.00018971
    Epoch:   680     LossContext: 0.00018962
    Epoch:   690     LossContext: 0.00018953
    Epoch:   700     LossContext: 0.00018944
    Epoch:   710     LossContext: 0.00018934
    Epoch:   720     LossContext: 0.00018925
    Epoch:   730     LossContext: 0.00018916
    Epoch:   740     LossContext: 0.00018906
    Epoch:   750     LossContext: 0.00018896
    Epoch:   760     LossContext: 0.00018887
    Epoch:   770     LossContext: 0.00018877
    Epoch:   780     LossContext: 0.00018867
    Epoch:   790     LossContext: 0.00018857
    Epoch:   800     LossContext: 0.00018847
    Epoch:   810     LossContext: 0.00018837
    Epoch:   820     LossContext: 0.00018827
    Epoch:   830     LossContext: 0.00018816
    Epoch:   840     LossContext: 0.00018806
    Epoch:   850     LossContext: 0.00018795
    Epoch:   860     LossContext: 0.00018785
    Epoch:   870     LossContext: 0.00018774
    Epoch:   880     LossContext: 0.00018763
    Epoch:   890     LossContext: 0.00018753
    Epoch:   900     LossContext: 0.00018742
    Epoch:   910     LossContext: 0.00018731
    Epoch:   920     LossContext: 0.00018719
    Epoch:   930     LossContext: 0.00018708
    Epoch:   940     LossContext: 0.00018697
    Epoch:   950     LossContext: 0.00018686
    Epoch:   960     LossContext: 0.00018674
    Epoch:   970     LossContext: 0.00018663
    Epoch:   980     LossContext: 0.00018651
    Epoch:   990     LossContext: 0.00018639
    Epoch:  1000     LossContext: 0.00018627
    Epoch:  1010     LossContext: 0.00018615
    Epoch:  1020     LossContext: 0.00018603
    Epoch:  1030     LossContext: 0.00018591
    Epoch:  1040     LossContext: 0.00018579
    Epoch:  1050     LossContext: 0.00018567
    Epoch:  1060     LossContext: 0.00018555
    Epoch:  1070     LossContext: 0.00018542
    Epoch:  1080     LossContext: 0.00018530
    Epoch:  1090     LossContext: 0.00018517
    Epoch:  1100     LossContext: 0.00018504
    Epoch:  1110     LossContext: 0.00018491
    Epoch:  1120     LossContext: 0.00018479
    Epoch:  1130     LossContext: 0.00018466
    Epoch:  1140     LossContext: 0.00018452
    Epoch:  1150     LossContext: 0.00018439
    Epoch:  1160     LossContext: 0.00018426
    Epoch:  1170     LossContext: 0.00018413
    Epoch:  1180     LossContext: 0.00018399
    Epoch:  1190     LossContext: 0.00018386
    Epoch:  1200     LossContext: 0.00018372
    Epoch:  1210     LossContext: 0.00018358
    Epoch:  1220     LossContext: 0.00018345
    Epoch:  1230     LossContext: 0.00018331
    Epoch:  1240     LossContext: 0.00018317
    Epoch:  1250     LossContext: 0.00018303
    Epoch:  1260     LossContext: 0.00018288
    Epoch:  1270     LossContext: 0.00018274
    Epoch:  1280     LossContext: 0.00018260
    Epoch:  1290     LossContext: 0.00018245
    Epoch:  1300     LossContext: 0.00018231
    Epoch:  1310     LossContext: 0.00018216
    Epoch:  1320     LossContext: 0.00018202
    Epoch:  1330     LossContext: 0.00018187
    Epoch:  1340     LossContext: 0.00018172
    Epoch:  1350     LossContext: 0.00018157
    Epoch:  1360     LossContext: 0.00018142
    Epoch:  1370     LossContext: 0.00018127
    Epoch:  1380     LossContext: 0.00018111
    Epoch:  1390     LossContext: 0.00018096
    Epoch:  1400     LossContext: 0.00018080
    Epoch:  1410     LossContext: 0.00018065
    Epoch:  1420     LossContext: 0.00018049
    Epoch:  1430     LossContext: 0.00018033
    Epoch:  1440     LossContext: 0.00018018
    Epoch:  1450     LossContext: 0.00018002
    Epoch:  1460     LossContext: 0.00017986
    Epoch:  1470     LossContext: 0.00017970
    Epoch:  1480     LossContext: 0.00017953
    Epoch:  1490     LossContext: 0.00017937
    Epoch:  1499     LossContext: 0.00017922

Gradient descent adaptation time: 0 hours 1 mins 33 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./05052024-110926/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 0.0001521809

==  Begining out-of-distribution visualisation ... ==
    Environment id: 0
    Trajectory id: 0
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./05052024-110926/adapt/results_ood.png
Testing finished. Figure saved in: ./05052024-110926/adapt/results_ood_uq.pdf

======================= DONE ======================== 

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

========= STARTING =========
Working with a context size of 2^3 = 8
Run folder created successfuly: ./05052024-130226/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 130243
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 130243
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 48144 


WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed


=== Beginning training with proximal alternating minimization ... ===
    Number of examples in a batch: 4
    Maximum number of steps per inner minimization: 25
    Maximum number of outer minimizations: 500
    Maximum total number of training steps: 12500

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Outer Step:     0      LossTrajs: 1.81044006     ContextsNorm: 0.00000000     ValIndCrit: 1.69865823
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.65e-06
        -DiffCxt:  inf
    Outer Step:     1      LossTrajs: 1.51909816     ContextsNorm: 0.00023149     ValIndCrit: 1.42377770
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.58e-06
        -DiffCxt:  2.34e-03
    Outer Step:     2      LossTrajs: 1.19318104     ContextsNorm: 0.00049457     ValIndCrit: 1.11541831
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.42e-06
        -DiffCxt:  1.05e-03
    Outer Step:     3      LossTrajs: 0.72773725     ContextsNorm: 0.00128276     ValIndCrit: 0.68195891
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.40e-06
        -DiffCxt:  1.99e-04
    Outer Step:    10      LossTrajs: 0.25128937     ContextsNorm: 0.00420123     ValIndCrit: 0.29705289
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.46e-07
        -DiffCxt:  9.09e-06
    Outer Step:    20      LossTrajs: 0.17130701     ContextsNorm: 0.00944572     ValIndCrit: 0.18817778
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.51e-07
        -DiffCxt:  2.27e-06
    Outer Step:    30      LossTrajs: 0.08114817     ContextsNorm: 0.02227939     ValIndCrit: 0.08439813
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.36e-07
        -DiffCxt:  3.67e-06
    Outer Step:    40      LossTrajs: 0.05055302     ContextsNorm: 0.03130320     ValIndCrit: 0.05382872
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.38e-08
        -DiffCxt:  2.09e-07
    Outer Step:    50      LossTrajs: 0.04259521     ContextsNorm: 0.03208183     ValIndCrit: 0.04652797
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.86e-08
        -DiffCxt:  1.44e-07
    Outer Step:    60      LossTrajs: 0.03381952     ContextsNorm: 0.03496276     ValIndCrit: 0.03808458
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.93e-08
        -DiffCxt:  2.13e-07
    Outer Step:    70      LossTrajs: 0.03017300     ContextsNorm: 0.03667390     ValIndCrit: 0.03469159
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.31e-08
        -DiffCxt:  6.13e-08
    Outer Step:    80      LossTrajs: 0.02865295     ContextsNorm: 0.03659269     ValIndCrit: 0.03363829
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.17e-08
        -DiffCxt:  1.28e-07
    Outer Step:    90      LossTrajs: 0.02761278     ContextsNorm: 0.03782595     ValIndCrit: 0.03257617
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.15e-08
        -DiffCxt:  2.25e-07
    Outer Step:   100      LossTrajs: 0.02550292     ContextsNorm: 0.03785558     ValIndCrit: 0.03063952
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.78e-08
        -DiffCxt:  1.89e-07
    Outer Step:   110      LossTrajs: 0.02428062     ContextsNorm: 0.03946935     ValIndCrit: 0.02931736
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.08e-08
        -DiffCxt:  1.88e-07
    Outer Step:   120      LossTrajs: 0.02015848     ContextsNorm: 0.04520631     ValIndCrit: 0.02372515
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.09e-07
        -DiffCxt:  7.92e-07
    Outer Step:   130      LossTrajs: 0.00863938     ContextsNorm: 0.04696460     ValIndCrit: 0.01116471
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.63e-08
        -DiffCxt:  2.03e-07
    Outer Step:   140      LossTrajs: 0.00493412     ContextsNorm: 0.04745827     ValIndCrit: 0.00645480
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.85e-08
        -DiffCxt:  1.46e-07
    Outer Step:   150      LossTrajs: 0.00236839     ContextsNorm: 0.04773600     ValIndCrit: 0.00329168
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.65e-08
        -DiffCxt:  8.38e-08
    Outer Step:   160      LossTrajs: 0.00106794     ContextsNorm: 0.04769742     ValIndCrit: 0.00212993
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.58e-09
        -DiffCxt:  2.42e-08
    Outer Step:   170      LossTrajs: 0.00079856     ContextsNorm: 0.04752195     ValIndCrit: 0.00161151
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.82e-09
        -DiffCxt:  2.84e-08
    Outer Step:   180      LossTrajs: 0.00061909     ContextsNorm: 0.04726307     ValIndCrit: 0.00124710
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.18e-09
        -DiffCxt:  2.74e-08
    Outer Step:   190      LossTrajs: 0.00049120     ContextsNorm: 0.04669701     ValIndCrit: 0.00095258
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.77e-09
        -DiffCxt:  2.37e-08
    Outer Step:   200      LossTrajs: 0.00036851     ContextsNorm: 0.04638465     ValIndCrit: 0.00075021
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.63e-09
        -DiffCxt:  2.87e-08
    Outer Step:   210      LossTrajs: 0.00029745     ContextsNorm: 0.04633876     ValIndCrit: 0.00061201
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   18
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.40e-09
        -DiffCxt:  9.45e-09
    Outer Step:   220      LossTrajs: 0.00024929     ContextsNorm: 0.04625718     ValIndCrit: 0.00051706
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.47e-08
        -DiffCxt:  2.06e-08
    Outer Step:   230      LossTrajs: 0.00021775     ContextsNorm: 0.04622902     ValIndCrit: 0.00047912
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.91e-09
        -DiffCxt:  7.72e-09
    Outer Step:   240      LossTrajs: 0.00022027     ContextsNorm: 0.04605326     ValIndCrit: 0.00044374
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.36e-09
        -DiffCxt:  2.74e-08
    Outer Step:   250      LossTrajs: 0.00019543     ContextsNorm: 0.04588330     ValIndCrit: 0.00040809
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.69e-09
        -DiffCxt:  3.69e-08
    Outer Step:   260      LossTrajs: 0.00018632     ContextsNorm: 0.04569161     ValIndCrit: 0.00037727
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   18
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.70e-09
        -DiffCxt:  9.68e-09
    Outer Step:   270      LossTrajs: 0.00017483     ContextsNorm: 0.04552925     ValIndCrit: 0.00036694
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.09e-10
        -DiffCxt:  2.30e-08
    Outer Step:   280      LossTrajs: 0.00014933     ContextsNorm: 0.04552701     ValIndCrit: 0.00033180
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   23
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.39e-09
        -DiffCxt:  8.89e-09
    Outer Step:   290      LossTrajs: 0.00013895     ContextsNorm: 0.04550266     ValIndCrit: 0.00030328
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.94e-09
        -DiffCxt:  2.91e-08
    Outer Step:   300      LossTrajs: 0.00013766     ContextsNorm: 0.04537945     ValIndCrit: 0.00028291
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.00e-09
        -DiffCxt:  2.42e-08
    Outer Step:   310      LossTrajs: 0.00013654     ContextsNorm: 0.04526414     ValIndCrit: 0.00027339
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   17
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.31e-10
        -DiffCxt:  7.99e-09
    Outer Step:   320      LossTrajs: 0.00013117     ContextsNorm: 0.04505173     ValIndCrit: 0.00026383
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.89e-09
        -DiffCxt:  1.62e-08
    Outer Step:   330      LossTrajs: 0.00012337     ContextsNorm: 0.04503800     ValIndCrit: 0.00025303
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.84e-09
        -DiffCxt:  1.19e-08
    Outer Step:   340      LossTrajs: 0.00011680     ContextsNorm: 0.04495589     ValIndCrit: 0.00022453
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.98e-09
        -DiffCxt:  1.36e-08
    Outer Step:   350      LossTrajs: 0.00011061     ContextsNorm: 0.04477370     ValIndCrit: 0.00021719
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   24
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.55e-09
        -DiffCxt:  9.94e-09
    Outer Step:   360      LossTrajs: 0.00011290     ContextsNorm: 0.04476384     ValIndCrit: 0.00021602
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.45e-09
        -DiffCxt:  5.18e-08
    Outer Step:   370      LossTrajs: 0.00010683     ContextsNorm: 0.04479861     ValIndCrit: 0.00019756
        Saving best model so far ...
        -NbInnerStepsNode:   13
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.93e-10
        -DiffCxt:  1.55e-08
    Outer Step:   380      LossTrajs: 0.00010497     ContextsNorm: 0.04466495     ValIndCrit: 0.00019229
        Saving best model so far ...
        -NbInnerStepsNode:   18
        -NbInnerStepsCxt:   21
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.88e-10
        -DiffCxt:  8.97e-09
    Outer Step:   390      LossTrajs: 0.00010149     ContextsNorm: 0.04455878     ValIndCrit: 0.00017877
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   22
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.18e-09
        -DiffCxt:  7.73e-09
    Outer Step:   400      LossTrajs: 0.00009880     ContextsNorm: 0.04446150     ValIndCrit: 0.00016832
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.90e-09
        -DiffCxt:  2.77e-08
    Outer Step:   410      LossTrajs: 0.00009395     ContextsNorm: 0.04439834     ValIndCrit: 0.00016388
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    7
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.85e-10
        -DiffCxt:  6.94e-09
    Outer Step:   420      LossTrajs: 0.00009281     ContextsNorm: 0.04435237     ValIndCrit: 0.00015723
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.98e-09
        -DiffCxt:  2.64e-08
    Outer Step:   430      LossTrajs: 0.00009180     ContextsNorm: 0.04432515     ValIndCrit: 0.00015689
        Saving best model so far ...
        -NbInnerStepsNode:   15
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.81e-10
        -DiffCxt:  2.63e-08
    Outer Step:   440      LossTrajs: 0.00009086     ContextsNorm: 0.04421623     ValIndCrit: 0.00014719
        Saving best model so far ...
        -NbInnerStepsNode:    4
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.83e-10
        -DiffCxt:  2.90e-08
    Outer Step:   450      LossTrajs: 0.00008916     ContextsNorm: 0.04414697     ValIndCrit: 0.00014571
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    6
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.71e-10
        -DiffCxt:  6.92e-09
    Outer Step:   460      LossTrajs: 0.00008857     ContextsNorm: 0.04409622     ValIndCrit: 0.00014333
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.68e-09
        -DiffCxt:  1.35e-08
    Outer Step:   470      LossTrajs: 0.00008731     ContextsNorm: 0.04403974     ValIndCrit: 0.00013600
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.21e-09
        -DiffCxt:  2.98e-08
    Outer Step:   480      LossTrajs: 0.00008588     ContextsNorm: 0.04401787     ValIndCrit: 0.00013345
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   23
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.40e-10
        -DiffCxt:  8.38e-09
    Outer Step:   490      LossTrajs: 0.00008645     ContextsNorm: 0.04391693     ValIndCrit: 0.00012943
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   22
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.33e-10
        -DiffCxt:  9.13e-09
    Outer Step:   499      LossTrajs: 0.00008415     ContextsNorm: 0.04390702     ValIndCrit: 0.00012688
        Saving best model so far ...
        -NbInnerStepsNode:    4
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.37e-10
        -DiffCxt:  1.42e-08

Total gradient descent training time: 1 hours 45 mins 3 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 144748
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 0.0001268837


############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]
==  Begining in-domain visualisation ... ==
    Environment id: 0
    Trajectory id: 29
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./05052024-130226/results_in_domain.png
Testing finished. Figure saved in: ./05052024-130226/results_in_domain_uq.pdf
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.01740540
    Epoch:     1     LossContext: 0.01242019
    Epoch:     2     LossContext: 0.00791077
    Epoch:     3     LossContext: 0.00436028
    Epoch:    10     LossContext: 0.00165894
    Epoch:    20     LossContext: 0.00016556
    Epoch:    30     LossContext: 0.00027590
    Epoch:    40     LossContext: 0.00018463
    Epoch:    50     LossContext: 0.00009568
    Epoch:    60     LossContext: 0.00007296
    Epoch:    70     LossContext: 0.00006973
    Epoch:    80     LossContext: 0.00006810
    Epoch:    90     LossContext: 0.00006777
    Epoch:   100     LossContext: 0.00006775
    Epoch:   110     LossContext: 0.00006758
    Epoch:   120     LossContext: 0.00006753
    Epoch:   130     LossContext: 0.00006747
    Epoch:   140     LossContext: 0.00006743
    Epoch:   150     LossContext: 0.00006738
    Epoch:   160     LossContext: 0.00006733
    Epoch:   170     LossContext: 0.00006729
    Epoch:   180     LossContext: 0.00006727
    Epoch:   190     LossContext: 0.00006724
    Epoch:   200     LossContext: 0.00006721
    Epoch:   210     LossContext: 0.00006718
    Epoch:   220     LossContext: 0.00006715
    Epoch:   230     LossContext: 0.00006713
    Epoch:   240     LossContext: 0.00006709
    Epoch:   250     LossContext: 0.00006706
    Epoch:   260     LossContext: 0.00006703
    Epoch:   270     LossContext: 0.00006700
    Epoch:   280     LossContext: 0.00006697
    Epoch:   290     LossContext: 0.00006693
    Epoch:   300     LossContext: 0.00006690
    Epoch:   310     LossContext: 0.00006686
    Epoch:   320     LossContext: 0.00006683
    Epoch:   330     LossContext: 0.00006679
    Epoch:   340     LossContext: 0.00006677
    Epoch:   350     LossContext: 0.00006675
    Epoch:   360     LossContext: 0.00006673
    Epoch:   370     LossContext: 0.00006671
    Epoch:   380     LossContext: 0.00006669
    Epoch:   390     LossContext: 0.00006667
    Epoch:   400     LossContext: 0.00006665
    Epoch:   410     LossContext: 0.00006663
    Epoch:   420     LossContext: 0.00006661
    Epoch:   430     LossContext: 0.00006658
    Epoch:   440     LossContext: 0.00006656
    Epoch:   450     LossContext: 0.00006654
    Epoch:   460     LossContext: 0.00006652
    Epoch:   470     LossContext: 0.00006649
    Epoch:   480     LossContext: 0.00006647
    Epoch:   490     LossContext: 0.00006645
    Epoch:   500     LossContext: 0.00006643
    Epoch:   510     LossContext: 0.00006640
    Epoch:   520     LossContext: 0.00006638
    Epoch:   530     LossContext: 0.00006635
    Epoch:   540     LossContext: 0.00006633
    Epoch:   550     LossContext: 0.00006630
    Epoch:   560     LossContext: 0.00006628
    Epoch:   570     LossContext: 0.00006625
    Epoch:   580     LossContext: 0.00006622
    Epoch:   590     LossContext: 0.00006620
    Epoch:   600     LossContext: 0.00006617
    Epoch:   610     LossContext: 0.00006615
    Epoch:   620     LossContext: 0.00006612
    Epoch:   630     LossContext: 0.00006609
    Epoch:   640     LossContext: 0.00006607
    Epoch:   650     LossContext: 0.00006605
    Epoch:   660     LossContext: 0.00006603
    Epoch:   670     LossContext: 0.00006601
    Epoch:   680     LossContext: 0.00006599
    Epoch:   690     LossContext: 0.00006596
    Epoch:   700     LossContext: 0.00006594
    Epoch:   710     LossContext: 0.00006592
    Epoch:   720     LossContext: 0.00006590
    Epoch:   730     LossContext: 0.00006588
    Epoch:   740     LossContext: 0.00006585
    Epoch:   750     LossContext: 0.00006583
    Epoch:   760     LossContext: 0.00006581
    Epoch:   770     LossContext: 0.00006579
    Epoch:   780     LossContext: 0.00006576
    Epoch:   790     LossContext: 0.00006574
    Epoch:   800     LossContext: 0.00006572
    Epoch:   810     LossContext: 0.00006569
    Epoch:   820     LossContext: 0.00006567
    Epoch:   830     LossContext: 0.00006564
    Epoch:   840     LossContext: 0.00006562
    Epoch:   850     LossContext: 0.00006559
    Epoch:   860     LossContext: 0.00006557
    Epoch:   870     LossContext: 0.00006554
    Epoch:   880     LossContext: 0.00006552
    Epoch:   890     LossContext: 0.00006549
    Epoch:   900     LossContext: 0.00006547
    Epoch:   910     LossContext: 0.00006544
    Epoch:   920     LossContext: 0.00006542
    Epoch:   930     LossContext: 0.00006539
    Epoch:   940     LossContext: 0.00006536
    Epoch:   950     LossContext: 0.00006534
    Epoch:   960     LossContext: 0.00006531
    Epoch:   970     LossContext: 0.00006529
    Epoch:   980     LossContext: 0.00006528
    Epoch:   990     LossContext: 0.00006526
    Epoch:  1000     LossContext: 0.00006525
    Epoch:  1010     LossContext: 0.00006524
    Epoch:  1020     LossContext: 0.00006523
    Epoch:  1030     LossContext: 0.00006521
    Epoch:  1040     LossContext: 0.00006520
    Epoch:  1050     LossContext: 0.00006519
    Epoch:  1060     LossContext: 0.00006518
    Epoch:  1070     LossContext: 0.00006517
    Epoch:  1080     LossContext: 0.00006515
    Epoch:  1090     LossContext: 0.00006514
    Epoch:  1100     LossContext: 0.00006513
    Epoch:  1110     LossContext: 0.00006512
    Epoch:  1120     LossContext: 0.00006510
    Epoch:  1130     LossContext: 0.00006509
    Epoch:  1140     LossContext: 0.00006508
    Epoch:  1150     LossContext: 0.00006506
    Epoch:  1160     LossContext: 0.00006505
    Epoch:  1170     LossContext: 0.00006504
    Epoch:  1180     LossContext: 0.00006502
    Epoch:  1190     LossContext: 0.00006501
    Epoch:  1200     LossContext: 0.00006500
    Epoch:  1210     LossContext: 0.00006498
    Epoch:  1220     LossContext: 0.00006497
    Epoch:  1230     LossContext: 0.00006496
    Epoch:  1240     LossContext: 0.00006494
    Epoch:  1250     LossContext: 0.00006493
    Epoch:  1260     LossContext: 0.00006491
    Epoch:  1270     LossContext: 0.00006490
    Epoch:  1280     LossContext: 0.00006488
    Epoch:  1290     LossContext: 0.00006487
    Epoch:  1300     LossContext: 0.00006486
    Epoch:  1310     LossContext: 0.00006484
    Epoch:  1320     LossContext: 0.00006483
    Epoch:  1330     LossContext: 0.00006481
    Epoch:  1340     LossContext: 0.00006480
    Epoch:  1350     LossContext: 0.00006478
    Epoch:  1360     LossContext: 0.00006477
    Epoch:  1370     LossContext: 0.00006475
    Epoch:  1380     LossContext: 0.00006474
    Epoch:  1390     LossContext: 0.00006472
    Epoch:  1400     LossContext: 0.00006471
    Epoch:  1410     LossContext: 0.00006469
    Epoch:  1420     LossContext: 0.00006467
    Epoch:  1430     LossContext: 0.00006466
    Epoch:  1440     LossContext: 0.00006464
    Epoch:  1450     LossContext: 0.00006463
    Epoch:  1460     LossContext: 0.00006461
    Epoch:  1470     LossContext: 0.00006459
    Epoch:  1480     LossContext: 0.00006458
    Epoch:  1490     LossContext: 0.00006456
    Epoch:  1499     LossContext: 0.00006455

Gradient descent adaptation time: 0 hours 1 mins 48 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.13008757
    Epoch:     1     LossContext: 0.11644365
    Epoch:     2     LossContext: 0.10231050
    Epoch:     3     LossContext: 0.08782846
    Epoch:    10     LossContext: 0.00767996
    Epoch:    20     LossContext: 0.00323326
    Epoch:    30     LossContext: 0.00249089
    Epoch:    40     LossContext: 0.00107834
    Epoch:    50     LossContext: 0.00071667
    Epoch:    60     LossContext: 0.00021508
    Epoch:    70     LossContext: 0.00016432
    Epoch:    80     LossContext: 0.00017490
    Epoch:    90     LossContext: 0.00015854
    Epoch:   100     LossContext: 0.00015322
    Epoch:   110     LossContext: 0.00015253
    Epoch:   120     LossContext: 0.00015180
    Epoch:   130     LossContext: 0.00015172
    Epoch:   140     LossContext: 0.00015173
    Epoch:   150     LossContext: 0.00015168
    Epoch:   160     LossContext: 0.00015166
    Epoch:   170     LossContext: 0.00015165
    Epoch:   180     LossContext: 0.00015164
    Epoch:   190     LossContext: 0.00015163
    Epoch:   200     LossContext: 0.00015162
    Epoch:   210     LossContext: 0.00015162
    Epoch:   220     LossContext: 0.00015161
    Epoch:   230     LossContext: 0.00015160
    Epoch:   240     LossContext: 0.00015159
    Epoch:   250     LossContext: 0.00015158
    Epoch:   260     LossContext: 0.00015158
    Epoch:   270     LossContext: 0.00015157
    Epoch:   280     LossContext: 0.00015156
    Epoch:   290     LossContext: 0.00015155
    Epoch:   300     LossContext: 0.00015154
    Epoch:   310     LossContext: 0.00015153
    Epoch:   320     LossContext: 0.00015152
    Epoch:   330     LossContext: 0.00015151
    Epoch:   340     LossContext: 0.00015150
    Epoch:   350     LossContext: 0.00015150
    Epoch:   360     LossContext: 0.00015149
    Epoch:   370     LossContext: 0.00015149
    Epoch:   380     LossContext: 0.00015148
    Epoch:   390     LossContext: 0.00015148
    Epoch:   400     LossContext: 0.00015147
    Epoch:   410     LossContext: 0.00015147
    Epoch:   420     LossContext: 0.00015146
    Epoch:   430     LossContext: 0.00015146
    Epoch:   440     LossContext: 0.00015145
    Epoch:   450     LossContext: 0.00015144
    Epoch:   460     LossContext: 0.00015144
    Epoch:   470     LossContext: 0.00015143
    Epoch:   480     LossContext: 0.00015143
    Epoch:   490     LossContext: 0.00015142
    Epoch:   500     LossContext: 0.00015141
    Epoch:   510     LossContext: 0.00015141
    Epoch:   520     LossContext: 0.00015140
    Epoch:   530     LossContext: 0.00015139
    Epoch:   540     LossContext: 0.00015139
    Epoch:   550     LossContext: 0.00015138
    Epoch:   560     LossContext: 0.00015137
    Epoch:   570     LossContext: 0.00015137
    Epoch:   580     LossContext: 0.00015136
    Epoch:   590     LossContext: 0.00015135
    Epoch:   600     LossContext: 0.00015135
    Epoch:   610     LossContext: 0.00015134
    Epoch:   620     LossContext: 0.00015133
    Epoch:   630     LossContext: 0.00015132
    Epoch:   640     LossContext: 0.00015132
    Epoch:   650     LossContext: 0.00015131
    Epoch:   660     LossContext: 0.00015130
    Epoch:   670     LossContext: 0.00015129
    Epoch:   680     LossContext: 0.00015129
    Epoch:   690     LossContext: 0.00015128
    Epoch:   700     LossContext: 0.00015127
    Epoch:   710     LossContext: 0.00015126
    Epoch:   720     LossContext: 0.00015126
    Epoch:   730     LossContext: 0.00015125
    Epoch:   740     LossContext: 0.00015124
    Epoch:   750     LossContext: 0.00015123
    Epoch:   760     LossContext: 0.00015122
    Epoch:   770     LossContext: 0.00015121
    Epoch:   780     LossContext: 0.00015121
    Epoch:   790     LossContext: 0.00015120
    Epoch:   800     LossContext: 0.00015119
    Epoch:   810     LossContext: 0.00015118
    Epoch:   820     LossContext: 0.00015117
    Epoch:   830     LossContext: 0.00015116
    Epoch:   840     LossContext: 0.00015115
    Epoch:   850     LossContext: 0.00015114
    Epoch:   860     LossContext: 0.00015114
    Epoch:   870     LossContext: 0.00015113
    Epoch:   880     LossContext: 0.00015112
    Epoch:   890     LossContext: 0.00015111
    Epoch:   900     LossContext: 0.00015110
    Epoch:   910     LossContext: 0.00015109
    Epoch:   920     LossContext: 0.00015108
    Epoch:   930     LossContext: 0.00015107
    Epoch:   940     LossContext: 0.00015106
    Epoch:   950     LossContext: 0.00015105
    Epoch:   960     LossContext: 0.00015104
    Epoch:   970     LossContext: 0.00015103
    Epoch:   980     LossContext: 0.00015102
    Epoch:   990     LossContext: 0.00015101
    Epoch:  1000     LossContext: 0.00015100
    Epoch:  1010     LossContext: 0.00015099
    Epoch:  1020     LossContext: 0.00015098
    Epoch:  1030     LossContext: 0.00015097
    Epoch:  1040     LossContext: 0.00015096
    Epoch:  1050     LossContext: 0.00015095
    Epoch:  1060     LossContext: 0.00015094
    Epoch:  1070     LossContext: 0.00015093
    Epoch:  1080     LossContext: 0.00015092
    Epoch:  1090     LossContext: 0.00015091
    Epoch:  1100     LossContext: 0.00015090
    Epoch:  1110     LossContext: 0.00015089
    Epoch:  1120     LossContext: 0.00015087
    Epoch:  1130     LossContext: 0.00015086
    Epoch:  1140     LossContext: 0.00015085
    Epoch:  1150     LossContext: 0.00015084
    Epoch:  1160     LossContext: 0.00015083
    Epoch:  1170     LossContext: 0.00015082
    Epoch:  1180     LossContext: 0.00015081
    Epoch:  1190     LossContext: 0.00015079
    Epoch:  1200     LossContext: 0.00015078
    Epoch:  1210     LossContext: 0.00015077
    Epoch:  1220     LossContext: 0.00015076
    Epoch:  1230     LossContext: 0.00015075
    Epoch:  1240     LossContext: 0.00015074
    Epoch:  1250     LossContext: 0.00015072
    Epoch:  1260     LossContext: 0.00015071
    Epoch:  1270     LossContext: 0.00015070
    Epoch:  1280     LossContext: 0.00015069
    Epoch:  1290     LossContext: 0.00015067
    Epoch:  1300     LossContext: 0.00015066
    Epoch:  1310     LossContext: 0.00015065
    Epoch:  1320     LossContext: 0.00015064
    Epoch:  1330     LossContext: 0.00015062
    Epoch:  1340     LossContext: 0.00015061
    Epoch:  1350     LossContext: 0.00015060
    Epoch:  1360     LossContext: 0.00015059
    Epoch:  1370     LossContext: 0.00015057
    Epoch:  1380     LossContext: 0.00015056
    Epoch:  1390     LossContext: 0.00015055
    Epoch:  1400     LossContext: 0.00015053
    Epoch:  1410     LossContext: 0.00015052
    Epoch:  1420     LossContext: 0.00015051
    Epoch:  1430     LossContext: 0.00015049
    Epoch:  1440     LossContext: 0.00015048
    Epoch:  1450     LossContext: 0.00015047
    Epoch:  1460     LossContext: 0.00015045
    Epoch:  1470     LossContext: 0.00015044
    Epoch:  1480     LossContext: 0.00015043
    Epoch:  1490     LossContext: 0.00015041
    Epoch:  1499     LossContext: 0.00015040

Gradient descent adaptation time: 0 hours 1 mins 35 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.04660499
    Epoch:     1     LossContext: 0.04008409
    Epoch:     2     LossContext: 0.03425054
    Epoch:     3     LossContext: 0.02896573
    Epoch:    10     LossContext: 0.00653729
    Epoch:    20     LossContext: 0.00030897
    Epoch:    30     LossContext: 0.00026918
    Epoch:    40     LossContext: 0.00042522
    Epoch:    50     LossContext: 0.00034385
    Epoch:    60     LossContext: 0.00021654
    Epoch:    70     LossContext: 0.00015310
    Epoch:    80     LossContext: 0.00014251
    Epoch:    90     LossContext: 0.00014417
    Epoch:   100     LossContext: 0.00014349
    Epoch:   110     LossContext: 0.00014229
    Epoch:   120     LossContext: 0.00014191
    Epoch:   130     LossContext: 0.00014179
    Epoch:   140     LossContext: 0.00014160
    Epoch:   150     LossContext: 0.00014141
    Epoch:   160     LossContext: 0.00014123
    Epoch:   170     LossContext: 0.00014108
    Epoch:   180     LossContext: 0.00014099
    Epoch:   190     LossContext: 0.00014089
    Epoch:   200     LossContext: 0.00014079
    Epoch:   210     LossContext: 0.00014068
    Epoch:   220     LossContext: 0.00014058
    Epoch:   230     LossContext: 0.00014047
    Epoch:   240     LossContext: 0.00014035
    Epoch:   250     LossContext: 0.00014024
    Epoch:   260     LossContext: 0.00014012
    Epoch:   270     LossContext: 0.00014000
    Epoch:   280     LossContext: 0.00013988
    Epoch:   290     LossContext: 0.00013975
    Epoch:   300     LossContext: 0.00013963
    Epoch:   310     LossContext: 0.00013950
    Epoch:   320     LossContext: 0.00013937
    Epoch:   330     LossContext: 0.00013923
    Epoch:   340     LossContext: 0.00013914
    Epoch:   350     LossContext: 0.00013907
    Epoch:   360     LossContext: 0.00013900
    Epoch:   370     LossContext: 0.00013893
    Epoch:   380     LossContext: 0.00013886
    Epoch:   390     LossContext: 0.00013878
    Epoch:   400     LossContext: 0.00013871
    Epoch:   410     LossContext: 0.00013863
    Epoch:   420     LossContext: 0.00013855
    Epoch:   430     LossContext: 0.00013847
    Epoch:   440     LossContext: 0.00013839
    Epoch:   450     LossContext: 0.00013831
    Epoch:   460     LossContext: 0.00013823
    Epoch:   470     LossContext: 0.00013815
    Epoch:   480     LossContext: 0.00013806
    Epoch:   490     LossContext: 0.00013798
    Epoch:   500     LossContext: 0.00013789
    Epoch:   510     LossContext: 0.00013780
    Epoch:   520     LossContext: 0.00013772
    Epoch:   530     LossContext: 0.00013763
    Epoch:   540     LossContext: 0.00013754
    Epoch:   550     LossContext: 0.00013744
    Epoch:   560     LossContext: 0.00013735
    Epoch:   570     LossContext: 0.00013726
    Epoch:   580     LossContext: 0.00013716
    Epoch:   590     LossContext: 0.00013707
    Epoch:   600     LossContext: 0.00013697
    Epoch:   610     LossContext: 0.00013687
    Epoch:   620     LossContext: 0.00013677
    Epoch:   630     LossContext: 0.00013667
    Epoch:   640     LossContext: 0.00013657
    Epoch:   650     LossContext: 0.00013647
    Epoch:   660     LossContext: 0.00013637
    Epoch:   670     LossContext: 0.00013626
    Epoch:   680     LossContext: 0.00013616
    Epoch:   690     LossContext: 0.00013605
    Epoch:   700     LossContext: 0.00013594
    Epoch:   710     LossContext: 0.00013584
    Epoch:   720     LossContext: 0.00013573
    Epoch:   730     LossContext: 0.00013562
    Epoch:   740     LossContext: 0.00013550
    Epoch:   750     LossContext: 0.00013539
    Epoch:   760     LossContext: 0.00013529
    Epoch:   770     LossContext: 0.00013528
    Epoch:   780     LossContext: 0.00013526
    Epoch:   790     LossContext: 0.00013523
    Epoch:   800     LossContext: 0.00013522
    Epoch:   810     LossContext: 0.00013520
    Epoch:   820     LossContext: 0.00013519
    Epoch:   830     LossContext: 0.00013517
    Epoch:   840     LossContext: 0.00013515
    Epoch:   850     LossContext: 0.00013514
    Epoch:   860     LossContext: 0.00013512
    Epoch:   870     LossContext: 0.00013511
    Epoch:   880     LossContext: 0.00013509
    Epoch:   890     LossContext: 0.00013508
    Epoch:   900     LossContext: 0.00013506
    Epoch:   910     LossContext: 0.00013504
    Epoch:   920     LossContext: 0.00013503
    Epoch:   930     LossContext: 0.00013501
    Epoch:   940     LossContext: 0.00013499
    Epoch:   950     LossContext: 0.00013497
    Epoch:   960     LossContext: 0.00013496
    Epoch:   970     LossContext: 0.00013494
    Epoch:   980     LossContext: 0.00013492
    Epoch:   990     LossContext: 0.00013490
    Epoch:  1000     LossContext: 0.00013489
    Epoch:  1010     LossContext: 0.00013487
    Epoch:  1020     LossContext: 0.00013485
    Epoch:  1030     LossContext: 0.00013483
    Epoch:  1040     LossContext: 0.00013481
    Epoch:  1050     LossContext: 0.00013479
    Epoch:  1060     LossContext: 0.00013478
    Epoch:  1070     LossContext: 0.00013476
    Epoch:  1080     LossContext: 0.00013474
    Epoch:  1090     LossContext: 0.00013472
    Epoch:  1100     LossContext: 0.00013470
    Epoch:  1110     LossContext: 0.00013468
    Epoch:  1120     LossContext: 0.00013466
    Epoch:  1130     LossContext: 0.00013464
    Epoch:  1140     LossContext: 0.00013462
    Epoch:  1150     LossContext: 0.00013460
    Epoch:  1160     LossContext: 0.00013458
    Epoch:  1170     LossContext: 0.00013456
    Epoch:  1180     LossContext: 0.00013455
    Epoch:  1190     LossContext: 0.00013452
    Epoch:  1200     LossContext: 0.00013450
    Epoch:  1210     LossContext: 0.00013448
    Epoch:  1220     LossContext: 0.00013446
    Epoch:  1230     LossContext: 0.00013444
    Epoch:  1240     LossContext: 0.00013442
    Epoch:  1250     LossContext: 0.00013440
    Epoch:  1260     LossContext: 0.00013438
    Epoch:  1270     LossContext: 0.00013436
    Epoch:  1280     LossContext: 0.00013434
    Epoch:  1290     LossContext: 0.00013432
    Epoch:  1300     LossContext: 0.00013429
    Epoch:  1310     LossContext: 0.00013427
    Epoch:  1320     LossContext: 0.00013425
    Epoch:  1330     LossContext: 0.00013423
    Epoch:  1340     LossContext: 0.00013421
    Epoch:  1350     LossContext: 0.00013418
    Epoch:  1360     LossContext: 0.00013416
    Epoch:  1370     LossContext: 0.00013414
    Epoch:  1380     LossContext: 0.00013412
    Epoch:  1390     LossContext: 0.00013409
    Epoch:  1400     LossContext: 0.00013407
    Epoch:  1410     LossContext: 0.00013405
    Epoch:  1420     LossContext: 0.00013403
    Epoch:  1430     LossContext: 0.00013400
    Epoch:  1440     LossContext: 0.00013398
    Epoch:  1450     LossContext: 0.00013396
    Epoch:  1460     LossContext: 0.00013393
    Epoch:  1470     LossContext: 0.00013391
    Epoch:  1480     LossContext: 0.00013389
    Epoch:  1490     LossContext: 0.00013386
    Epoch:  1499     LossContext: 0.00013384

Gradient descent adaptation time: 0 hours 1 mins 35 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.03214216
    Epoch:     1     LossContext: 0.02595636
    Epoch:     2     LossContext: 0.02079809
    Epoch:     3     LossContext: 0.01657874
    Epoch:    10     LossContext: 0.00294926
    Epoch:    20     LossContext: 0.00027236
    Epoch:    30     LossContext: 0.00031037
    Epoch:    40     LossContext: 0.00036035
    Epoch:    50     LossContext: 0.00032017
    Epoch:    60     LossContext: 0.00025513
    Epoch:    70     LossContext: 0.00021841
    Epoch:    80     LossContext: 0.00020840
    Epoch:    90     LossContext: 0.00020758
    Epoch:   100     LossContext: 0.00020746
    Epoch:   110     LossContext: 0.00020675
    Epoch:   120     LossContext: 0.00020609
    Epoch:   130     LossContext: 0.00020559
    Epoch:   140     LossContext: 0.00020512
    Epoch:   150     LossContext: 0.00020463
    Epoch:   160     LossContext: 0.00020411
    Epoch:   170     LossContext: 0.00020370
    Epoch:   180     LossContext: 0.00020343
    Epoch:   190     LossContext: 0.00020315
    Epoch:   200     LossContext: 0.00020287
    Epoch:   210     LossContext: 0.00020258
    Epoch:   220     LossContext: 0.00020228
    Epoch:   230     LossContext: 0.00020197
    Epoch:   240     LossContext: 0.00020180
    Epoch:   250     LossContext: 0.00020170
    Epoch:   260     LossContext: 0.00020156
    Epoch:   270     LossContext: 0.00020144
    Epoch:   280     LossContext: 0.00020132
    Epoch:   290     LossContext: 0.00020120
    Epoch:   300     LossContext: 0.00020108
    Epoch:   310     LossContext: 0.00020095
    Epoch:   320     LossContext: 0.00020083
    Epoch:   330     LossContext: 0.00020070
    Epoch:   340     LossContext: 0.00020061
    Epoch:   350     LossContext: 0.00020054
    Epoch:   360     LossContext: 0.00020048
    Epoch:   370     LossContext: 0.00020041
    Epoch:   380     LossContext: 0.00020034
    Epoch:   390     LossContext: 0.00020027
    Epoch:   400     LossContext: 0.00020020
    Epoch:   410     LossContext: 0.00020013
    Epoch:   420     LossContext: 0.00020005
    Epoch:   430     LossContext: 0.00019997
    Epoch:   440     LossContext: 0.00019990
    Epoch:   450     LossContext: 0.00019982
    Epoch:   460     LossContext: 0.00019974
    Epoch:   470     LossContext: 0.00019967
    Epoch:   480     LossContext: 0.00019959
    Epoch:   490     LossContext: 0.00019951
    Epoch:   500     LossContext: 0.00019942
    Epoch:   510     LossContext: 0.00019934
    Epoch:   520     LossContext: 0.00019926
    Epoch:   530     LossContext: 0.00019917
    Epoch:   540     LossContext: 0.00019909
    Epoch:   550     LossContext: 0.00019900
    Epoch:   560     LossContext: 0.00019891
    Epoch:   570     LossContext: 0.00019882
    Epoch:   580     LossContext: 0.00019874
    Epoch:   590     LossContext: 0.00019864
    Epoch:   600     LossContext: 0.00019855
    Epoch:   610     LossContext: 0.00019846
    Epoch:   620     LossContext: 0.00019837
    Epoch:   630     LossContext: 0.00019828
    Epoch:   640     LossContext: 0.00019818
    Epoch:   650     LossContext: 0.00019808
    Epoch:   660     LossContext: 0.00019799
    Epoch:   670     LossContext: 0.00019789
    Epoch:   680     LossContext: 0.00019779
    Epoch:   690     LossContext: 0.00019769
    Epoch:   700     LossContext: 0.00019759
    Epoch:   710     LossContext: 0.00019749
    Epoch:   720     LossContext: 0.00019739
    Epoch:   730     LossContext: 0.00019729
    Epoch:   740     LossContext: 0.00019718
    Epoch:   750     LossContext: 0.00019708
    Epoch:   760     LossContext: 0.00019697
    Epoch:   770     LossContext: 0.00019686
    Epoch:   780     LossContext: 0.00019676
    Epoch:   790     LossContext: 0.00019665
    Epoch:   800     LossContext: 0.00019654
    Epoch:   810     LossContext: 0.00019643
    Epoch:   820     LossContext: 0.00019632
    Epoch:   830     LossContext: 0.00019621
    Epoch:   840     LossContext: 0.00019610
    Epoch:   850     LossContext: 0.00019598
    Epoch:   860     LossContext: 0.00019587
    Epoch:   870     LossContext: 0.00019575
    Epoch:   880     LossContext: 0.00019563
    Epoch:   890     LossContext: 0.00019552
    Epoch:   900     LossContext: 0.00019540
    Epoch:   910     LossContext: 0.00019528
    Epoch:   920     LossContext: 0.00019516
    Epoch:   930     LossContext: 0.00019504
    Epoch:   940     LossContext: 0.00019492
    Epoch:   950     LossContext: 0.00019480
    Epoch:   960     LossContext: 0.00019468
    Epoch:   970     LossContext: 0.00019456
    Epoch:   980     LossContext: 0.00019443
    Epoch:   990     LossContext: 0.00019431
    Epoch:  1000     LossContext: 0.00019418
    Epoch:  1010     LossContext: 0.00019405
    Epoch:  1020     LossContext: 0.00019393
    Epoch:  1030     LossContext: 0.00019380
    Epoch:  1040     LossContext: 0.00019367
    Epoch:  1050     LossContext: 0.00019354
    Epoch:  1060     LossContext: 0.00019341
    Epoch:  1070     LossContext: 0.00019328
    Epoch:  1080     LossContext: 0.00019314
    Epoch:  1090     LossContext: 0.00019301
    Epoch:  1100     LossContext: 0.00019288
    Epoch:  1110     LossContext: 0.00019274
    Epoch:  1120     LossContext: 0.00019261
    Epoch:  1130     LossContext: 0.00019247
    Epoch:  1140     LossContext: 0.00019233
    Epoch:  1150     LossContext: 0.00019220
    Epoch:  1160     LossContext: 0.00019206
    Epoch:  1170     LossContext: 0.00019191
    Epoch:  1180     LossContext: 0.00019177
    Epoch:  1190     LossContext: 0.00019164
    Epoch:  1200     LossContext: 0.00019149
    Epoch:  1210     LossContext: 0.00019135
    Epoch:  1220     LossContext: 0.00019121
    Epoch:  1230     LossContext: 0.00019106
    Epoch:  1240     LossContext: 0.00019092
    Epoch:  1250     LossContext: 0.00019077
    Epoch:  1260     LossContext: 0.00019062
    Epoch:  1270     LossContext: 0.00019048
    Epoch:  1280     LossContext: 0.00019033
    Epoch:  1290     LossContext: 0.00019018
    Epoch:  1300     LossContext: 0.00019004
    Epoch:  1310     LossContext: 0.00018988
    Epoch:  1320     LossContext: 0.00018973
    Epoch:  1330     LossContext: 0.00018958
    Epoch:  1340     LossContext: 0.00018942
    Epoch:  1350     LossContext: 0.00018927
    Epoch:  1360     LossContext: 0.00018912
    Epoch:  1370     LossContext: 0.00018896
    Epoch:  1380     LossContext: 0.00018881
    Epoch:  1390     LossContext: 0.00018865
    Epoch:  1400     LossContext: 0.00018849
    Epoch:  1410     LossContext: 0.00018834
    Epoch:  1420     LossContext: 0.00018818
    Epoch:  1430     LossContext: 0.00018802
    Epoch:  1440     LossContext: 0.00018785
    Epoch:  1450     LossContext: 0.00018770
    Epoch:  1460     LossContext: 0.00018753
    Epoch:  1470     LossContext: 0.00018738
    Epoch:  1480     LossContext: 0.00018721
    Epoch:  1490     LossContext: 0.00018704
    Epoch:  1499     LossContext: 0.00018690

Gradient descent adaptation time: 0 hours 1 mins 35 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./05052024-130226/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 0.0001427407

==  Begining out-of-distribution visualisation ... ==
    Environment id: 0
    Trajectory id: 0
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./05052024-130226/adapt/results_ood.png
Testing finished. Figure saved in: ./05052024-130226/adapt/results_ood_uq.pdf

======================= DONE ======================== 

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

========= STARTING =========
Working with a context size of 2^4 = 16
Run folder created successfuly: ./05052024-145443/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 145500
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 145501
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 50192 


WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed


=== Beginning training with proximal alternating minimization ... ===
    Number of examples in a batch: 4
    Maximum number of steps per inner minimization: 25
    Maximum number of outer minimizations: 500
    Maximum total number of training steps: 12500

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Outer Step:     0      LossTrajs: 1.81045318     ContextsNorm: 0.00000000     ValIndCrit: 1.69878161
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.72e-06
        -DiffCxt:  inf
    Outer Step:     1      LossTrajs: 1.52237475     ContextsNorm: 0.00026241     ValIndCrit: 1.42706156
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.64e-06
        -DiffCxt:  1.64e-03
    Outer Step:     2      LossTrajs: 1.20568419     ContextsNorm: 0.00052571     ValIndCrit: 1.12770617
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.45e-06
        -DiffCxt:  9.14e-04
    Outer Step:     3      LossTrajs: 0.76364887     ContextsNorm: 0.00140946     ValIndCrit: 0.71543795
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.54e-06
        -DiffCxt:  2.86e-04
    Outer Step:    10      LossTrajs: 0.24822618     ContextsNorm: 0.00645833     ValIndCrit: 0.29349387
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.17e-07
        -DiffCxt:  1.22e-05
    Outer Step:    20      LossTrajs: 0.17750993     ContextsNorm: 0.01777588     ValIndCrit: 0.19871624
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.63e-07
        -DiffCxt:  3.58e-06
    Outer Step:    30      LossTrajs: 0.07944080     ContextsNorm: 0.02894101     ValIndCrit: 0.08241788
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.98e-07
        -DiffCxt:  9.09e-07
    Outer Step:    40      LossTrajs: 0.05068760     ContextsNorm: 0.03303586     ValIndCrit: 0.05344844
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.20e-08
        -DiffCxt:  1.45e-07
    Outer Step:    50      LossTrajs: 0.03649868     ContextsNorm: 0.03738633     ValIndCrit: 0.04029170
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.16e-07
        -DiffCxt:  2.81e-07
    Outer Step:    60      LossTrajs: 0.03073393     ContextsNorm: 0.03875263     ValIndCrit: 0.03427271
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.31e-08
        -DiffCxt:  1.00e-07
    Outer Step:    70      LossTrajs: 0.02683577     ContextsNorm: 0.04004709     ValIndCrit: 0.03024586
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.53e-08
        -DiffCxt:  2.53e-07
    Outer Step:    80      LossTrajs: 0.02405883     ContextsNorm: 0.04328595     ValIndCrit: 0.02795221
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.58e-08
        -DiffCxt:  1.32e-07
    Outer Step:    90      LossTrajs: 0.01087414     ContextsNorm: 0.04903500     ValIndCrit: 0.01429391
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.28e-08
        -DiffCxt:  1.44e-07
    Outer Step:   100      LossTrajs: 0.00707791     ContextsNorm: 0.04923319     ValIndCrit: 0.01021012
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.59e-08
        -DiffCxt:  1.47e-07
    Outer Step:   110      LossTrajs: 0.00514456     ContextsNorm: 0.04892081     ValIndCrit: 0.00794977
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.66e-08
        -DiffCxt:  4.33e-08
    Outer Step:   120      LossTrajs: 0.00360141     ContextsNorm: 0.04888959     ValIndCrit: 0.00559931
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.64e-08
        -DiffCxt:  3.60e-08
    Outer Step:   130      LossTrajs: 0.00185631     ContextsNorm: 0.04934995     ValIndCrit: 0.00300007
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.59e-08
        -DiffCxt:  8.08e-08
    Outer Step:   140      LossTrajs: 0.00103922     ContextsNorm: 0.04897942     ValIndCrit: 0.00179340
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.81e-09
        -DiffCxt:  2.15e-08
    Outer Step:   150      LossTrajs: 0.00074521     ContextsNorm: 0.04879444     ValIndCrit: 0.00136798
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.71e-09
        -DiffCxt:  1.94e-08
    Outer Step:   160      LossTrajs: 0.00060074     ContextsNorm: 0.04872599     ValIndCrit: 0.00113475
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.65e-09
        -DiffCxt:  1.59e-08
    Outer Step:   170      LossTrajs: 0.00050771     ContextsNorm: 0.04882848     ValIndCrit: 0.00090296
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.44e-09
        -DiffCxt:  2.43e-08
    Outer Step:   180      LossTrajs: 0.00040930     ContextsNorm: 0.04891434     ValIndCrit: 0.00071911
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.74e-09
        -DiffCxt:  1.49e-08
    Outer Step:   190      LossTrajs: 0.00032989     ContextsNorm: 0.04866698     ValIndCrit: 0.00057861
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.26e-08
        -DiffCxt:  1.71e-08
    Outer Step:   200      LossTrajs: 0.00026383     ContextsNorm: 0.04847189     ValIndCrit: 0.00047914
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.38e-09
        -DiffCxt:  3.30e-08
    Outer Step:   210      LossTrajs: 0.00022179     ContextsNorm: 0.04855004     ValIndCrit: 0.00039710
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.25e-09
        -DiffCxt:  2.64e-08
    Outer Step:   220      LossTrajs: 0.00018883     ContextsNorm: 0.04854861     ValIndCrit: 0.00033683
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.93e-09
        -DiffCxt:  2.69e-08
    Outer Step:   230      LossTrajs: 0.00016454     ContextsNorm: 0.04858901     ValIndCrit: 0.00029566
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   24
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.91e-09
        -DiffCxt:  9.80e-09
    Outer Step:   240      LossTrajs: 0.00014839     ContextsNorm: 0.04847475     ValIndCrit: 0.00026365
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.71e-09
        -DiffCxt:  1.73e-08
    Outer Step:   250      LossTrajs: 0.00014354     ContextsNorm: 0.04835805     ValIndCrit: 0.00024191
        Saving best model so far ...
        -NbInnerStepsNode:    2
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.76e-10
        -DiffCxt:  7.81e-09
    Outer Step:   260      LossTrajs: 0.00013523     ContextsNorm: 0.04828035     ValIndCrit: 0.00022306
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   21
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.07e-09
        -DiffCxt:  9.57e-09
    Outer Step:   270      LossTrajs: 0.00013515     ContextsNorm: 0.04828053     ValIndCrit: 0.00022195
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.80e-09
        -DiffCxt:  1.34e-08
    Outer Step:   280      LossTrajs: 0.00012123     ContextsNorm: 0.04826518     ValIndCrit: 0.00019290
        Saving best model so far ...
        -NbInnerStepsNode:   24
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.61e-10
        -DiffCxt:  1.26e-08
    Outer Step:   290      LossTrajs: 0.00011680     ContextsNorm: 0.04817518     ValIndCrit: 0.00017837
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.79e-10
        -DiffCxt:  1.06e-08
    Outer Step:   300      LossTrajs: 0.00011248     ContextsNorm: 0.04803026     ValIndCrit: 0.00016521
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.36e-09
        -DiffCxt:  2.26e-08
    Outer Step:   310      LossTrajs: 0.00010659     ContextsNorm: 0.04790222     ValIndCrit: 0.00014987
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   23
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.22e-09
        -DiffCxt:  7.79e-09
    Outer Step:   320      LossTrajs: 0.00010097     ContextsNorm: 0.04782774     ValIndCrit: 0.00015055
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   21
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.63e-09
        -DiffCxt:  9.02e-09
    Outer Step:   330      LossTrajs: 0.00010476     ContextsNorm: 0.04776006     ValIndCrit: 0.00014081
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.34e-08
        -DiffCxt:  7.14e-08
    Outer Step:   340      LossTrajs: 0.00009842     ContextsNorm: 0.04772498     ValIndCrit: 0.00012935
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.69e-09
        -DiffCxt:  2.07e-08
    Outer Step:   350      LossTrajs: 0.00009380     ContextsNorm: 0.04761854     ValIndCrit: 0.00012578
        Saving best model so far ...
        -NbInnerStepsNode:    3
        -NbInnerStepsCxt:   22
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.32e-10
        -DiffCxt:  8.43e-09
    Outer Step:   360      LossTrajs: 0.00008985     ContextsNorm: 0.04745771     ValIndCrit: 0.00011875
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   19
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.59e-09
        -DiffCxt:  8.56e-09
    Outer Step:   370      LossTrajs: 0.00009345     ContextsNorm: 0.04747659     ValIndCrit: 0.00011960
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.91e-09
        -DiffCxt:  1.47e-08
    Outer Step:   380      LossTrajs: 0.00008641     ContextsNorm: 0.04743068     ValIndCrit: 0.00011544
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.26e-09
        -DiffCxt:  2.33e-08
    Outer Step:   390      LossTrajs: 0.00008553     ContextsNorm: 0.04739036     ValIndCrit: 0.00011247
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   17
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.86e-10
        -DiffCxt:  9.51e-09
    Outer Step:   400      LossTrajs: 0.00008767     ContextsNorm: 0.04742086     ValIndCrit: 0.00011317
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.92e-09
        -DiffCxt:  1.04e-08
    Outer Step:   410      LossTrajs: 0.00008368     ContextsNorm: 0.04732735     ValIndCrit: 0.00010574
        Saving best model so far ...
        -NbInnerStepsNode:    2
        -NbInnerStepsCxt:   17
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.38e-10
        -DiffCxt:  8.38e-09
    Outer Step:   420      LossTrajs: 0.00008201     ContextsNorm: 0.04724546     ValIndCrit: 0.00010481
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.36e-09
        -DiffCxt:  4.05e-08
    Outer Step:   430      LossTrajs: 0.00008080     ContextsNorm: 0.04714006     ValIndCrit: 0.00010532
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.90e-09
        -DiffCxt:  5.06e-08
    Outer Step:   440      LossTrajs: 0.00007956     ContextsNorm: 0.04710544     ValIndCrit: 0.00009994
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   18
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.28e-09
        -DiffCxt:  9.97e-09
    Outer Step:   450      LossTrajs: 0.00007964     ContextsNorm: 0.04700675     ValIndCrit: 0.00009958
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.98e-10
        -DiffCxt:  1.13e-08
    Outer Step:   460      LossTrajs: 0.00008107     ContextsNorm: 0.04688929     ValIndCrit: 0.00009924
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   21
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.71e-09
        -DiffCxt:  7.95e-09
    Outer Step:   470      LossTrajs: 0.00008003     ContextsNorm: 0.04682837     ValIndCrit: 0.00009455
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.90e-09
        -DiffCxt:  4.32e-08
    Outer Step:   480      LossTrajs: 0.00007633     ContextsNorm: 0.04685109     ValIndCrit: 0.00009510
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.69e-09
        -DiffCxt:  1.93e-08
    Outer Step:   490      LossTrajs: 0.00007546     ContextsNorm: 0.04680955     ValIndCrit: 0.00008868
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.65e-09
        -DiffCxt:  2.71e-08
    Outer Step:   499      LossTrajs: 0.00007515     ContextsNorm: 0.04672141     ValIndCrit: 0.00009137
        -NbInnerStepsNode:    5
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.14e-10
        -DiffCxt:  1.46e-08

Total gradient descent training time: 1 hours 46 mins 6 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 164108
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 8.8680885e-05


############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]
==  Begining in-domain visualisation ... ==
    Environment id: 0
    Trajectory id: 15
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./05052024-145443/results_in_domain.png
Testing finished. Figure saved in: ./05052024-145443/results_in_domain_uq.pdf
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.01787836
    Epoch:     1     LossContext: 0.01376297
    Epoch:     2     LossContext: 0.00986334
    Epoch:     3     LossContext: 0.00638602
    Epoch:    10     LossContext: 0.00204478
    Epoch:    20     LossContext: 0.00007785
    Epoch:    30     LossContext: 0.00036306
    Epoch:    40     LossContext: 0.00021419
    Epoch:    50     LossContext: 0.00008159
    Epoch:    60     LossContext: 0.00005808
    Epoch:    70     LossContext: 0.00005911
    Epoch:    80     LossContext: 0.00005918
    Epoch:    90     LossContext: 0.00005841
    Epoch:   100     LossContext: 0.00005783
    Epoch:   110     LossContext: 0.00005748
    Epoch:   120     LossContext: 0.00005724
    Epoch:   130     LossContext: 0.00005706
    Epoch:   140     LossContext: 0.00005688
    Epoch:   150     LossContext: 0.00005671
    Epoch:   160     LossContext: 0.00005653
    Epoch:   170     LossContext: 0.00005639
    Epoch:   180     LossContext: 0.00005629
    Epoch:   190     LossContext: 0.00005619
    Epoch:   200     LossContext: 0.00005609
    Epoch:   210     LossContext: 0.00005599
    Epoch:   220     LossContext: 0.00005589
    Epoch:   230     LossContext: 0.00005578
    Epoch:   240     LossContext: 0.00005567
    Epoch:   250     LossContext: 0.00005558
    Epoch:   260     LossContext: 0.00005548
    Epoch:   270     LossContext: 0.00005539
    Epoch:   280     LossContext: 0.00005531
    Epoch:   290     LossContext: 0.00005521
    Epoch:   300     LossContext: 0.00005512
    Epoch:   310     LossContext: 0.00005502
    Epoch:   320     LossContext: 0.00005493
    Epoch:   330     LossContext: 0.00005483
    Epoch:   340     LossContext: 0.00005476
    Epoch:   350     LossContext: 0.00005471
    Epoch:   360     LossContext: 0.00005466
    Epoch:   370     LossContext: 0.00005460
    Epoch:   380     LossContext: 0.00005455
    Epoch:   390     LossContext: 0.00005449
    Epoch:   400     LossContext: 0.00005444
    Epoch:   410     LossContext: 0.00005439
    Epoch:   420     LossContext: 0.00005436
    Epoch:   430     LossContext: 0.00005432
    Epoch:   440     LossContext: 0.00005428
    Epoch:   450     LossContext: 0.00005425
    Epoch:   460     LossContext: 0.00005421
    Epoch:   470     LossContext: 0.00005417
    Epoch:   480     LossContext: 0.00005413
    Epoch:   490     LossContext: 0.00005409
    Epoch:   500     LossContext: 0.00005405
    Epoch:   510     LossContext: 0.00005401
    Epoch:   520     LossContext: 0.00005397
    Epoch:   530     LossContext: 0.00005393
    Epoch:   540     LossContext: 0.00005389
    Epoch:   550     LossContext: 0.00005385
    Epoch:   560     LossContext: 0.00005381
    Epoch:   570     LossContext: 0.00005376
    Epoch:   580     LossContext: 0.00005372
    Epoch:   590     LossContext: 0.00005368
    Epoch:   600     LossContext: 0.00005363
    Epoch:   610     LossContext: 0.00005359
    Epoch:   620     LossContext: 0.00005354
    Epoch:   630     LossContext: 0.00005349
    Epoch:   640     LossContext: 0.00005345
    Epoch:   650     LossContext: 0.00005340
    Epoch:   660     LossContext: 0.00005335
    Epoch:   670     LossContext: 0.00005331
    Epoch:   680     LossContext: 0.00005326
    Epoch:   690     LossContext: 0.00005321
    Epoch:   700     LossContext: 0.00005316
    Epoch:   710     LossContext: 0.00005311
    Epoch:   720     LossContext: 0.00005306
    Epoch:   730     LossContext: 0.00005301
    Epoch:   740     LossContext: 0.00005296
    Epoch:   750     LossContext: 0.00005290
    Epoch:   760     LossContext: 0.00005285
    Epoch:   770     LossContext: 0.00005280
    Epoch:   780     LossContext: 0.00005275
    Epoch:   790     LossContext: 0.00005269
    Epoch:   800     LossContext: 0.00005264
    Epoch:   810     LossContext: 0.00005258
    Epoch:   820     LossContext: 0.00005253
    Epoch:   830     LossContext: 0.00005247
    Epoch:   840     LossContext: 0.00005241
    Epoch:   850     LossContext: 0.00005236
    Epoch:   860     LossContext: 0.00005230
    Epoch:   870     LossContext: 0.00005224
    Epoch:   880     LossContext: 0.00005218
    Epoch:   890     LossContext: 0.00005212
    Epoch:   900     LossContext: 0.00005207
    Epoch:   910     LossContext: 0.00005201
    Epoch:   920     LossContext: 0.00005195
    Epoch:   930     LossContext: 0.00005188
    Epoch:   940     LossContext: 0.00005182
    Epoch:   950     LossContext: 0.00005176
    Epoch:   960     LossContext: 0.00005170
    Epoch:   970     LossContext: 0.00005164
    Epoch:   980     LossContext: 0.00005157
    Epoch:   990     LossContext: 0.00005151
    Epoch:  1000     LossContext: 0.00005144
    Epoch:  1010     LossContext: 0.00005139
    Epoch:  1020     LossContext: 0.00005134
    Epoch:  1030     LossContext: 0.00005130
    Epoch:  1040     LossContext: 0.00005125
    Epoch:  1050     LossContext: 0.00005121
    Epoch:  1060     LossContext: 0.00005116
    Epoch:  1070     LossContext: 0.00005111
    Epoch:  1080     LossContext: 0.00005107
    Epoch:  1090     LossContext: 0.00005102
    Epoch:  1100     LossContext: 0.00005097
    Epoch:  1110     LossContext: 0.00005093
    Epoch:  1120     LossContext: 0.00005088
    Epoch:  1130     LossContext: 0.00005083
    Epoch:  1140     LossContext: 0.00005078
    Epoch:  1150     LossContext: 0.00005073
    Epoch:  1160     LossContext: 0.00005068
    Epoch:  1170     LossContext: 0.00005063
    Epoch:  1180     LossContext: 0.00005058
    Epoch:  1190     LossContext: 0.00005053
    Epoch:  1200     LossContext: 0.00005048
    Epoch:  1210     LossContext: 0.00005043
    Epoch:  1220     LossContext: 0.00005038
    Epoch:  1230     LossContext: 0.00005033
    Epoch:  1240     LossContext: 0.00005027
    Epoch:  1250     LossContext: 0.00005022
    Epoch:  1260     LossContext: 0.00005017
    Epoch:  1270     LossContext: 0.00005012
    Epoch:  1280     LossContext: 0.00005006
    Epoch:  1290     LossContext: 0.00005001
    Epoch:  1300     LossContext: 0.00004995
    Epoch:  1310     LossContext: 0.00004990
    Epoch:  1320     LossContext: 0.00004984
    Epoch:  1330     LossContext: 0.00004979
    Epoch:  1340     LossContext: 0.00004973
    Epoch:  1350     LossContext: 0.00004968
    Epoch:  1360     LossContext: 0.00004962
    Epoch:  1370     LossContext: 0.00004956
    Epoch:  1380     LossContext: 0.00004950
    Epoch:  1390     LossContext: 0.00004945
    Epoch:  1400     LossContext: 0.00004939
    Epoch:  1410     LossContext: 0.00004933
    Epoch:  1420     LossContext: 0.00004927
    Epoch:  1430     LossContext: 0.00004922
    Epoch:  1440     LossContext: 0.00004915
    Epoch:  1450     LossContext: 0.00004909
    Epoch:  1460     LossContext: 0.00004903
    Epoch:  1470     LossContext: 0.00004897
    Epoch:  1480     LossContext: 0.00004891
    Epoch:  1490     LossContext: 0.00004885
    Epoch:  1499     LossContext: 0.00004879

Gradient descent adaptation time: 0 hours 1 mins 40 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.13791381
    Epoch:     1     LossContext: 0.12480795
    Epoch:     2     LossContext: 0.11111437
    Epoch:     3     LossContext: 0.09694345
    Epoch:    10     LossContext: 0.01200533
    Epoch:    20     LossContext: 0.00409982
    Epoch:    30     LossContext: 0.00207103
    Epoch:    40     LossContext: 0.00091160
    Epoch:    50     LossContext: 0.00075833
    Epoch:    60     LossContext: 0.00023435
    Epoch:    70     LossContext: 0.00012388
    Epoch:    80     LossContext: 0.00014114
    Epoch:    90     LossContext: 0.00013355
    Epoch:   100     LossContext: 0.00012455
    Epoch:   110     LossContext: 0.00012322
    Epoch:   120     LossContext: 0.00012320
    Epoch:   130     LossContext: 0.00012296
    Epoch:   140     LossContext: 0.00012289
    Epoch:   150     LossContext: 0.00012285
    Epoch:   160     LossContext: 0.00012280
    Epoch:   170     LossContext: 0.00012277
    Epoch:   180     LossContext: 0.00012275
    Epoch:   190     LossContext: 0.00012273
    Epoch:   200     LossContext: 0.00012271
    Epoch:   210     LossContext: 0.00012268
    Epoch:   220     LossContext: 0.00012266
    Epoch:   230     LossContext: 0.00012264
    Epoch:   240     LossContext: 0.00012262
    Epoch:   250     LossContext: 0.00012259
    Epoch:   260     LossContext: 0.00012257
    Epoch:   270     LossContext: 0.00012255
    Epoch:   280     LossContext: 0.00012252
    Epoch:   290     LossContext: 0.00012249
    Epoch:   300     LossContext: 0.00012247
    Epoch:   310     LossContext: 0.00012244
    Epoch:   320     LossContext: 0.00012242
    Epoch:   330     LossContext: 0.00012239
    Epoch:   340     LossContext: 0.00012237
    Epoch:   350     LossContext: 0.00012236
    Epoch:   360     LossContext: 0.00012234
    Epoch:   370     LossContext: 0.00012233
    Epoch:   380     LossContext: 0.00012231
    Epoch:   390     LossContext: 0.00012230
    Epoch:   400     LossContext: 0.00012228
    Epoch:   410     LossContext: 0.00012227
    Epoch:   420     LossContext: 0.00012225
    Epoch:   430     LossContext: 0.00012223
    Epoch:   440     LossContext: 0.00012222
    Epoch:   450     LossContext: 0.00012220
    Epoch:   460     LossContext: 0.00012218
    Epoch:   470     LossContext: 0.00012217
    Epoch:   480     LossContext: 0.00012215
    Epoch:   490     LossContext: 0.00012213
    Epoch:   500     LossContext: 0.00012211
    Epoch:   510     LossContext: 0.00012210
    Epoch:   520     LossContext: 0.00012208
    Epoch:   530     LossContext: 0.00012206
    Epoch:   540     LossContext: 0.00012204
    Epoch:   550     LossContext: 0.00012202
    Epoch:   560     LossContext: 0.00012200
    Epoch:   570     LossContext: 0.00012199
    Epoch:   580     LossContext: 0.00012197
    Epoch:   590     LossContext: 0.00012195
    Epoch:   600     LossContext: 0.00012193
    Epoch:   610     LossContext: 0.00012191
    Epoch:   620     LossContext: 0.00012189
    Epoch:   630     LossContext: 0.00012186
    Epoch:   640     LossContext: 0.00012184
    Epoch:   650     LossContext: 0.00012182
    Epoch:   660     LossContext: 0.00012180
    Epoch:   670     LossContext: 0.00012178
    Epoch:   680     LossContext: 0.00012176
    Epoch:   690     LossContext: 0.00012174
    Epoch:   700     LossContext: 0.00012171
    Epoch:   710     LossContext: 0.00012169
    Epoch:   720     LossContext: 0.00012167
    Epoch:   730     LossContext: 0.00012165
    Epoch:   740     LossContext: 0.00012162
    Epoch:   750     LossContext: 0.00012160
    Epoch:   760     LossContext: 0.00012158
    Epoch:   770     LossContext: 0.00012155
    Epoch:   780     LossContext: 0.00012153
    Epoch:   790     LossContext: 0.00012150
    Epoch:   800     LossContext: 0.00012148
    Epoch:   810     LossContext: 0.00012146
    Epoch:   820     LossContext: 0.00012143
    Epoch:   830     LossContext: 0.00012141
    Epoch:   840     LossContext: 0.00012138
    Epoch:   850     LossContext: 0.00012135
    Epoch:   860     LossContext: 0.00012133
    Epoch:   870     LossContext: 0.00012130
    Epoch:   880     LossContext: 0.00012128
    Epoch:   890     LossContext: 0.00012125
    Epoch:   900     LossContext: 0.00012123
    Epoch:   910     LossContext: 0.00012120
    Epoch:   920     LossContext: 0.00012117
    Epoch:   930     LossContext: 0.00012114
    Epoch:   940     LossContext: 0.00012112
    Epoch:   950     LossContext: 0.00012109
    Epoch:   960     LossContext: 0.00012106
    Epoch:   970     LossContext: 0.00012103
    Epoch:   980     LossContext: 0.00012100
    Epoch:   990     LossContext: 0.00012098
    Epoch:  1000     LossContext: 0.00012095
    Epoch:  1010     LossContext: 0.00012092
    Epoch:  1020     LossContext: 0.00012089
    Epoch:  1030     LossContext: 0.00012086
    Epoch:  1040     LossContext: 0.00012083
    Epoch:  1050     LossContext: 0.00012080
    Epoch:  1060     LossContext: 0.00012077
    Epoch:  1070     LossContext: 0.00012074
    Epoch:  1080     LossContext: 0.00012071
    Epoch:  1090     LossContext: 0.00012068
    Epoch:  1100     LossContext: 0.00012065
    Epoch:  1110     LossContext: 0.00012061
    Epoch:  1120     LossContext: 0.00012058
    Epoch:  1130     LossContext: 0.00012055
    Epoch:  1140     LossContext: 0.00012052
    Epoch:  1150     LossContext: 0.00012049
    Epoch:  1160     LossContext: 0.00012045
    Epoch:  1170     LossContext: 0.00012042
    Epoch:  1180     LossContext: 0.00012039
    Epoch:  1190     LossContext: 0.00012036
    Epoch:  1200     LossContext: 0.00012032
    Epoch:  1210     LossContext: 0.00012029
    Epoch:  1220     LossContext: 0.00012025
    Epoch:  1230     LossContext: 0.00012022
    Epoch:  1240     LossContext: 0.00012019
    Epoch:  1250     LossContext: 0.00012015
    Epoch:  1260     LossContext: 0.00012012
    Epoch:  1270     LossContext: 0.00012008
    Epoch:  1280     LossContext: 0.00012005
    Epoch:  1290     LossContext: 0.00012001
    Epoch:  1300     LossContext: 0.00011997
    Epoch:  1310     LossContext: 0.00011994
    Epoch:  1320     LossContext: 0.00011990
    Epoch:  1330     LossContext: 0.00011986
    Epoch:  1340     LossContext: 0.00011983
    Epoch:  1350     LossContext: 0.00011979
    Epoch:  1360     LossContext: 0.00011975
    Epoch:  1370     LossContext: 0.00011972
    Epoch:  1380     LossContext: 0.00011968
    Epoch:  1390     LossContext: 0.00011964
    Epoch:  1400     LossContext: 0.00011960
    Epoch:  1410     LossContext: 0.00011956
    Epoch:  1420     LossContext: 0.00011952
    Epoch:  1430     LossContext: 0.00011949
    Epoch:  1440     LossContext: 0.00011945
    Epoch:  1450     LossContext: 0.00011941
    Epoch:  1460     LossContext: 0.00011937
    Epoch:  1470     LossContext: 0.00011933
    Epoch:  1480     LossContext: 0.00011929
    Epoch:  1490     LossContext: 0.00011924
    Epoch:  1499     LossContext: 0.00011921

Gradient descent adaptation time: 0 hours 1 mins 27 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.04260739
    Epoch:     1     LossContext: 0.03643689
    Epoch:     2     LossContext: 0.03090879
    Epoch:     3     LossContext: 0.02602133
    Epoch:    10     LossContext: 0.00563873
    Epoch:    20     LossContext: 0.00027662
    Epoch:    30     LossContext: 0.00034754
    Epoch:    40     LossContext: 0.00047929
    Epoch:    50     LossContext: 0.00033377
    Epoch:    60     LossContext: 0.00019092
    Epoch:    70     LossContext: 0.00014785
    Epoch:    80     LossContext: 0.00014904
    Epoch:    90     LossContext: 0.00014935
    Epoch:   100     LossContext: 0.00014698
    Epoch:   110     LossContext: 0.00014623
    Epoch:   120     LossContext: 0.00014623
    Epoch:   130     LossContext: 0.00014610
    Epoch:   140     LossContext: 0.00014596
    Epoch:   150     LossContext: 0.00014587
    Epoch:   160     LossContext: 0.00014577
    Epoch:   170     LossContext: 0.00014568
    Epoch:   180     LossContext: 0.00014563
    Epoch:   190     LossContext: 0.00014557
    Epoch:   200     LossContext: 0.00014551
    Epoch:   210     LossContext: 0.00014545
    Epoch:   220     LossContext: 0.00014539
    Epoch:   230     LossContext: 0.00014533
    Epoch:   240     LossContext: 0.00014527
    Epoch:   250     LossContext: 0.00014520
    Epoch:   260     LossContext: 0.00014514
    Epoch:   270     LossContext: 0.00014507
    Epoch:   280     LossContext: 0.00014500
    Epoch:   290     LossContext: 0.00014493
    Epoch:   300     LossContext: 0.00014486
    Epoch:   310     LossContext: 0.00014478
    Epoch:   320     LossContext: 0.00014471
    Epoch:   330     LossContext: 0.00014463
    Epoch:   340     LossContext: 0.00014458
    Epoch:   350     LossContext: 0.00014454
    Epoch:   360     LossContext: 0.00014450
    Epoch:   370     LossContext: 0.00014446
    Epoch:   380     LossContext: 0.00014442
    Epoch:   390     LossContext: 0.00014438
    Epoch:   400     LossContext: 0.00014434
    Epoch:   410     LossContext: 0.00014429
    Epoch:   420     LossContext: 0.00014425
    Epoch:   430     LossContext: 0.00014420
    Epoch:   440     LossContext: 0.00014416
    Epoch:   450     LossContext: 0.00014411
    Epoch:   460     LossContext: 0.00014407
    Epoch:   470     LossContext: 0.00014402
    Epoch:   480     LossContext: 0.00014397
    Epoch:   490     LossContext: 0.00014392
    Epoch:   500     LossContext: 0.00014387
    Epoch:   510     LossContext: 0.00014382
    Epoch:   520     LossContext: 0.00014377
    Epoch:   530     LossContext: 0.00014372
    Epoch:   540     LossContext: 0.00014367
    Epoch:   550     LossContext: 0.00014362
    Epoch:   560     LossContext: 0.00014356
    Epoch:   570     LossContext: 0.00014351
    Epoch:   580     LossContext: 0.00014346
    Epoch:   590     LossContext: 0.00014340
    Epoch:   600     LossContext: 0.00014335
    Epoch:   610     LossContext: 0.00014329
    Epoch:   620     LossContext: 0.00014323
    Epoch:   630     LossContext: 0.00014318
    Epoch:   640     LossContext: 0.00014312
    Epoch:   650     LossContext: 0.00014306
    Epoch:   660     LossContext: 0.00014300
    Epoch:   670     LossContext: 0.00014294
    Epoch:   680     LossContext: 0.00014288
    Epoch:   690     LossContext: 0.00014282
    Epoch:   700     LossContext: 0.00014276
    Epoch:   710     LossContext: 0.00014270
    Epoch:   720     LossContext: 0.00014263
    Epoch:   730     LossContext: 0.00014257
    Epoch:   740     LossContext: 0.00014251
    Epoch:   750     LossContext: 0.00014244
    Epoch:   760     LossContext: 0.00014238
    Epoch:   770     LossContext: 0.00014231
    Epoch:   780     LossContext: 0.00014225
    Epoch:   790     LossContext: 0.00014218
    Epoch:   800     LossContext: 0.00014211
    Epoch:   810     LossContext: 0.00014204
    Epoch:   820     LossContext: 0.00014197
    Epoch:   830     LossContext: 0.00014190
    Epoch:   840     LossContext: 0.00014183
    Epoch:   850     LossContext: 0.00014176
    Epoch:   860     LossContext: 0.00014169
    Epoch:   870     LossContext: 0.00014162
    Epoch:   880     LossContext: 0.00014155
    Epoch:   890     LossContext: 0.00014147
    Epoch:   900     LossContext: 0.00014141
    Epoch:   910     LossContext: 0.00014134
    Epoch:   920     LossContext: 0.00014127
    Epoch:   930     LossContext: 0.00014120
    Epoch:   940     LossContext: 0.00014113
    Epoch:   950     LossContext: 0.00014106
    Epoch:   960     LossContext: 0.00014099
    Epoch:   970     LossContext: 0.00014091
    Epoch:   980     LossContext: 0.00014084
    Epoch:   990     LossContext: 0.00014077
    Epoch:  1000     LossContext: 0.00014069
    Epoch:  1010     LossContext: 0.00014062
    Epoch:  1020     LossContext: 0.00014054
    Epoch:  1030     LossContext: 0.00014047
    Epoch:  1040     LossContext: 0.00014039
    Epoch:  1050     LossContext: 0.00014032
    Epoch:  1060     LossContext: 0.00014024
    Epoch:  1070     LossContext: 0.00014016
    Epoch:  1080     LossContext: 0.00014008
    Epoch:  1090     LossContext: 0.00014000
    Epoch:  1100     LossContext: 0.00013992
    Epoch:  1110     LossContext: 0.00013984
    Epoch:  1120     LossContext: 0.00013976
    Epoch:  1130     LossContext: 0.00013968
    Epoch:  1140     LossContext: 0.00013960
    Epoch:  1150     LossContext: 0.00013952
    Epoch:  1160     LossContext: 0.00013943
    Epoch:  1170     LossContext: 0.00013935
    Epoch:  1180     LossContext: 0.00013926
    Epoch:  1190     LossContext: 0.00013918
    Epoch:  1200     LossContext: 0.00013909
    Epoch:  1210     LossContext: 0.00013901
    Epoch:  1220     LossContext: 0.00013892
    Epoch:  1230     LossContext: 0.00013883
    Epoch:  1240     LossContext: 0.00013875
    Epoch:  1250     LossContext: 0.00013866
    Epoch:  1260     LossContext: 0.00013857
    Epoch:  1270     LossContext: 0.00013848
    Epoch:  1280     LossContext: 0.00013839
    Epoch:  1290     LossContext: 0.00013830
    Epoch:  1300     LossContext: 0.00013820
    Epoch:  1310     LossContext: 0.00013821
    Epoch:  1320     LossContext: 0.00013819
    Epoch:  1330     LossContext: 0.00013817
    Epoch:  1340     LossContext: 0.00013816
    Epoch:  1350     LossContext: 0.00013815
    Epoch:  1360     LossContext: 0.00013814
    Epoch:  1370     LossContext: 0.00013812
    Epoch:  1380     LossContext: 0.00013811
    Epoch:  1390     LossContext: 0.00013810
    Epoch:  1400     LossContext: 0.00013809
    Epoch:  1410     LossContext: 0.00013808
    Epoch:  1420     LossContext: 0.00013807
    Epoch:  1430     LossContext: 0.00013806
    Epoch:  1440     LossContext: 0.00013804
    Epoch:  1450     LossContext: 0.00013803
    Epoch:  1460     LossContext: 0.00013802
    Epoch:  1470     LossContext: 0.00013801
    Epoch:  1480     LossContext: 0.00013799
    Epoch:  1490     LossContext: 0.00013798
    Epoch:  1499     LossContext: 0.00013797

Gradient descent adaptation time: 0 hours 1 mins 26 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.03183921
    Epoch:     1     LossContext: 0.02658355
    Epoch:     2     LossContext: 0.02198540
    Epoch:     3     LossContext: 0.01799882
    Epoch:    10     LossContext: 0.00342897
    Epoch:    20     LossContext: 0.00026787
    Epoch:    30     LossContext: 0.00022670
    Epoch:    40     LossContext: 0.00030799
    Epoch:    50     LossContext: 0.00027569
    Epoch:    60     LossContext: 0.00021165
    Epoch:    70     LossContext: 0.00017359
    Epoch:    80     LossContext: 0.00016282
    Epoch:    90     LossContext: 0.00016231
    Epoch:   100     LossContext: 0.00016235
    Epoch:   110     LossContext: 0.00016175
    Epoch:   120     LossContext: 0.00016123
    Epoch:   130     LossContext: 0.00016089
    Epoch:   140     LossContext: 0.00016059
    Epoch:   150     LossContext: 0.00016026
    Epoch:   160     LossContext: 0.00015991
    Epoch:   170     LossContext: 0.00015963
    Epoch:   180     LossContext: 0.00015945
    Epoch:   190     LossContext: 0.00015927
    Epoch:   200     LossContext: 0.00015908
    Epoch:   210     LossContext: 0.00015888
    Epoch:   220     LossContext: 0.00015868
    Epoch:   230     LossContext: 0.00015847
    Epoch:   240     LossContext: 0.00015826
    Epoch:   250     LossContext: 0.00015804
    Epoch:   260     LossContext: 0.00015782
    Epoch:   270     LossContext: 0.00015759
    Epoch:   280     LossContext: 0.00015736
    Epoch:   290     LossContext: 0.00015713
    Epoch:   300     LossContext: 0.00015689
    Epoch:   310     LossContext: 0.00015664
    Epoch:   320     LossContext: 0.00015639
    Epoch:   330     LossContext: 0.00015614
    Epoch:   340     LossContext: 0.00015597
    Epoch:   350     LossContext: 0.00015584
    Epoch:   360     LossContext: 0.00015570
    Epoch:   370     LossContext: 0.00015557
    Epoch:   380     LossContext: 0.00015543
    Epoch:   390     LossContext: 0.00015529
    Epoch:   400     LossContext: 0.00015515
    Epoch:   410     LossContext: 0.00015500
    Epoch:   420     LossContext: 0.00015486
    Epoch:   430     LossContext: 0.00015474
    Epoch:   440     LossContext: 0.00015465
    Epoch:   450     LossContext: 0.00015455
    Epoch:   460     LossContext: 0.00015446
    Epoch:   470     LossContext: 0.00015437
    Epoch:   480     LossContext: 0.00015427
    Epoch:   490     LossContext: 0.00015418
    Epoch:   500     LossContext: 0.00015408
    Epoch:   510     LossContext: 0.00015399
    Epoch:   520     LossContext: 0.00015389
    Epoch:   530     LossContext: 0.00015379
    Epoch:   540     LossContext: 0.00015369
    Epoch:   550     LossContext: 0.00015359
    Epoch:   560     LossContext: 0.00015349
    Epoch:   570     LossContext: 0.00015338
    Epoch:   580     LossContext: 0.00015328
    Epoch:   590     LossContext: 0.00015317
    Epoch:   600     LossContext: 0.00015306
    Epoch:   610     LossContext: 0.00015295
    Epoch:   620     LossContext: 0.00015286
    Epoch:   630     LossContext: 0.00015280
    Epoch:   640     LossContext: 0.00015274
    Epoch:   650     LossContext: 0.00015268
    Epoch:   660     LossContext: 0.00015262
    Epoch:   670     LossContext: 0.00015256
    Epoch:   680     LossContext: 0.00015250
    Epoch:   690     LossContext: 0.00015244
    Epoch:   700     LossContext: 0.00015238
    Epoch:   710     LossContext: 0.00015232
    Epoch:   720     LossContext: 0.00015225
    Epoch:   730     LossContext: 0.00015219
    Epoch:   740     LossContext: 0.00015213
    Epoch:   750     LossContext: 0.00015206
    Epoch:   760     LossContext: 0.00015200
    Epoch:   770     LossContext: 0.00015194
    Epoch:   780     LossContext: 0.00015187
    Epoch:   790     LossContext: 0.00015180
    Epoch:   800     LossContext: 0.00015174
    Epoch:   810     LossContext: 0.00015167
    Epoch:   820     LossContext: 0.00015160
    Epoch:   830     LossContext: 0.00015153
    Epoch:   840     LossContext: 0.00015146
    Epoch:   850     LossContext: 0.00015140
    Epoch:   860     LossContext: 0.00015132
    Epoch:   870     LossContext: 0.00015125
    Epoch:   880     LossContext: 0.00015118
    Epoch:   890     LossContext: 0.00015111
    Epoch:   900     LossContext: 0.00015104
    Epoch:   910     LossContext: 0.00015096
    Epoch:   920     LossContext: 0.00015089
    Epoch:   930     LossContext: 0.00015082
    Epoch:   940     LossContext: 0.00015074
    Epoch:   950     LossContext: 0.00015067
    Epoch:   960     LossContext: 0.00015059
    Epoch:   970     LossContext: 0.00015051
    Epoch:   980     LossContext: 0.00015043
    Epoch:   990     LossContext: 0.00015036
    Epoch:  1000     LossContext: 0.00015028
    Epoch:  1010     LossContext: 0.00015020
    Epoch:  1020     LossContext: 0.00015012
    Epoch:  1030     LossContext: 0.00015004
    Epoch:  1040     LossContext: 0.00014996
    Epoch:  1050     LossContext: 0.00014987
    Epoch:  1060     LossContext: 0.00014979
    Epoch:  1070     LossContext: 0.00014971
    Epoch:  1080     LossContext: 0.00014963
    Epoch:  1090     LossContext: 0.00014954
    Epoch:  1100     LossContext: 0.00014946
    Epoch:  1110     LossContext: 0.00014937
    Epoch:  1120     LossContext: 0.00014929
    Epoch:  1130     LossContext: 0.00014920
    Epoch:  1140     LossContext: 0.00014911
    Epoch:  1150     LossContext: 0.00014902
    Epoch:  1160     LossContext: 0.00014894
    Epoch:  1170     LossContext: 0.00014885
    Epoch:  1180     LossContext: 0.00014876
    Epoch:  1190     LossContext: 0.00014867
    Epoch:  1200     LossContext: 0.00014857
    Epoch:  1210     LossContext: 0.00014848
    Epoch:  1220     LossContext: 0.00014839
    Epoch:  1230     LossContext: 0.00014830
    Epoch:  1240     LossContext: 0.00014821
    Epoch:  1250     LossContext: 0.00014811
    Epoch:  1260     LossContext: 0.00014801
    Epoch:  1270     LossContext: 0.00014792
    Epoch:  1280     LossContext: 0.00014783
    Epoch:  1290     LossContext: 0.00014773
    Epoch:  1300     LossContext: 0.00014763
    Epoch:  1310     LossContext: 0.00014754
    Epoch:  1320     LossContext: 0.00014743
    Epoch:  1330     LossContext: 0.00014734
    Epoch:  1340     LossContext: 0.00014724
    Epoch:  1350     LossContext: 0.00014714
    Epoch:  1360     LossContext: 0.00014703
    Epoch:  1370     LossContext: 0.00014693
    Epoch:  1380     LossContext: 0.00014683
    Epoch:  1390     LossContext: 0.00014673
    Epoch:  1400     LossContext: 0.00014663
    Epoch:  1410     LossContext: 0.00014652
    Epoch:  1420     LossContext: 0.00014641
    Epoch:  1430     LossContext: 0.00014632
    Epoch:  1440     LossContext: 0.00014626
    Epoch:  1450     LossContext: 0.00014619
    Epoch:  1460     LossContext: 0.00014613
    Epoch:  1470     LossContext: 0.00014607
    Epoch:  1480     LossContext: 0.00014601
    Epoch:  1490     LossContext: 0.00014594
    Epoch:  1499     LossContext: 0.00014588

Gradient descent adaptation time: 0 hours 1 mins 27 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./05052024-145443/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 9.639266e-05

==  Begining out-of-distribution visualisation ... ==
    Environment id: 0
    Trajectory id: 0
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./05052024-145443/adapt/results_ood.png
Testing finished. Figure saved in: ./05052024-145443/adapt/results_ood_uq.pdf

======================= DONE ======================== 

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

========= STARTING =========
Working with a context size of 2^5 = 32
Run folder created successfuly: ./05052024-164731/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 164748
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 164748
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 54288 


WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed


=== Beginning training with proximal alternating minimization ... ===
    Number of examples in a batch: 4
    Maximum number of steps per inner minimization: 25
    Maximum number of outer minimizations: 500
    Maximum total number of training steps: 12500

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Outer Step:     0      LossTrajs: 1.81046832     ContextsNorm: 0.00000000     ValIndCrit: 1.69887340
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.72e-06
        -DiffCxt:  inf
    Outer Step:     1      LossTrajs: 1.52453196     ContextsNorm: 0.00027433     ValIndCrit: 1.42922628
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.62e-06
        -DiffCxt:  1.58e-03
    Outer Step:     2      LossTrajs: 1.21364141     ContextsNorm: 0.00066665     ValIndCrit: 1.13557053
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.35e-06
        -DiffCxt:  5.51e-04
    Outer Step:     3      LossTrajs: 0.78977293     ContextsNorm: 0.00165218     ValIndCrit: 0.74010491
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.44e-06
        -DiffCxt:  2.94e-04
    Outer Step:    10      LossTrajs: 0.24502191     ContextsNorm: 0.00999030     ValIndCrit: 0.28963318
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.78e-07
        -DiffCxt:  1.89e-05
    Outer Step:    20      LossTrajs: 0.16794720     ContextsNorm: 0.02960493     ValIndCrit: 0.19044225
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.21e-07
        -DiffCxt:  1.44e-06
    Outer Step:    30      LossTrajs: 0.07880946     ContextsNorm: 0.02852230     ValIndCrit: 0.08111713
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.60e-07
        -DiffCxt:  5.53e-07
    Outer Step:    40      LossTrajs: 0.05297471     ContextsNorm: 0.02905935     ValIndCrit: 0.05502050
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.34e-08
        -DiffCxt:  5.92e-08
    Outer Step:    50      LossTrajs: 0.04123385     ContextsNorm: 0.03405172     ValIndCrit: 0.04179687
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.16e-07
        -DiffCxt:  1.94e-06
    Outer Step:    60      LossTrajs: 0.02167614     ContextsNorm: 0.03820983     ValIndCrit: 0.02423579
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.88e-08
        -DiffCxt:  4.54e-07
    Outer Step:    70      LossTrajs: 0.01832827     ContextsNorm: 0.03948390     ValIndCrit: 0.02167621
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.41e-08
        -DiffCxt:  8.91e-08
    Outer Step:    80      LossTrajs: 0.01585742     ContextsNorm: 0.04044952     ValIndCrit: 0.01930507
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.48e-08
        -DiffCxt:  4.27e-08
    Outer Step:    90      LossTrajs: 0.01378459     ContextsNorm: 0.04154604     ValIndCrit: 0.01663779
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.16e-08
        -DiffCxt:  1.38e-07
    Outer Step:   100      LossTrajs: 0.00978532     ContextsNorm: 0.04260956     ValIndCrit: 0.01247925
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.67e-08
        -DiffCxt:  3.87e-07
    Outer Step:   110      LossTrajs: 0.00627186     ContextsNorm: 0.04269838     ValIndCrit: 0.00902299
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.18e-08
        -DiffCxt:  1.12e-07
    Outer Step:   120      LossTrajs: 0.00389451     ContextsNorm: 0.04212310     ValIndCrit: 0.00582197
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.20e-08
        -DiffCxt:  9.60e-08
    Outer Step:   130      LossTrajs: 0.00159148     ContextsNorm: 0.04205972     ValIndCrit: 0.00239545
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.29e-08
        -DiffCxt:  6.55e-08
    Outer Step:   140      LossTrajs: 0.00096336     ContextsNorm: 0.04194462     ValIndCrit: 0.00162509
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.12e-08
        -DiffCxt:  1.60e-08
    Outer Step:   150      LossTrajs: 0.00071796     ContextsNorm: 0.04172416     ValIndCrit: 0.00128589
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.63e-09
        -DiffCxt:  1.44e-08
    Outer Step:   160      LossTrajs: 0.00053990     ContextsNorm: 0.04186139     ValIndCrit: 0.00102023
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   18
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.61e-09
        -DiffCxt:  9.26e-09
    Outer Step:   170      LossTrajs: 0.00044568     ContextsNorm: 0.04195112     ValIndCrit: 0.00080668
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.81e-09
        -DiffCxt:  4.45e-09
    Outer Step:   180      LossTrajs: 0.00035544     ContextsNorm: 0.04205174     ValIndCrit: 0.00063608
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   24
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.28e-09
        -DiffCxt:  9.63e-09
    Outer Step:   190      LossTrajs: 0.00028816     ContextsNorm: 0.04202118     ValIndCrit: 0.00052689
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   24
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.37e-09
        -DiffCxt:  9.12e-09
    Outer Step:   200      LossTrajs: 0.00024839     ContextsNorm: 0.04197631     ValIndCrit: 0.00041786
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   23
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.33e-09
        -DiffCxt:  9.96e-09
    Outer Step:   210      LossTrajs: 0.00021610     ContextsNorm: 0.04200224     ValIndCrit: 0.00034922
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   24
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.36e-09
        -DiffCxt:  9.46e-09
    Outer Step:   220      LossTrajs: 0.00019628     ContextsNorm: 0.04192297     ValIndCrit: 0.00029780
        Saving best model so far ...
        -NbInnerStepsNode:   14
        -NbInnerStepsCxt:   21
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.39e-10
        -DiffCxt:  9.82e-09
    Outer Step:   230      LossTrajs: 0.00017206     ContextsNorm: 0.04189231     ValIndCrit: 0.00026186
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   22
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.98e-09
        -DiffCxt:  5.74e-09
    Outer Step:   240      LossTrajs: 0.00015050     ContextsNorm: 0.04191457     ValIndCrit: 0.00022441
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   22
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.67e-09
        -DiffCxt:  9.94e-09
    Outer Step:   250      LossTrajs: 0.00014525     ContextsNorm: 0.04190974     ValIndCrit: 0.00021131
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   21
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.44e-09
        -DiffCxt:  8.05e-09
    Outer Step:   260      LossTrajs: 0.00013860     ContextsNorm: 0.04187923     ValIndCrit: 0.00018450
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   21
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.68e-09
        -DiffCxt:  9.86e-09
    Outer Step:   270      LossTrajs: 0.00013127     ContextsNorm: 0.04192229     ValIndCrit: 0.00016715
        Saving best model so far ...
        -NbInnerStepsNode:    2
        -NbInnerStepsCxt:   18
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.78e-10
        -DiffCxt:  7.92e-09
    Outer Step:   280      LossTrajs: 0.00012303     ContextsNorm: 0.04182443     ValIndCrit: 0.00015779
        Saving best model so far ...
        -NbInnerStepsNode:    3
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.32e-10
        -DiffCxt:  8.22e-09
    Outer Step:   290      LossTrajs: 0.00011191     ContextsNorm: 0.04192152     ValIndCrit: 0.00014360
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.36e-09
        -DiffCxt:  1.49e-08
    Outer Step:   300      LossTrajs: 0.00011083     ContextsNorm: 0.04192572     ValIndCrit: 0.00014102
        Saving best model so far ...
        -NbInnerStepsNode:    5
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.69e-10
        -DiffCxt:  3.55e-08
    Outer Step:   310      LossTrajs: 0.00010570     ContextsNorm: 0.04191715     ValIndCrit: 0.00013065
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.69e-09
        -DiffCxt:  1.16e-08
    Outer Step:   320      LossTrajs: 0.00010654     ContextsNorm: 0.04183822     ValIndCrit: 0.00013076
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.50e-09
        -DiffCxt:  2.21e-08
    Outer Step:   330      LossTrajs: 0.00009942     ContextsNorm: 0.04184732     ValIndCrit: 0.00012317
        Saving best model so far ...
        -NbInnerStepsNode:    2
        -NbInnerStepsCxt:   23
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.81e-10
        -DiffCxt:  9.13e-09
    Outer Step:   340      LossTrajs: 0.00009961     ContextsNorm: 0.04169681     ValIndCrit: 0.00012338
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.96e-09
        -DiffCxt:  3.18e-08
    Outer Step:   350      LossTrajs: 0.00009531     ContextsNorm: 0.04163994     ValIndCrit: 0.00011971
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.04e-09
        -DiffCxt:  3.19e-08
    Outer Step:   360      LossTrajs: 0.00009906     ContextsNorm: 0.04161000     ValIndCrit: 0.00011444
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.35e-09
        -DiffCxt:  1.25e-08
    Outer Step:   370      LossTrajs: 0.00009025     ContextsNorm: 0.04155738     ValIndCrit: 0.00011390
        Saving best model so far ...
        -NbInnerStepsNode:   11
        -NbInnerStepsCxt:   22
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.62e-10
        -DiffCxt:  8.66e-09
    Outer Step:   380      LossTrajs: 0.00008941     ContextsNorm: 0.04142049     ValIndCrit: 0.00011182
        Saving best model so far ...
        -NbInnerStepsNode:   17
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.03e-10
        -DiffCxt:  1.95e-08
    Outer Step:   390      LossTrajs: 0.00008782     ContextsNorm: 0.04123970     ValIndCrit: 0.00011193
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.96e-09
        -DiffCxt:  4.58e-08
    Outer Step:   400      LossTrajs: 0.00008311     ContextsNorm: 0.04110188     ValIndCrit: 0.00010874
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    7
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.25e-10
        -DiffCxt:  8.80e-09
    Outer Step:   410      LossTrajs: 0.00008311     ContextsNorm: 0.04100147     ValIndCrit: 0.00010868
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.86e-09
        -DiffCxt:  2.36e-08
    Outer Step:   420      LossTrajs: 0.00008647     ContextsNorm: 0.04090364     ValIndCrit: 0.00010517
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.31e-09
        -DiffCxt:  1.25e-07
    Outer Step:   430      LossTrajs: 0.00008099     ContextsNorm: 0.04087304     ValIndCrit: 0.00010740
        -NbInnerStepsNode:    3
        -NbInnerStepsCxt:   16
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.70e-10
        -DiffCxt:  9.91e-09
    Outer Step:   440      LossTrajs: 0.00008149     ContextsNorm: 0.04085215     ValIndCrit: 0.00010811
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.97e-09
        -DiffCxt:  5.03e-08
    Outer Step:   450      LossTrajs: 0.00007917     ContextsNorm: 0.04077458     ValIndCrit: 0.00010411
        Saving best model so far ...
        -NbInnerStepsNode:    3
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.27e-10
        -DiffCxt:  2.61e-08
    Outer Step:   460      LossTrajs: 0.00007811     ContextsNorm: 0.04079035     ValIndCrit: 0.00010169
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.49e-08
        -DiffCxt:  1.10e-07
    Outer Step:   470      LossTrajs: 0.00007804     ContextsNorm: 0.04079082     ValIndCrit: 0.00010189
        -NbInnerStepsNode:   12
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.42e-10
        -DiffCxt:  1.22e-08
    Outer Step:   480      LossTrajs: 0.00007651     ContextsNorm: 0.04075457     ValIndCrit: 0.00010187
        -NbInnerStepsNode:   13
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.16e-10
        -DiffCxt:  1.12e-08
    Outer Step:   490      LossTrajs: 0.00007414     ContextsNorm: 0.04067931     ValIndCrit: 0.00010050
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   22
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.84e-10
        -DiffCxt:  8.52e-09
    Outer Step:   499      LossTrajs: 0.00007368     ContextsNorm: 0.04061977     ValIndCrit: 0.00009913
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   22
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.54e-10
        -DiffCxt:  8.05e-09

Total gradient descent training time: 1 hours 37 mins 5 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 182454
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 9.9128556e-05


############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]
==  Begining in-domain visualisation ... ==
    Environment id: 0
    Trajectory id: 18
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./05052024-164731/results_in_domain.png
Testing finished. Figure saved in: ./05052024-164731/results_in_domain_uq.pdf
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.03021655
    Epoch:     1     LossContext: 0.02535845
    Epoch:     2     LossContext: 0.02037378
    Epoch:     3     LossContext: 0.01543434
    Epoch:    10     LossContext: 0.00209664
    Epoch:    20     LossContext: 0.00058704
    Epoch:    30     LossContext: 0.00075954
    Epoch:    40     LossContext: 0.00016980
    Epoch:    50     LossContext: 0.00006089
    Epoch:    60     LossContext: 0.00007791
    Epoch:    70     LossContext: 0.00007243
    Epoch:    80     LossContext: 0.00006330
    Epoch:    90     LossContext: 0.00006024
    Epoch:   100     LossContext: 0.00005912
    Epoch:   110     LossContext: 0.00005867
    Epoch:   120     LossContext: 0.00005846
    Epoch:   130     LossContext: 0.00005833
    Epoch:   140     LossContext: 0.00005821
    Epoch:   150     LossContext: 0.00005809
    Epoch:   160     LossContext: 0.00005797
    Epoch:   170     LossContext: 0.00005787
    Epoch:   180     LossContext: 0.00005781
    Epoch:   190     LossContext: 0.00005775
    Epoch:   200     LossContext: 0.00005768
    Epoch:   210     LossContext: 0.00005761
    Epoch:   220     LossContext: 0.00005755
    Epoch:   230     LossContext: 0.00005747
    Epoch:   240     LossContext: 0.00005740
    Epoch:   250     LossContext: 0.00005733
    Epoch:   260     LossContext: 0.00005726
    Epoch:   270     LossContext: 0.00005718
    Epoch:   280     LossContext: 0.00005710
    Epoch:   290     LossContext: 0.00005702
    Epoch:   300     LossContext: 0.00005694
    Epoch:   310     LossContext: 0.00005686
    Epoch:   320     LossContext: 0.00005678
    Epoch:   330     LossContext: 0.00005670
    Epoch:   340     LossContext: 0.00005665
    Epoch:   350     LossContext: 0.00005660
    Epoch:   360     LossContext: 0.00005656
    Epoch:   370     LossContext: 0.00005652
    Epoch:   380     LossContext: 0.00005647
    Epoch:   390     LossContext: 0.00005643
    Epoch:   400     LossContext: 0.00005638
    Epoch:   410     LossContext: 0.00005634
    Epoch:   420     LossContext: 0.00005629
    Epoch:   430     LossContext: 0.00005624
    Epoch:   440     LossContext: 0.00005619
    Epoch:   450     LossContext: 0.00005615
    Epoch:   460     LossContext: 0.00005610
    Epoch:   470     LossContext: 0.00005605
    Epoch:   480     LossContext: 0.00005600
    Epoch:   490     LossContext: 0.00005595
    Epoch:   500     LossContext: 0.00005590
    Epoch:   510     LossContext: 0.00005585
    Epoch:   520     LossContext: 0.00005580
    Epoch:   530     LossContext: 0.00005574
    Epoch:   540     LossContext: 0.00005569
    Epoch:   550     LossContext: 0.00005564
    Epoch:   560     LossContext: 0.00005558
    Epoch:   570     LossContext: 0.00005553
    Epoch:   580     LossContext: 0.00005547
    Epoch:   590     LossContext: 0.00005542
    Epoch:   600     LossContext: 0.00005536
    Epoch:   610     LossContext: 0.00005531
    Epoch:   620     LossContext: 0.00005525
    Epoch:   630     LossContext: 0.00005519
    Epoch:   640     LossContext: 0.00005514
    Epoch:   650     LossContext: 0.00005508
    Epoch:   660     LossContext: 0.00005502
    Epoch:   670     LossContext: 0.00005496
    Epoch:   680     LossContext: 0.00005490
    Epoch:   690     LossContext: 0.00005484
    Epoch:   700     LossContext: 0.00005478
    Epoch:   710     LossContext: 0.00005472
    Epoch:   720     LossContext: 0.00005466
    Epoch:   730     LossContext: 0.00005460
    Epoch:   740     LossContext: 0.00005454
    Epoch:   750     LossContext: 0.00005448
    Epoch:   760     LossContext: 0.00005442
    Epoch:   770     LossContext: 0.00005435
    Epoch:   780     LossContext: 0.00005429
    Epoch:   790     LossContext: 0.00005423
    Epoch:   800     LossContext: 0.00005416
    Epoch:   810     LossContext: 0.00005410
    Epoch:   820     LossContext: 0.00005403
    Epoch:   830     LossContext: 0.00005397
    Epoch:   840     LossContext: 0.00005390
    Epoch:   850     LossContext: 0.00005384
    Epoch:   860     LossContext: 0.00005377
    Epoch:   870     LossContext: 0.00005370
    Epoch:   880     LossContext: 0.00005364
    Epoch:   890     LossContext: 0.00005357
    Epoch:   900     LossContext: 0.00005350
    Epoch:   910     LossContext: 0.00005343
    Epoch:   920     LossContext: 0.00005336
    Epoch:   930     LossContext: 0.00005329
    Epoch:   940     LossContext: 0.00005322
    Epoch:   950     LossContext: 0.00005315
    Epoch:   960     LossContext: 0.00005308
    Epoch:   970     LossContext: 0.00005301
    Epoch:   980     LossContext: 0.00005294
    Epoch:   990     LossContext: 0.00005287
    Epoch:  1000     LossContext: 0.00005280
    Epoch:  1010     LossContext: 0.00005275
    Epoch:  1020     LossContext: 0.00005269
    Epoch:  1030     LossContext: 0.00005264
    Epoch:  1040     LossContext: 0.00005259
    Epoch:  1050     LossContext: 0.00005253
    Epoch:  1060     LossContext: 0.00005248
    Epoch:  1070     LossContext: 0.00005243
    Epoch:  1080     LossContext: 0.00005237
    Epoch:  1090     LossContext: 0.00005232
    Epoch:  1100     LossContext: 0.00005227
    Epoch:  1110     LossContext: 0.00005221
    Epoch:  1120     LossContext: 0.00005216
    Epoch:  1130     LossContext: 0.00005210
    Epoch:  1140     LossContext: 0.00005205
    Epoch:  1150     LossContext: 0.00005199
    Epoch:  1160     LossContext: 0.00005193
    Epoch:  1170     LossContext: 0.00005188
    Epoch:  1180     LossContext: 0.00005182
    Epoch:  1190     LossContext: 0.00005176
    Epoch:  1200     LossContext: 0.00005170
    Epoch:  1210     LossContext: 0.00005165
    Epoch:  1220     LossContext: 0.00005159
    Epoch:  1230     LossContext: 0.00005153
    Epoch:  1240     LossContext: 0.00005147
    Epoch:  1250     LossContext: 0.00005141
    Epoch:  1260     LossContext: 0.00005135
    Epoch:  1270     LossContext: 0.00005129
    Epoch:  1280     LossContext: 0.00005125
    Epoch:  1290     LossContext: 0.00005120
    Epoch:  1300     LossContext: 0.00005116
    Epoch:  1310     LossContext: 0.00005111
    Epoch:  1320     LossContext: 0.00005106
    Epoch:  1330     LossContext: 0.00005102
    Epoch:  1340     LossContext: 0.00005097
    Epoch:  1350     LossContext: 0.00005092
    Epoch:  1360     LossContext: 0.00005087
    Epoch:  1370     LossContext: 0.00005083
    Epoch:  1380     LossContext: 0.00005078
    Epoch:  1390     LossContext: 0.00005073
    Epoch:  1400     LossContext: 0.00005068
    Epoch:  1410     LossContext: 0.00005063
    Epoch:  1420     LossContext: 0.00005058
    Epoch:  1430     LossContext: 0.00005053
    Epoch:  1440     LossContext: 0.00005048
    Epoch:  1450     LossContext: 0.00005043
    Epoch:  1460     LossContext: 0.00005038
    Epoch:  1470     LossContext: 0.00005033
    Epoch:  1480     LossContext: 0.00005028
    Epoch:  1490     LossContext: 0.00005023
    Epoch:  1499     LossContext: 0.00005019

Gradient descent adaptation time: 0 hours 1 mins 40 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.11965090
    Epoch:     1     LossContext: 0.10445926
    Epoch:     2     LossContext: 0.08941098
    Epoch:     3     LossContext: 0.07470269
    Epoch:    10     LossContext: 0.00588571
    Epoch:    20     LossContext: 0.00507823
    Epoch:    30     LossContext: 0.00204615
    Epoch:    40     LossContext: 0.00161575
    Epoch:    50     LossContext: 0.00059532
    Epoch:    60     LossContext: 0.00019544
    Epoch:    70     LossContext: 0.00025438
    Epoch:    80     LossContext: 0.00021616
    Epoch:    90     LossContext: 0.00019447
    Epoch:   100     LossContext: 0.00019565
    Epoch:   110     LossContext: 0.00019372
    Epoch:   120     LossContext: 0.00019285
    Epoch:   130     LossContext: 0.00019237
    Epoch:   140     LossContext: 0.00019180
    Epoch:   150     LossContext: 0.00019138
    Epoch:   160     LossContext: 0.00019093
    Epoch:   170     LossContext: 0.00019057
    Epoch:   180     LossContext: 0.00019035
    Epoch:   190     LossContext: 0.00019011
    Epoch:   200     LossContext: 0.00018987
    Epoch:   210     LossContext: 0.00018963
    Epoch:   220     LossContext: 0.00018939
    Epoch:   230     LossContext: 0.00018914
    Epoch:   240     LossContext: 0.00018888
    Epoch:   250     LossContext: 0.00018863
    Epoch:   260     LossContext: 0.00018837
    Epoch:   270     LossContext: 0.00018810
    Epoch:   280     LossContext: 0.00018784
    Epoch:   290     LossContext: 0.00018757
    Epoch:   300     LossContext: 0.00018730
    Epoch:   310     LossContext: 0.00018702
    Epoch:   320     LossContext: 0.00018675
    Epoch:   330     LossContext: 0.00018647
    Epoch:   340     LossContext: 0.00018629
    Epoch:   350     LossContext: 0.00018615
    Epoch:   360     LossContext: 0.00018601
    Epoch:   370     LossContext: 0.00018586
    Epoch:   380     LossContext: 0.00018572
    Epoch:   390     LossContext: 0.00018557
    Epoch:   400     LossContext: 0.00018542
    Epoch:   410     LossContext: 0.00018527
    Epoch:   420     LossContext: 0.00018512
    Epoch:   430     LossContext: 0.00018497
    Epoch:   440     LossContext: 0.00018482
    Epoch:   450     LossContext: 0.00018466
    Epoch:   460     LossContext: 0.00018451
    Epoch:   470     LossContext: 0.00018435
    Epoch:   480     LossContext: 0.00018419
    Epoch:   490     LossContext: 0.00018404
    Epoch:   500     LossContext: 0.00018388
    Epoch:   510     LossContext: 0.00018372
    Epoch:   520     LossContext: 0.00018356
    Epoch:   530     LossContext: 0.00018339
    Epoch:   540     LossContext: 0.00018323
    Epoch:   550     LossContext: 0.00018307
    Epoch:   560     LossContext: 0.00018291
    Epoch:   570     LossContext: 0.00018274
    Epoch:   580     LossContext: 0.00018258
    Epoch:   590     LossContext: 0.00018241
    Epoch:   600     LossContext: 0.00018224
    Epoch:   610     LossContext: 0.00018208
    Epoch:   620     LossContext: 0.00018191
    Epoch:   630     LossContext: 0.00018174
    Epoch:   640     LossContext: 0.00018158
    Epoch:   650     LossContext: 0.00018141
    Epoch:   660     LossContext: 0.00018124
    Epoch:   670     LossContext: 0.00018107
    Epoch:   680     LossContext: 0.00018090
    Epoch:   690     LossContext: 0.00018073
    Epoch:   700     LossContext: 0.00018056
    Epoch:   710     LossContext: 0.00018039
    Epoch:   720     LossContext: 0.00018022
    Epoch:   730     LossContext: 0.00018005
    Epoch:   740     LossContext: 0.00017988
    Epoch:   750     LossContext: 0.00017971
    Epoch:   760     LossContext: 0.00017954
    Epoch:   770     LossContext: 0.00017937
    Epoch:   780     LossContext: 0.00017920
    Epoch:   790     LossContext: 0.00017903
    Epoch:   800     LossContext: 0.00017886
    Epoch:   810     LossContext: 0.00017869
    Epoch:   820     LossContext: 0.00017852
    Epoch:   830     LossContext: 0.00017835
    Epoch:   840     LossContext: 0.00017818
    Epoch:   850     LossContext: 0.00017802
    Epoch:   860     LossContext: 0.00017785
    Epoch:   870     LossContext: 0.00017768
    Epoch:   880     LossContext: 0.00017752
    Epoch:   890     LossContext: 0.00017735
    Epoch:   900     LossContext: 0.00017718
    Epoch:   910     LossContext: 0.00017701
    Epoch:   920     LossContext: 0.00017685
    Epoch:   930     LossContext: 0.00017668
    Epoch:   940     LossContext: 0.00017652
    Epoch:   950     LossContext: 0.00017635
    Epoch:   960     LossContext: 0.00017619
    Epoch:   970     LossContext: 0.00017602
    Epoch:   980     LossContext: 0.00017586
    Epoch:   990     LossContext: 0.00017569
    Epoch:  1000     LossContext: 0.00017553
    Epoch:  1010     LossContext: 0.00017537
    Epoch:  1020     LossContext: 0.00017521
    Epoch:  1030     LossContext: 0.00017504
    Epoch:  1040     LossContext: 0.00017488
    Epoch:  1050     LossContext: 0.00017472
    Epoch:  1060     LossContext: 0.00017456
    Epoch:  1070     LossContext: 0.00017440
    Epoch:  1080     LossContext: 0.00017424
    Epoch:  1090     LossContext: 0.00017408
    Epoch:  1100     LossContext: 0.00017393
    Epoch:  1110     LossContext: 0.00017377
    Epoch:  1120     LossContext: 0.00017361
    Epoch:  1130     LossContext: 0.00017346
    Epoch:  1140     LossContext: 0.00017330
    Epoch:  1150     LossContext: 0.00017315
    Epoch:  1160     LossContext: 0.00017299
    Epoch:  1170     LossContext: 0.00017284
    Epoch:  1180     LossContext: 0.00017268
    Epoch:  1190     LossContext: 0.00017253
    Epoch:  1200     LossContext: 0.00017238
    Epoch:  1210     LossContext: 0.00017223
    Epoch:  1220     LossContext: 0.00017208
    Epoch:  1230     LossContext: 0.00017193
    Epoch:  1240     LossContext: 0.00017178
    Epoch:  1250     LossContext: 0.00017163
    Epoch:  1260     LossContext: 0.00017148
    Epoch:  1270     LossContext: 0.00017133
    Epoch:  1280     LossContext: 0.00017119
    Epoch:  1290     LossContext: 0.00017104
    Epoch:  1300     LossContext: 0.00017089
    Epoch:  1310     LossContext: 0.00017075
    Epoch:  1320     LossContext: 0.00017061
    Epoch:  1330     LossContext: 0.00017046
    Epoch:  1340     LossContext: 0.00017032
    Epoch:  1350     LossContext: 0.00017018
    Epoch:  1360     LossContext: 0.00017004
    Epoch:  1370     LossContext: 0.00016990
    Epoch:  1380     LossContext: 0.00016976
    Epoch:  1390     LossContext: 0.00016962
    Epoch:  1400     LossContext: 0.00016948
    Epoch:  1410     LossContext: 0.00016934
    Epoch:  1420     LossContext: 0.00016920
    Epoch:  1430     LossContext: 0.00016907
    Epoch:  1440     LossContext: 0.00016893
    Epoch:  1450     LossContext: 0.00016879
    Epoch:  1460     LossContext: 0.00016866
    Epoch:  1470     LossContext: 0.00016853
    Epoch:  1480     LossContext: 0.00016839
    Epoch:  1490     LossContext: 0.00016826
    Epoch:  1499     LossContext: 0.00016814

Gradient descent adaptation time: 0 hours 1 mins 27 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.04346459
    Epoch:     1     LossContext: 0.03504100
    Epoch:     2     LossContext: 0.02748032
    Epoch:     3     LossContext: 0.02091280
    Epoch:    10     LossContext: 0.00332328
    Epoch:    20     LossContext: 0.00064014
    Epoch:    30     LossContext: 0.00070732
    Epoch:    40     LossContext: 0.00066474
    Epoch:    50     LossContext: 0.00037083
    Epoch:    60     LossContext: 0.00017847
    Epoch:    70     LossContext: 0.00014994
    Epoch:    80     LossContext: 0.00015545
    Epoch:    90     LossContext: 0.00014805
    Epoch:   100     LossContext: 0.00014570
    Epoch:   110     LossContext: 0.00014600
    Epoch:   120     LossContext: 0.00014561
    Epoch:   130     LossContext: 0.00014542
    Epoch:   140     LossContext: 0.00014536
    Epoch:   150     LossContext: 0.00014527
    Epoch:   160     LossContext: 0.00014522
    Epoch:   170     LossContext: 0.00014519
    Epoch:   180     LossContext: 0.00014516
    Epoch:   190     LossContext: 0.00014513
    Epoch:   200     LossContext: 0.00014511
    Epoch:   210     LossContext: 0.00014509
    Epoch:   220     LossContext: 0.00014507
    Epoch:   230     LossContext: 0.00014504
    Epoch:   240     LossContext: 0.00014502
    Epoch:   250     LossContext: 0.00014499
    Epoch:   260     LossContext: 0.00014497
    Epoch:   270     LossContext: 0.00014494
    Epoch:   280     LossContext: 0.00014492
    Epoch:   290     LossContext: 0.00014489
    Epoch:   300     LossContext: 0.00014487
    Epoch:   310     LossContext: 0.00014484
    Epoch:   320     LossContext: 0.00014481
    Epoch:   330     LossContext: 0.00014478
    Epoch:   340     LossContext: 0.00014476
    Epoch:   350     LossContext: 0.00014475
    Epoch:   360     LossContext: 0.00014473
    Epoch:   370     LossContext: 0.00014472
    Epoch:   380     LossContext: 0.00014470
    Epoch:   390     LossContext: 0.00014469
    Epoch:   400     LossContext: 0.00014467
    Epoch:   410     LossContext: 0.00014466
    Epoch:   420     LossContext: 0.00014464
    Epoch:   430     LossContext: 0.00014462
    Epoch:   440     LossContext: 0.00014461
    Epoch:   450     LossContext: 0.00014459
    Epoch:   460     LossContext: 0.00014457
    Epoch:   470     LossContext: 0.00014456
    Epoch:   480     LossContext: 0.00014454
    Epoch:   490     LossContext: 0.00014452
    Epoch:   500     LossContext: 0.00014450
    Epoch:   510     LossContext: 0.00014449
    Epoch:   520     LossContext: 0.00014447
    Epoch:   530     LossContext: 0.00014445
    Epoch:   540     LossContext: 0.00014443
    Epoch:   550     LossContext: 0.00014441
    Epoch:   560     LossContext: 0.00014439
    Epoch:   570     LossContext: 0.00014437
    Epoch:   580     LossContext: 0.00014435
    Epoch:   590     LossContext: 0.00014434
    Epoch:   600     LossContext: 0.00014432
    Epoch:   610     LossContext: 0.00014429
    Epoch:   620     LossContext: 0.00014427
    Epoch:   630     LossContext: 0.00014425
    Epoch:   640     LossContext: 0.00014423
    Epoch:   650     LossContext: 0.00014421
    Epoch:   660     LossContext: 0.00014419
    Epoch:   670     LossContext: 0.00014417
    Epoch:   680     LossContext: 0.00014415
    Epoch:   690     LossContext: 0.00014413
    Epoch:   700     LossContext: 0.00014411
    Epoch:   710     LossContext: 0.00014408
    Epoch:   720     LossContext: 0.00014406
    Epoch:   730     LossContext: 0.00014404
    Epoch:   740     LossContext: 0.00014402
    Epoch:   750     LossContext: 0.00014399
    Epoch:   760     LossContext: 0.00014397
    Epoch:   770     LossContext: 0.00014395
    Epoch:   780     LossContext: 0.00014392
    Epoch:   790     LossContext: 0.00014390
    Epoch:   800     LossContext: 0.00014388
    Epoch:   810     LossContext: 0.00014385
    Epoch:   820     LossContext: 0.00014383
    Epoch:   830     LossContext: 0.00014380
    Epoch:   840     LossContext: 0.00014378
    Epoch:   850     LossContext: 0.00014375
    Epoch:   860     LossContext: 0.00014373
    Epoch:   870     LossContext: 0.00014370
    Epoch:   880     LossContext: 0.00014368
    Epoch:   890     LossContext: 0.00014365
    Epoch:   900     LossContext: 0.00014363
    Epoch:   910     LossContext: 0.00014360
    Epoch:   920     LossContext: 0.00014358
    Epoch:   930     LossContext: 0.00014355
    Epoch:   940     LossContext: 0.00014353
    Epoch:   950     LossContext: 0.00014351
    Epoch:   960     LossContext: 0.00014349
    Epoch:   970     LossContext: 0.00014347
    Epoch:   980     LossContext: 0.00014345
    Epoch:   990     LossContext: 0.00014343
    Epoch:  1000     LossContext: 0.00014341
    Epoch:  1010     LossContext: 0.00014339
    Epoch:  1020     LossContext: 0.00014337
    Epoch:  1030     LossContext: 0.00014335
    Epoch:  1040     LossContext: 0.00014333
    Epoch:  1050     LossContext: 0.00014331
    Epoch:  1060     LossContext: 0.00014329
    Epoch:  1070     LossContext: 0.00014327
    Epoch:  1080     LossContext: 0.00014325
    Epoch:  1090     LossContext: 0.00014323
    Epoch:  1100     LossContext: 0.00014321
    Epoch:  1110     LossContext: 0.00014319
    Epoch:  1120     LossContext: 0.00014317
    Epoch:  1130     LossContext: 0.00014315
    Epoch:  1140     LossContext: 0.00014313
    Epoch:  1150     LossContext: 0.00014311
    Epoch:  1160     LossContext: 0.00014309
    Epoch:  1170     LossContext: 0.00014306
    Epoch:  1180     LossContext: 0.00014304
    Epoch:  1190     LossContext: 0.00014302
    Epoch:  1200     LossContext: 0.00014300
    Epoch:  1210     LossContext: 0.00014298
    Epoch:  1220     LossContext: 0.00014296
    Epoch:  1230     LossContext: 0.00014294
    Epoch:  1240     LossContext: 0.00014292
    Epoch:  1250     LossContext: 0.00014290
    Epoch:  1260     LossContext: 0.00014288
    Epoch:  1270     LossContext: 0.00014285
    Epoch:  1280     LossContext: 0.00014283
    Epoch:  1290     LossContext: 0.00014281
    Epoch:  1300     LossContext: 0.00014279
    Epoch:  1310     LossContext: 0.00014277
    Epoch:  1320     LossContext: 0.00014275
    Epoch:  1330     LossContext: 0.00014272
    Epoch:  1340     LossContext: 0.00014270
    Epoch:  1350     LossContext: 0.00014268
    Epoch:  1360     LossContext: 0.00014266
    Epoch:  1370     LossContext: 0.00014264
    Epoch:  1380     LossContext: 0.00014261
    Epoch:  1390     LossContext: 0.00014259
    Epoch:  1400     LossContext: 0.00014257
    Epoch:  1410     LossContext: 0.00014255
    Epoch:  1420     LossContext: 0.00014252
    Epoch:  1430     LossContext: 0.00014250
    Epoch:  1440     LossContext: 0.00014248
    Epoch:  1450     LossContext: 0.00014246
    Epoch:  1460     LossContext: 0.00014243
    Epoch:  1470     LossContext: 0.00014241
    Epoch:  1480     LossContext: 0.00014239
    Epoch:  1490     LossContext: 0.00014236
    Epoch:  1499     LossContext: 0.00014234

Gradient descent adaptation time: 0 hours 1 mins 27 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.01987908
    Epoch:     1     LossContext: 0.01657370
    Epoch:     2     LossContext: 0.01366044
    Epoch:     3     LossContext: 0.01113278
    Epoch:    10     LossContext: 0.00221755
    Epoch:    20     LossContext: 0.00028197
    Epoch:    30     LossContext: 0.00023151
    Epoch:    40     LossContext: 0.00027797
    Epoch:    50     LossContext: 0.00026524
    Epoch:    60     LossContext: 0.00023317
    Epoch:    70     LossContext: 0.00021231
    Epoch:    80     LossContext: 0.00020503
    Epoch:    90     LossContext: 0.00020355
    Epoch:   100     LossContext: 0.00020278
    Epoch:   110     LossContext: 0.00020167
    Epoch:   120     LossContext: 0.00020050
    Epoch:   130     LossContext: 0.00019936
    Epoch:   140     LossContext: 0.00019822
    Epoch:   150     LossContext: 0.00019704
    Epoch:   160     LossContext: 0.00019607
    Epoch:   170     LossContext: 0.00019534
    Epoch:   180     LossContext: 0.00019485
    Epoch:   190     LossContext: 0.00019440
    Epoch:   200     LossContext: 0.00019395
    Epoch:   210     LossContext: 0.00019350
    Epoch:   220     LossContext: 0.00019305
    Epoch:   230     LossContext: 0.00019274
    Epoch:   240     LossContext: 0.00019242
    Epoch:   250     LossContext: 0.00019210
    Epoch:   260     LossContext: 0.00019177
    Epoch:   270     LossContext: 0.00019150
    Epoch:   280     LossContext: 0.00019123
    Epoch:   290     LossContext: 0.00019098
    Epoch:   300     LossContext: 0.00019072
    Epoch:   310     LossContext: 0.00019045
    Epoch:   320     LossContext: 0.00019018
    Epoch:   330     LossContext: 0.00018991
    Epoch:   340     LossContext: 0.00018973
    Epoch:   350     LossContext: 0.00018959
    Epoch:   360     LossContext: 0.00018944
    Epoch:   370     LossContext: 0.00018929
    Epoch:   380     LossContext: 0.00018914
    Epoch:   390     LossContext: 0.00018899
    Epoch:   400     LossContext: 0.00018884
    Epoch:   410     LossContext: 0.00018869
    Epoch:   420     LossContext: 0.00018853
    Epoch:   430     LossContext: 0.00018837
    Epoch:   440     LossContext: 0.00018821
    Epoch:   450     LossContext: 0.00018804
    Epoch:   460     LossContext: 0.00018788
    Epoch:   470     LossContext: 0.00018771
    Epoch:   480     LossContext: 0.00018754
    Epoch:   490     LossContext: 0.00018739
    Epoch:   500     LossContext: 0.00018724
    Epoch:   510     LossContext: 0.00018709
    Epoch:   520     LossContext: 0.00018694
    Epoch:   530     LossContext: 0.00018679
    Epoch:   540     LossContext: 0.00018664
    Epoch:   550     LossContext: 0.00018648
    Epoch:   560     LossContext: 0.00018633
    Epoch:   570     LossContext: 0.00018617
    Epoch:   580     LossContext: 0.00018601
    Epoch:   590     LossContext: 0.00018585
    Epoch:   600     LossContext: 0.00018568
    Epoch:   610     LossContext: 0.00018552
    Epoch:   620     LossContext: 0.00018535
    Epoch:   630     LossContext: 0.00018518
    Epoch:   640     LossContext: 0.00018501
    Epoch:   650     LossContext: 0.00018484
    Epoch:   660     LossContext: 0.00018466
    Epoch:   670     LossContext: 0.00018449
    Epoch:   680     LossContext: 0.00018431
    Epoch:   690     LossContext: 0.00018413
    Epoch:   700     LossContext: 0.00018395
    Epoch:   710     LossContext: 0.00018377
    Epoch:   720     LossContext: 0.00018359
    Epoch:   730     LossContext: 0.00018340
    Epoch:   740     LossContext: 0.00018322
    Epoch:   750     LossContext: 0.00018303
    Epoch:   760     LossContext: 0.00018284
    Epoch:   770     LossContext: 0.00018265
    Epoch:   780     LossContext: 0.00018245
    Epoch:   790     LossContext: 0.00018226
    Epoch:   800     LossContext: 0.00018206
    Epoch:   810     LossContext: 0.00018186
    Epoch:   820     LossContext: 0.00018166
    Epoch:   830     LossContext: 0.00018151
    Epoch:   840     LossContext: 0.00018135
    Epoch:   850     LossContext: 0.00018119
    Epoch:   860     LossContext: 0.00018104
    Epoch:   870     LossContext: 0.00018089
    Epoch:   880     LossContext: 0.00018073
    Epoch:   890     LossContext: 0.00018058
    Epoch:   900     LossContext: 0.00018042
    Epoch:   910     LossContext: 0.00018026
    Epoch:   920     LossContext: 0.00018011
    Epoch:   930     LossContext: 0.00017994
    Epoch:   940     LossContext: 0.00017978
    Epoch:   950     LossContext: 0.00017962
    Epoch:   960     LossContext: 0.00017946
    Epoch:   970     LossContext: 0.00017930
    Epoch:   980     LossContext: 0.00017913
    Epoch:   990     LossContext: 0.00017896
    Epoch:  1000     LossContext: 0.00017878
    Epoch:  1010     LossContext: 0.00017862
    Epoch:  1020     LossContext: 0.00017844
    Epoch:  1030     LossContext: 0.00017827
    Epoch:  1040     LossContext: 0.00017810
    Epoch:  1050     LossContext: 0.00017792
    Epoch:  1060     LossContext: 0.00017774
    Epoch:  1070     LossContext: 0.00017756
    Epoch:  1080     LossContext: 0.00017739
    Epoch:  1090     LossContext: 0.00017720
    Epoch:  1100     LossContext: 0.00017702
    Epoch:  1110     LossContext: 0.00017684
    Epoch:  1120     LossContext: 0.00017665
    Epoch:  1130     LossContext: 0.00017647
    Epoch:  1140     LossContext: 0.00017628
    Epoch:  1150     LossContext: 0.00017609
    Epoch:  1160     LossContext: 0.00017590
    Epoch:  1170     LossContext: 0.00017571
    Epoch:  1180     LossContext: 0.00017552
    Epoch:  1190     LossContext: 0.00017535
    Epoch:  1200     LossContext: 0.00017517
    Epoch:  1210     LossContext: 0.00017500
    Epoch:  1220     LossContext: 0.00017482
    Epoch:  1230     LossContext: 0.00017464
    Epoch:  1240     LossContext: 0.00017446
    Epoch:  1250     LossContext: 0.00017429
    Epoch:  1260     LossContext: 0.00017410
    Epoch:  1270     LossContext: 0.00017392
    Epoch:  1280     LossContext: 0.00017374
    Epoch:  1290     LossContext: 0.00017356
    Epoch:  1300     LossContext: 0.00017337
    Epoch:  1310     LossContext: 0.00017318
    Epoch:  1320     LossContext: 0.00017300
    Epoch:  1330     LossContext: 0.00017281
    Epoch:  1340     LossContext: 0.00017262
    Epoch:  1350     LossContext: 0.00017243
    Epoch:  1360     LossContext: 0.00017224
    Epoch:  1370     LossContext: 0.00017205
    Epoch:  1380     LossContext: 0.00017190
    Epoch:  1390     LossContext: 0.00017174
    Epoch:  1400     LossContext: 0.00017158
    Epoch:  1410     LossContext: 0.00017143
    Epoch:  1420     LossContext: 0.00017131
    Epoch:  1430     LossContext: 0.00017119
    Epoch:  1440     LossContext: 0.00017108
    Epoch:  1450     LossContext: 0.00017097
    Epoch:  1460     LossContext: 0.00017086
    Epoch:  1470     LossContext: 0.00017075
    Epoch:  1480     LossContext: 0.00017063
    Epoch:  1490     LossContext: 0.00017053
    Epoch:  1499     LossContext: 0.00017043

Gradient descent adaptation time: 0 hours 1 mins 27 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./05052024-164731/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 0.00016311432

==  Begining out-of-distribution visualisation ... ==
    Environment id: 0
    Trajectory id: 0
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./05052024-164731/adapt/results_ood.png
Testing finished. Figure saved in: ./05052024-164731/adapt/results_ood_uq.pdf

======================= DONE ======================== 

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

========= STARTING =========
Working with a context size of 2^6 = 64
Run folder created successfuly: ./05052024-183119/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 183136
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 183136
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 62480 


WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed


=== Beginning training with proximal alternating minimization ... ===
    Number of examples in a batch: 4
    Maximum number of steps per inner minimization: 25
    Maximum number of outer minimizations: 500
    Maximum total number of training steps: 12500

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Outer Step:     0      LossTrajs: 1.81048226     ContextsNorm: 0.00000000     ValIndCrit: 1.69893932
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.72e-06
        -DiffCxt:  inf
    Outer Step:     1      LossTrajs: 1.52597380     ContextsNorm: 0.00033217     ValIndCrit: 1.43067980
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.63e-06
        -DiffCxt:  1.41e-03
    Outer Step:     2      LossTrajs: 1.21870816     ContextsNorm: 0.00100661     ValIndCrit: 1.14051926
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.32e-06
        -DiffCxt:  4.63e-04
    Outer Step:     3      LossTrajs: 0.80525935     ContextsNorm: 0.00282360     ValIndCrit: 0.75452369
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.54e-06
        -DiffCxt:  3.21e-04
    Outer Step:    10      LossTrajs: 0.23933432     ContextsNorm: 0.01478798     ValIndCrit: 0.28284597
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.16e-06
        -DiffCxt:  2.49e-05
    Outer Step:    20      LossTrajs: 0.16557273     ContextsNorm: 0.02216101     ValIndCrit: 0.18924999
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.14e-07
        -DiffCxt:  4.33e-07
    Outer Step:    30      LossTrajs: 0.07620234     ContextsNorm: 0.02073322     ValIndCrit: 0.07843439
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.45e-07
        -DiffCxt:  4.29e-07
    Outer Step:    40      LossTrajs: 0.04899263     ContextsNorm: 0.02463493     ValIndCrit: 0.05020675
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.58e-07
        -DiffCxt:  3.70e-06
    Outer Step:    50      LossTrajs: 0.01649262     ContextsNorm: 0.03229766     ValIndCrit: 0.01827986
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.66e-07
        -DiffCxt:  3.43e-07
    Outer Step:    60      LossTrajs: 0.01029306     ContextsNorm: 0.03265550     ValIndCrit: 0.01272775
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.76e-08
        -DiffCxt:  6.00e-08
    Outer Step:    70      LossTrajs: 0.00701607     ContextsNorm: 0.03302237     ValIndCrit: 0.00942728
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.67e-08
        -DiffCxt:  8.93e-08
    Outer Step:    80      LossTrajs: 0.00523411     ContextsNorm: 0.03290367     ValIndCrit: 0.00759334
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.73e-08
        -DiffCxt:  5.46e-08
    Outer Step:    90      LossTrajs: 0.00413003     ContextsNorm: 0.03321952     ValIndCrit: 0.00628128
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.81e-08
        -DiffCxt:  4.27e-08
    Outer Step:   100      LossTrajs: 0.00294235     ContextsNorm: 0.03338530     ValIndCrit: 0.00469544
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.65e-08
        -DiffCxt:  3.20e-08
    Outer Step:   110      LossTrajs: 0.00197847     ContextsNorm: 0.03348503     ValIndCrit: 0.00335817
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.40e-08
        -DiffCxt:  4.07e-08
    Outer Step:   120      LossTrajs: 0.00143201     ContextsNorm: 0.03324093     ValIndCrit: 0.00256406
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.02e-08
        -DiffCxt:  2.32e-08
    Outer Step:   130      LossTrajs: 0.00105885     ContextsNorm: 0.03346541     ValIndCrit: 0.00200626
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.40e-09
        -DiffCxt:  5.89e-08
    Outer Step:   140      LossTrajs: 0.00083892     ContextsNorm: 0.03306967     ValIndCrit: 0.00159099
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.78e-09
        -DiffCxt:  2.91e-08
    Outer Step:   150      LossTrajs: 0.00066642     ContextsNorm: 0.03328653     ValIndCrit: 0.00125336
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.80e-09
        -DiffCxt:  4.33e-08
    Outer Step:   160      LossTrajs: 0.00054861     ContextsNorm: 0.03335964     ValIndCrit: 0.00101505
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.17e-09
        -DiffCxt:  2.80e-08
    Outer Step:   170      LossTrajs: 0.00046315     ContextsNorm: 0.03339843     ValIndCrit: 0.00082214
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.23e-09
        -DiffCxt:  3.49e-08
    Outer Step:   180      LossTrajs: 0.00037063     ContextsNorm: 0.03324720     ValIndCrit: 0.00064310
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.56e-09
        -DiffCxt:  2.53e-08
    Outer Step:   190      LossTrajs: 0.00028973     ContextsNorm: 0.03312240     ValIndCrit: 0.00051040
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.05e-09
        -DiffCxt:  4.97e-08
    Outer Step:   200      LossTrajs: 0.00023452     ContextsNorm: 0.03290447     ValIndCrit: 0.00041199
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.07e-08
        -DiffCxt:  7.71e-08
    Outer Step:   210      LossTrajs: 0.00018643     ContextsNorm: 0.03301559     ValIndCrit: 0.00033723
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.15e-09
        -DiffCxt:  1.91e-08
    Outer Step:   220      LossTrajs: 0.00015516     ContextsNorm: 0.03307920     ValIndCrit: 0.00028645
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.40e-09
        -DiffCxt:  3.96e-08
    Outer Step:   230      LossTrajs: 0.00013642     ContextsNorm: 0.03306305     ValIndCrit: 0.00024225
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.02e-08
        -DiffCxt:  2.56e-08
    Outer Step:   240      LossTrajs: 0.00011504     ContextsNorm: 0.03307759     ValIndCrit: 0.00021059
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.34e-09
        -DiffCxt:  1.04e-08
    Outer Step:   250      LossTrajs: 0.00010855     ContextsNorm: 0.03306099     ValIndCrit: 0.00018781
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.46e-09
        -DiffCxt:  1.10e-08
    Outer Step:   260      LossTrajs: 0.00010311     ContextsNorm: 0.03295121     ValIndCrit: 0.00017183
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.61e-09
        -DiffCxt:  2.25e-08
    Outer Step:   270      LossTrajs: 0.00009015     ContextsNorm: 0.03293805     ValIndCrit: 0.00015212
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.18e-09
        -DiffCxt:  1.26e-08
    Outer Step:   280      LossTrajs: 0.00008556     ContextsNorm: 0.03301554     ValIndCrit: 0.00014481
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.31e-09
        -DiffCxt:  4.16e-08
    Outer Step:   290      LossTrajs: 0.00008066     ContextsNorm: 0.03297944     ValIndCrit: 0.00013004
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.24e-09
        -DiffCxt:  5.74e-08
    Outer Step:   300      LossTrajs: 0.00007912     ContextsNorm: 0.03291349     ValIndCrit: 0.00012397
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.50e-09
        -DiffCxt:  1.51e-08
    Outer Step:   310      LossTrajs: 0.00007778     ContextsNorm: 0.03287883     ValIndCrit: 0.00011898
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   12
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.87e-09
        -DiffCxt:  6.99e-09
    Outer Step:   320      LossTrajs: 0.00007016     ContextsNorm: 0.03285431     ValIndCrit: 0.00011418
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.81e-10
        -DiffCxt:  1.32e-08
    Outer Step:   330      LossTrajs: 0.00007085     ContextsNorm: 0.03279797     ValIndCrit: 0.00010926
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.66e-09
        -DiffCxt:  3.46e-08
    Outer Step:   340      LossTrajs: 0.00006910     ContextsNorm: 0.03278540     ValIndCrit: 0.00010664
        Saving best model so far ...
        -NbInnerStepsNode:    6
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.27e-10
        -DiffCxt:  1.73e-08
    Outer Step:   350      LossTrajs: 0.00006847     ContextsNorm: 0.03277346     ValIndCrit: 0.00010474
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   13
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.87e-10
        -DiffCxt:  9.88e-09
    Outer Step:   360      LossTrajs: 0.00006688     ContextsNorm: 0.03273167     ValIndCrit: 0.00010274
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.48e-10
        -DiffCxt:  9.02e-09
    Outer Step:   370      LossTrajs: 0.00006801     ContextsNorm: 0.03267571     ValIndCrit: 0.00010182
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.13e-08
        -DiffCxt:  1.28e-07
    Outer Step:   380      LossTrajs: 0.00006456     ContextsNorm: 0.03264459     ValIndCrit: 0.00009684
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.96e-09
        -DiffCxt:  1.84e-08
    Outer Step:   390      LossTrajs: 0.00006393     ContextsNorm: 0.03264748     ValIndCrit: 0.00009518
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.74e-09
        -DiffCxt:  1.42e-07
    Outer Step:   400      LossTrajs: 0.00006168     ContextsNorm: 0.03267595     ValIndCrit: 0.00008914
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.39e-09
        -DiffCxt:  3.35e-08
    Outer Step:   410      LossTrajs: 0.00006146     ContextsNorm: 0.03262550     ValIndCrit: 0.00008771
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.78e-09
        -DiffCxt:  3.26e-08
    Outer Step:   420      LossTrajs: 0.00006140     ContextsNorm: 0.03257016     ValIndCrit: 0.00008480
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.85e-09
        -DiffCxt:  8.38e-08
    Outer Step:   430      LossTrajs: 0.00005874     ContextsNorm: 0.03251548     ValIndCrit: 0.00008364
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.73e-09
        -DiffCxt:  1.81e-08
    Outer Step:   440      LossTrajs: 0.00006173     ContextsNorm: 0.03253146     ValIndCrit: 0.00008093
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.81e-09
        -DiffCxt:  1.86e-08
    Outer Step:   450      LossTrajs: 0.00006008     ContextsNorm: 0.03250374     ValIndCrit: 0.00007765
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.03e-09
        -DiffCxt:  3.36e-08
    Outer Step:   460      LossTrajs: 0.00005723     ContextsNorm: 0.03243598     ValIndCrit: 0.00007605
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.80e-09
        -DiffCxt:  3.42e-08
    Outer Step:   470      LossTrajs: 0.00005607     ContextsNorm: 0.03237136     ValIndCrit: 0.00007583
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.62e-09
        -DiffCxt:  9.39e-08
    Outer Step:   480      LossTrajs: 0.00005421     ContextsNorm: 0.03232678     ValIndCrit: 0.00007247
        Saving best model so far ...
        -NbInnerStepsNode:    2
        -NbInnerStepsCxt:   24
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.66e-10
        -DiffCxt:  8.04e-09
    Outer Step:   490      LossTrajs: 0.00005474     ContextsNorm: 0.03231046     ValIndCrit: 0.00007093
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   18
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.81e-10
        -DiffCxt:  9.85e-09
    Outer Step:   499      LossTrajs: 0.00005403     ContextsNorm: 0.03225015     ValIndCrit: 0.00006989
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.13e-09
        -DiffCxt:  1.27e-07

Total gradient descent training time: 1 hours 51 mins 24 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 202301
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 6.9889975e-05


############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]
==  Begining in-domain visualisation ... ==
    Environment id: 0
    Trajectory id: 28
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./05052024-183119/results_in_domain.png
Testing finished. Figure saved in: ./05052024-183119/results_in_domain_uq.pdf
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.02636777
    Epoch:     1     LossContext: 0.01963442
    Epoch:     2     LossContext: 0.01332687
    Epoch:     3     LossContext: 0.00786487
    Epoch:    10     LossContext: 0.00512994
    Epoch:    20     LossContext: 0.00138375
    Epoch:    30     LossContext: 0.00014829
    Epoch:    40     LossContext: 0.00004671
    Epoch:    50     LossContext: 0.00004263
    Epoch:    60     LossContext: 0.00004188
    Epoch:    70     LossContext: 0.00004116
    Epoch:    80     LossContext: 0.00004098
    Epoch:    90     LossContext: 0.00004113
    Epoch:   100     LossContext: 0.00004117
    Epoch:   110     LossContext: 0.00004106
    Epoch:   120     LossContext: 0.00004093
    Epoch:   130     LossContext: 0.00004085
    Epoch:   140     LossContext: 0.00004081
    Epoch:   150     LossContext: 0.00004079
    Epoch:   160     LossContext: 0.00004076
    Epoch:   170     LossContext: 0.00004073
    Epoch:   180     LossContext: 0.00004072
    Epoch:   190     LossContext: 0.00004070
    Epoch:   200     LossContext: 0.00004068
    Epoch:   210     LossContext: 0.00004067
    Epoch:   220     LossContext: 0.00004065
    Epoch:   230     LossContext: 0.00004063
    Epoch:   240     LossContext: 0.00004061
    Epoch:   250     LossContext: 0.00004060
    Epoch:   260     LossContext: 0.00004058
    Epoch:   270     LossContext: 0.00004056
    Epoch:   280     LossContext: 0.00004054
    Epoch:   290     LossContext: 0.00004052
    Epoch:   300     LossContext: 0.00004050
    Epoch:   310     LossContext: 0.00004048
    Epoch:   320     LossContext: 0.00004045
    Epoch:   330     LossContext: 0.00004043
    Epoch:   340     LossContext: 0.00004042
    Epoch:   350     LossContext: 0.00004041
    Epoch:   360     LossContext: 0.00004040
    Epoch:   370     LossContext: 0.00004038
    Epoch:   380     LossContext: 0.00004037
    Epoch:   390     LossContext: 0.00004036
    Epoch:   400     LossContext: 0.00004035
    Epoch:   410     LossContext: 0.00004034
    Epoch:   420     LossContext: 0.00004032
    Epoch:   430     LossContext: 0.00004031
    Epoch:   440     LossContext: 0.00004030
    Epoch:   450     LossContext: 0.00004028
    Epoch:   460     LossContext: 0.00004027
    Epoch:   470     LossContext: 0.00004026
    Epoch:   480     LossContext: 0.00004024
    Epoch:   490     LossContext: 0.00004023
    Epoch:   500     LossContext: 0.00004022
    Epoch:   510     LossContext: 0.00004020
    Epoch:   520     LossContext: 0.00004019
    Epoch:   530     LossContext: 0.00004017
    Epoch:   540     LossContext: 0.00004016
    Epoch:   550     LossContext: 0.00004014
    Epoch:   560     LossContext: 0.00004013
    Epoch:   570     LossContext: 0.00004011
    Epoch:   580     LossContext: 0.00004010
    Epoch:   590     LossContext: 0.00004008
    Epoch:   600     LossContext: 0.00004007
    Epoch:   610     LossContext: 0.00004005
    Epoch:   620     LossContext: 0.00004003
    Epoch:   630     LossContext: 0.00004002
    Epoch:   640     LossContext: 0.00004000
    Epoch:   650     LossContext: 0.00003999
    Epoch:   660     LossContext: 0.00003997
    Epoch:   670     LossContext: 0.00003995
    Epoch:   680     LossContext: 0.00003993
    Epoch:   690     LossContext: 0.00003992
    Epoch:   700     LossContext: 0.00003990
    Epoch:   710     LossContext: 0.00003988
    Epoch:   720     LossContext: 0.00003986
    Epoch:   730     LossContext: 0.00003985
    Epoch:   740     LossContext: 0.00003983
    Epoch:   750     LossContext: 0.00003981
    Epoch:   760     LossContext: 0.00003979
    Epoch:   770     LossContext: 0.00003977
    Epoch:   780     LossContext: 0.00003975
    Epoch:   790     LossContext: 0.00003973
    Epoch:   800     LossContext: 0.00003971
    Epoch:   810     LossContext: 0.00003969
    Epoch:   820     LossContext: 0.00003968
    Epoch:   830     LossContext: 0.00003966
    Epoch:   840     LossContext: 0.00003964
    Epoch:   850     LossContext: 0.00003962
    Epoch:   860     LossContext: 0.00003959
    Epoch:   870     LossContext: 0.00003957
    Epoch:   880     LossContext: 0.00003955
    Epoch:   890     LossContext: 0.00003953
    Epoch:   900     LossContext: 0.00003951
    Epoch:   910     LossContext: 0.00003949
    Epoch:   920     LossContext: 0.00003947
    Epoch:   930     LossContext: 0.00003945
    Epoch:   940     LossContext: 0.00003943
    Epoch:   950     LossContext: 0.00003940
    Epoch:   960     LossContext: 0.00003938
    Epoch:   970     LossContext: 0.00003936
    Epoch:   980     LossContext: 0.00003934
    Epoch:   990     LossContext: 0.00003932
    Epoch:  1000     LossContext: 0.00003930
    Epoch:  1010     LossContext: 0.00003927
    Epoch:  1020     LossContext: 0.00003925
    Epoch:  1030     LossContext: 0.00003923
    Epoch:  1040     LossContext: 0.00003921
    Epoch:  1050     LossContext: 0.00003918
    Epoch:  1060     LossContext: 0.00003916
    Epoch:  1070     LossContext: 0.00003914
    Epoch:  1080     LossContext: 0.00003911
    Epoch:  1090     LossContext: 0.00003909
    Epoch:  1100     LossContext: 0.00003907
    Epoch:  1110     LossContext: 0.00003904
    Epoch:  1120     LossContext: 0.00003902
    Epoch:  1130     LossContext: 0.00003899
    Epoch:  1140     LossContext: 0.00003897
    Epoch:  1150     LossContext: 0.00003895
    Epoch:  1160     LossContext: 0.00003892
    Epoch:  1170     LossContext: 0.00003890
    Epoch:  1180     LossContext: 0.00003887
    Epoch:  1190     LossContext: 0.00003885
    Epoch:  1200     LossContext: 0.00003882
    Epoch:  1210     LossContext: 0.00003880
    Epoch:  1220     LossContext: 0.00003877
    Epoch:  1230     LossContext: 0.00003875
    Epoch:  1240     LossContext: 0.00003872
    Epoch:  1250     LossContext: 0.00003869
    Epoch:  1260     LossContext: 0.00003867
    Epoch:  1270     LossContext: 0.00003864
    Epoch:  1280     LossContext: 0.00003862
    Epoch:  1290     LossContext: 0.00003859
    Epoch:  1300     LossContext: 0.00003856
    Epoch:  1310     LossContext: 0.00003854
    Epoch:  1320     LossContext: 0.00003851
    Epoch:  1330     LossContext: 0.00003848
    Epoch:  1340     LossContext: 0.00003845
    Epoch:  1350     LossContext: 0.00003843
    Epoch:  1360     LossContext: 0.00003840
    Epoch:  1370     LossContext: 0.00003837
    Epoch:  1380     LossContext: 0.00003834
    Epoch:  1390     LossContext: 0.00003831
    Epoch:  1400     LossContext: 0.00003829
    Epoch:  1410     LossContext: 0.00003827
    Epoch:  1420     LossContext: 0.00003825
    Epoch:  1430     LossContext: 0.00003823
    Epoch:  1440     LossContext: 0.00003821
    Epoch:  1450     LossContext: 0.00003819
    Epoch:  1460     LossContext: 0.00003817
    Epoch:  1470     LossContext: 0.00003815
    Epoch:  1480     LossContext: 0.00003813
    Epoch:  1490     LossContext: 0.00003811
    Epoch:  1499     LossContext: 0.00003809

Gradient descent adaptation time: 0 hours 1 mins 41 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.12024056
    Epoch:     1     LossContext: 0.10377018
    Epoch:     2     LossContext: 0.08706065
    Epoch:     3     LossContext: 0.07037111
    Epoch:    10     LossContext: 0.00535420
    Epoch:    20     LossContext: 0.00126520
    Epoch:    30     LossContext: 0.00347466
    Epoch:    40     LossContext: 0.00119581
    Epoch:    50     LossContext: 0.00013667
    Epoch:    60     LossContext: 0.00016926
    Epoch:    70     LossContext: 0.00011819
    Epoch:    80     LossContext: 0.00009729
    Epoch:    90     LossContext: 0.00009800
    Epoch:   100     LossContext: 0.00009176
    Epoch:   110     LossContext: 0.00009144
    Epoch:   120     LossContext: 0.00009074
    Epoch:   130     LossContext: 0.00009057
    Epoch:   140     LossContext: 0.00009049
    Epoch:   150     LossContext: 0.00009043
    Epoch:   160     LossContext: 0.00009039
    Epoch:   170     LossContext: 0.00009035
    Epoch:   180     LossContext: 0.00009033
    Epoch:   190     LossContext: 0.00009031
    Epoch:   200     LossContext: 0.00009029
    Epoch:   210     LossContext: 0.00009026
    Epoch:   220     LossContext: 0.00009024
    Epoch:   230     LossContext: 0.00009022
    Epoch:   240     LossContext: 0.00009019
    Epoch:   250     LossContext: 0.00009017
    Epoch:   260     LossContext: 0.00009014
    Epoch:   270     LossContext: 0.00009012
    Epoch:   280     LossContext: 0.00009009
    Epoch:   290     LossContext: 0.00009006
    Epoch:   300     LossContext: 0.00009003
    Epoch:   310     LossContext: 0.00009000
    Epoch:   320     LossContext: 0.00008998
    Epoch:   330     LossContext: 0.00008995
    Epoch:   340     LossContext: 0.00008993
    Epoch:   350     LossContext: 0.00008991
    Epoch:   360     LossContext: 0.00008990
    Epoch:   370     LossContext: 0.00008988
    Epoch:   380     LossContext: 0.00008986
    Epoch:   390     LossContext: 0.00008985
    Epoch:   400     LossContext: 0.00008983
    Epoch:   410     LossContext: 0.00008981
    Epoch:   420     LossContext: 0.00008980
    Epoch:   430     LossContext: 0.00008978
    Epoch:   440     LossContext: 0.00008976
    Epoch:   450     LossContext: 0.00008974
    Epoch:   460     LossContext: 0.00008973
    Epoch:   470     LossContext: 0.00008971
    Epoch:   480     LossContext: 0.00008969
    Epoch:   490     LossContext: 0.00008967
    Epoch:   500     LossContext: 0.00008965
    Epoch:   510     LossContext: 0.00008963
    Epoch:   520     LossContext: 0.00008961
    Epoch:   530     LossContext: 0.00008959
    Epoch:   540     LossContext: 0.00008957
    Epoch:   550     LossContext: 0.00008955
    Epoch:   560     LossContext: 0.00008953
    Epoch:   570     LossContext: 0.00008951
    Epoch:   580     LossContext: 0.00008949
    Epoch:   590     LossContext: 0.00008947
    Epoch:   600     LossContext: 0.00008945
    Epoch:   610     LossContext: 0.00008942
    Epoch:   620     LossContext: 0.00008940
    Epoch:   630     LossContext: 0.00008938
    Epoch:   640     LossContext: 0.00008936
    Epoch:   650     LossContext: 0.00008934
    Epoch:   660     LossContext: 0.00008931
    Epoch:   670     LossContext: 0.00008929
    Epoch:   680     LossContext: 0.00008927
    Epoch:   690     LossContext: 0.00008924
    Epoch:   700     LossContext: 0.00008922
    Epoch:   710     LossContext: 0.00008920
    Epoch:   720     LossContext: 0.00008917
    Epoch:   730     LossContext: 0.00008915
    Epoch:   740     LossContext: 0.00008912
    Epoch:   750     LossContext: 0.00008910
    Epoch:   760     LossContext: 0.00008907
    Epoch:   770     LossContext: 0.00008905
    Epoch:   780     LossContext: 0.00008902
    Epoch:   790     LossContext: 0.00008899
    Epoch:   800     LossContext: 0.00008897
    Epoch:   810     LossContext: 0.00008894
    Epoch:   820     LossContext: 0.00008891
    Epoch:   830     LossContext: 0.00008889
    Epoch:   840     LossContext: 0.00008886
    Epoch:   850     LossContext: 0.00008883
    Epoch:   860     LossContext: 0.00008881
    Epoch:   870     LossContext: 0.00008878
    Epoch:   880     LossContext: 0.00008875
    Epoch:   890     LossContext: 0.00008872
    Epoch:   900     LossContext: 0.00008869
    Epoch:   910     LossContext: 0.00008866
    Epoch:   920     LossContext: 0.00008864
    Epoch:   930     LossContext: 0.00008861
    Epoch:   940     LossContext: 0.00008858
    Epoch:   950     LossContext: 0.00008855
    Epoch:   960     LossContext: 0.00008852
    Epoch:   970     LossContext: 0.00008849
    Epoch:   980     LossContext: 0.00008846
    Epoch:   990     LossContext: 0.00008843
    Epoch:  1000     LossContext: 0.00008839
    Epoch:  1010     LossContext: 0.00008836
    Epoch:  1020     LossContext: 0.00008833
    Epoch:  1030     LossContext: 0.00008830
    Epoch:  1040     LossContext: 0.00008827
    Epoch:  1050     LossContext: 0.00008824
    Epoch:  1060     LossContext: 0.00008820
    Epoch:  1070     LossContext: 0.00008817
    Epoch:  1080     LossContext: 0.00008814
    Epoch:  1090     LossContext: 0.00008811
    Epoch:  1100     LossContext: 0.00008807
    Epoch:  1110     LossContext: 0.00008804
    Epoch:  1120     LossContext: 0.00008801
    Epoch:  1130     LossContext: 0.00008797
    Epoch:  1140     LossContext: 0.00008794
    Epoch:  1150     LossContext: 0.00008790
    Epoch:  1160     LossContext: 0.00008787
    Epoch:  1170     LossContext: 0.00008784
    Epoch:  1180     LossContext: 0.00008780
    Epoch:  1190     LossContext: 0.00008777
    Epoch:  1200     LossContext: 0.00008774
    Epoch:  1210     LossContext: 0.00008770
    Epoch:  1220     LossContext: 0.00008767
    Epoch:  1230     LossContext: 0.00008764
    Epoch:  1240     LossContext: 0.00008760
    Epoch:  1250     LossContext: 0.00008757
    Epoch:  1260     LossContext: 0.00008753
    Epoch:  1270     LossContext: 0.00008750
    Epoch:  1280     LossContext: 0.00008746
    Epoch:  1290     LossContext: 0.00008743
    Epoch:  1300     LossContext: 0.00008739
    Epoch:  1310     LossContext: 0.00008736
    Epoch:  1320     LossContext: 0.00008732
    Epoch:  1330     LossContext: 0.00008729
    Epoch:  1340     LossContext: 0.00008725
    Epoch:  1350     LossContext: 0.00008721
    Epoch:  1360     LossContext: 0.00008718
    Epoch:  1370     LossContext: 0.00008714
    Epoch:  1380     LossContext: 0.00008710
    Epoch:  1390     LossContext: 0.00008706
    Epoch:  1400     LossContext: 0.00008703
    Epoch:  1410     LossContext: 0.00008699
    Epoch:  1420     LossContext: 0.00008695
    Epoch:  1430     LossContext: 0.00008691
    Epoch:  1440     LossContext: 0.00008687
    Epoch:  1450     LossContext: 0.00008683
    Epoch:  1460     LossContext: 0.00008679
    Epoch:  1470     LossContext: 0.00008675
    Epoch:  1480     LossContext: 0.00008671
    Epoch:  1490     LossContext: 0.00008667
    Epoch:  1499     LossContext: 0.00008664

Gradient descent adaptation time: 0 hours 1 mins 27 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.04496571
    Epoch:     1     LossContext: 0.03563555
    Epoch:     2     LossContext: 0.02756223
    Epoch:     3     LossContext: 0.02074715
    Epoch:    10     LossContext: 0.00084668
    Epoch:    20     LossContext: 0.00118734
    Epoch:    30     LossContext: 0.00142810
    Epoch:    40     LossContext: 0.00044035
    Epoch:    50     LossContext: 0.00011440
    Epoch:    60     LossContext: 0.00017422
    Epoch:    70     LossContext: 0.00011948
    Epoch:    80     LossContext: 0.00011392
    Epoch:    90     LossContext: 0.00011370
    Epoch:   100     LossContext: 0.00011072
    Epoch:   110     LossContext: 0.00011097
    Epoch:   120     LossContext: 0.00011043
    Epoch:   130     LossContext: 0.00011031
    Epoch:   140     LossContext: 0.00011010
    Epoch:   150     LossContext: 0.00010993
    Epoch:   160     LossContext: 0.00010975
    Epoch:   170     LossContext: 0.00010961
    Epoch:   180     LossContext: 0.00010951
    Epoch:   190     LossContext: 0.00010941
    Epoch:   200     LossContext: 0.00010931
    Epoch:   210     LossContext: 0.00010921
    Epoch:   220     LossContext: 0.00010910
    Epoch:   230     LossContext: 0.00010900
    Epoch:   240     LossContext: 0.00010888
    Epoch:   250     LossContext: 0.00010877
    Epoch:   260     LossContext: 0.00010865
    Epoch:   270     LossContext: 0.00010854
    Epoch:   280     LossContext: 0.00010841
    Epoch:   290     LossContext: 0.00010829
    Epoch:   300     LossContext: 0.00010816
    Epoch:   310     LossContext: 0.00010804
    Epoch:   320     LossContext: 0.00010790
    Epoch:   330     LossContext: 0.00010777
    Epoch:   340     LossContext: 0.00010768
    Epoch:   350     LossContext: 0.00010761
    Epoch:   360     LossContext: 0.00010754
    Epoch:   370     LossContext: 0.00010747
    Epoch:   380     LossContext: 0.00010740
    Epoch:   390     LossContext: 0.00010733
    Epoch:   400     LossContext: 0.00010725
    Epoch:   410     LossContext: 0.00010717
    Epoch:   420     LossContext: 0.00010710
    Epoch:   430     LossContext: 0.00010702
    Epoch:   440     LossContext: 0.00010694
    Epoch:   450     LossContext: 0.00010686
    Epoch:   460     LossContext: 0.00010678
    Epoch:   470     LossContext: 0.00010669
    Epoch:   480     LossContext: 0.00010661
    Epoch:   490     LossContext: 0.00010652
    Epoch:   500     LossContext: 0.00010644
    Epoch:   510     LossContext: 0.00010635
    Epoch:   520     LossContext: 0.00010626
    Epoch:   530     LossContext: 0.00010617
    Epoch:   540     LossContext: 0.00010608
    Epoch:   550     LossContext: 0.00010599
    Epoch:   560     LossContext: 0.00010590
    Epoch:   570     LossContext: 0.00010581
    Epoch:   580     LossContext: 0.00010573
    Epoch:   590     LossContext: 0.00010565
    Epoch:   600     LossContext: 0.00010557
    Epoch:   610     LossContext: 0.00010549
    Epoch:   620     LossContext: 0.00010541
    Epoch:   630     LossContext: 0.00010533
    Epoch:   640     LossContext: 0.00010524
    Epoch:   650     LossContext: 0.00010516
    Epoch:   660     LossContext: 0.00010508
    Epoch:   670     LossContext: 0.00010499
    Epoch:   680     LossContext: 0.00010493
    Epoch:   690     LossContext: 0.00010486
    Epoch:   700     LossContext: 0.00010480
    Epoch:   710     LossContext: 0.00010473
    Epoch:   720     LossContext: 0.00010467
    Epoch:   730     LossContext: 0.00010460
    Epoch:   740     LossContext: 0.00010454
    Epoch:   750     LossContext: 0.00010447
    Epoch:   760     LossContext: 0.00010440
    Epoch:   770     LossContext: 0.00010433
    Epoch:   780     LossContext: 0.00010426
    Epoch:   790     LossContext: 0.00010419
    Epoch:   800     LossContext: 0.00010412
    Epoch:   810     LossContext: 0.00010405
    Epoch:   820     LossContext: 0.00010398
    Epoch:   830     LossContext: 0.00010391
    Epoch:   840     LossContext: 0.00010383
    Epoch:   850     LossContext: 0.00010376
    Epoch:   860     LossContext: 0.00010369
    Epoch:   870     LossContext: 0.00010361
    Epoch:   880     LossContext: 0.00010354
    Epoch:   890     LossContext: 0.00010348
    Epoch:   900     LossContext: 0.00010341
    Epoch:   910     LossContext: 0.00010334
    Epoch:   920     LossContext: 0.00010327
    Epoch:   930     LossContext: 0.00010320
    Epoch:   940     LossContext: 0.00010312
    Epoch:   950     LossContext: 0.00010305
    Epoch:   960     LossContext: 0.00010298
    Epoch:   970     LossContext: 0.00010291
    Epoch:   980     LossContext: 0.00010283
    Epoch:   990     LossContext: 0.00010276
    Epoch:  1000     LossContext: 0.00010268
    Epoch:  1010     LossContext: 0.00010261
    Epoch:  1020     LossContext: 0.00010253
    Epoch:  1030     LossContext: 0.00010246
    Epoch:  1040     LossContext: 0.00010238
    Epoch:  1050     LossContext: 0.00010231
    Epoch:  1060     LossContext: 0.00010224
    Epoch:  1070     LossContext: 0.00010217
    Epoch:  1080     LossContext: 0.00010209
    Epoch:  1090     LossContext: 0.00010202
    Epoch:  1100     LossContext: 0.00010195
    Epoch:  1110     LossContext: 0.00010187
    Epoch:  1120     LossContext: 0.00010180
    Epoch:  1130     LossContext: 0.00010172
    Epoch:  1140     LossContext: 0.00010165
    Epoch:  1150     LossContext: 0.00010157
    Epoch:  1160     LossContext: 0.00010149
    Epoch:  1170     LossContext: 0.00010141
    Epoch:  1180     LossContext: 0.00010134
    Epoch:  1190     LossContext: 0.00010126
    Epoch:  1200     LossContext: 0.00010118
    Epoch:  1210     LossContext: 0.00010110
    Epoch:  1220     LossContext: 0.00010102
    Epoch:  1230     LossContext: 0.00010094
    Epoch:  1240     LossContext: 0.00010085
    Epoch:  1250     LossContext: 0.00010077
    Epoch:  1260     LossContext: 0.00010069
    Epoch:  1270     LossContext: 0.00010061
    Epoch:  1280     LossContext: 0.00010053
    Epoch:  1290     LossContext: 0.00010045
    Epoch:  1300     LossContext: 0.00010037
    Epoch:  1310     LossContext: 0.00010030
    Epoch:  1320     LossContext: 0.00010022
    Epoch:  1330     LossContext: 0.00010015
    Epoch:  1340     LossContext: 0.00010008
    Epoch:  1350     LossContext: 0.00010003
    Epoch:  1360     LossContext: 0.00009998
    Epoch:  1370     LossContext: 0.00009993
    Epoch:  1380     LossContext: 0.00009989
    Epoch:  1390     LossContext: 0.00009984
    Epoch:  1400     LossContext: 0.00009979
    Epoch:  1410     LossContext: 0.00009974
    Epoch:  1420     LossContext: 0.00009969
    Epoch:  1430     LossContext: 0.00009964
    Epoch:  1440     LossContext: 0.00009959
    Epoch:  1450     LossContext: 0.00009954
    Epoch:  1460     LossContext: 0.00009949
    Epoch:  1470     LossContext: 0.00009944
    Epoch:  1480     LossContext: 0.00009939
    Epoch:  1490     LossContext: 0.00009934
    Epoch:  1499     LossContext: 0.00009929

Gradient descent adaptation time: 0 hours 1 mins 27 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.02296135
    Epoch:     1     LossContext: 0.01691890
    Epoch:     2     LossContext: 0.01221666
    Epoch:     3     LossContext: 0.00874690
    Epoch:    10     LossContext: 0.00105823
    Epoch:    20     LossContext: 0.00034152
    Epoch:    30     LossContext: 0.00045386
    Epoch:    40     LossContext: 0.00032384
    Epoch:    50     LossContext: 0.00015748
    Epoch:    60     LossContext: 0.00010110
    Epoch:    70     LossContext: 0.00010384
    Epoch:    80     LossContext: 0.00010268
    Epoch:    90     LossContext: 0.00009907
    Epoch:   100     LossContext: 0.00009869
    Epoch:   110     LossContext: 0.00009870
    Epoch:   120     LossContext: 0.00009848
    Epoch:   130     LossContext: 0.00009839
    Epoch:   140     LossContext: 0.00009832
    Epoch:   150     LossContext: 0.00009823
    Epoch:   160     LossContext: 0.00009815
    Epoch:   170     LossContext: 0.00009808
    Epoch:   180     LossContext: 0.00009804
    Epoch:   190     LossContext: 0.00009799
    Epoch:   200     LossContext: 0.00009795
    Epoch:   210     LossContext: 0.00009790
    Epoch:   220     LossContext: 0.00009785
    Epoch:   230     LossContext: 0.00009780
    Epoch:   240     LossContext: 0.00009775
    Epoch:   250     LossContext: 0.00009770
    Epoch:   260     LossContext: 0.00009765
    Epoch:   270     LossContext: 0.00009759
    Epoch:   280     LossContext: 0.00009754
    Epoch:   290     LossContext: 0.00009748
    Epoch:   300     LossContext: 0.00009742
    Epoch:   310     LossContext: 0.00009736
    Epoch:   320     LossContext: 0.00009731
    Epoch:   330     LossContext: 0.00009725
    Epoch:   340     LossContext: 0.00009721
    Epoch:   350     LossContext: 0.00009717
    Epoch:   360     LossContext: 0.00009714
    Epoch:   370     LossContext: 0.00009711
    Epoch:   380     LossContext: 0.00009708
    Epoch:   390     LossContext: 0.00009704
    Epoch:   400     LossContext: 0.00009701
    Epoch:   410     LossContext: 0.00009698
    Epoch:   420     LossContext: 0.00009694
    Epoch:   430     LossContext: 0.00009691
    Epoch:   440     LossContext: 0.00009687
    Epoch:   450     LossContext: 0.00009683
    Epoch:   460     LossContext: 0.00009680
    Epoch:   470     LossContext: 0.00009676
    Epoch:   480     LossContext: 0.00009672
    Epoch:   490     LossContext: 0.00009668
    Epoch:   500     LossContext: 0.00009665
    Epoch:   510     LossContext: 0.00009661
    Epoch:   520     LossContext: 0.00009657
    Epoch:   530     LossContext: 0.00009653
    Epoch:   540     LossContext: 0.00009649
    Epoch:   550     LossContext: 0.00009645
    Epoch:   560     LossContext: 0.00009641
    Epoch:   570     LossContext: 0.00009636
    Epoch:   580     LossContext: 0.00009632
    Epoch:   590     LossContext: 0.00009628
    Epoch:   600     LossContext: 0.00009624
    Epoch:   610     LossContext: 0.00009619
    Epoch:   620     LossContext: 0.00009615
    Epoch:   630     LossContext: 0.00009610
    Epoch:   640     LossContext: 0.00009606
    Epoch:   650     LossContext: 0.00009601
    Epoch:   660     LossContext: 0.00009597
    Epoch:   670     LossContext: 0.00009593
    Epoch:   680     LossContext: 0.00009589
    Epoch:   690     LossContext: 0.00009584
    Epoch:   700     LossContext: 0.00009580
    Epoch:   710     LossContext: 0.00009576
    Epoch:   720     LossContext: 0.00009572
    Epoch:   730     LossContext: 0.00009567
    Epoch:   740     LossContext: 0.00009563
    Epoch:   750     LossContext: 0.00009558
    Epoch:   760     LossContext: 0.00009554
    Epoch:   770     LossContext: 0.00009550
    Epoch:   780     LossContext: 0.00009545
    Epoch:   790     LossContext: 0.00009540
    Epoch:   800     LossContext: 0.00009536
    Epoch:   810     LossContext: 0.00009533
    Epoch:   820     LossContext: 0.00009531
    Epoch:   830     LossContext: 0.00009528
    Epoch:   840     LossContext: 0.00009525
    Epoch:   850     LossContext: 0.00009522
    Epoch:   860     LossContext: 0.00009520
    Epoch:   870     LossContext: 0.00009517
    Epoch:   880     LossContext: 0.00009514
    Epoch:   890     LossContext: 0.00009511
    Epoch:   900     LossContext: 0.00009508
    Epoch:   910     LossContext: 0.00009506
    Epoch:   920     LossContext: 0.00009503
    Epoch:   930     LossContext: 0.00009500
    Epoch:   940     LossContext: 0.00009497
    Epoch:   950     LossContext: 0.00009494
    Epoch:   960     LossContext: 0.00009491
    Epoch:   970     LossContext: 0.00009488
    Epoch:   980     LossContext: 0.00009485
    Epoch:   990     LossContext: 0.00009482
    Epoch:  1000     LossContext: 0.00009479
    Epoch:  1010     LossContext: 0.00009476
    Epoch:  1020     LossContext: 0.00009473
    Epoch:  1030     LossContext: 0.00009470
    Epoch:  1040     LossContext: 0.00009466
    Epoch:  1050     LossContext: 0.00009463
    Epoch:  1060     LossContext: 0.00009460
    Epoch:  1070     LossContext: 0.00009457
    Epoch:  1080     LossContext: 0.00009454
    Epoch:  1090     LossContext: 0.00009451
    Epoch:  1100     LossContext: 0.00009447
    Epoch:  1110     LossContext: 0.00009444
    Epoch:  1120     LossContext: 0.00009441
    Epoch:  1130     LossContext: 0.00009437
    Epoch:  1140     LossContext: 0.00009434
    Epoch:  1150     LossContext: 0.00009431
    Epoch:  1160     LossContext: 0.00009427
    Epoch:  1170     LossContext: 0.00009424
    Epoch:  1180     LossContext: 0.00009421
    Epoch:  1190     LossContext: 0.00009417
    Epoch:  1200     LossContext: 0.00009414
    Epoch:  1210     LossContext: 0.00009410
    Epoch:  1220     LossContext: 0.00009407
    Epoch:  1230     LossContext: 0.00009403
    Epoch:  1240     LossContext: 0.00009400
    Epoch:  1250     LossContext: 0.00009396
    Epoch:  1260     LossContext: 0.00009393
    Epoch:  1270     LossContext: 0.00009390
    Epoch:  1280     LossContext: 0.00009387
    Epoch:  1290     LossContext: 0.00009385
    Epoch:  1300     LossContext: 0.00009382
    Epoch:  1310     LossContext: 0.00009380
    Epoch:  1320     LossContext: 0.00009377
    Epoch:  1330     LossContext: 0.00009375
    Epoch:  1340     LossContext: 0.00009372
    Epoch:  1350     LossContext: 0.00009370
    Epoch:  1360     LossContext: 0.00009367
    Epoch:  1370     LossContext: 0.00009365
    Epoch:  1380     LossContext: 0.00009362
    Epoch:  1390     LossContext: 0.00009359
    Epoch:  1400     LossContext: 0.00009357
    Epoch:  1410     LossContext: 0.00009354
    Epoch:  1420     LossContext: 0.00009352
    Epoch:  1430     LossContext: 0.00009349
    Epoch:  1440     LossContext: 0.00009347
    Epoch:  1450     LossContext: 0.00009344
    Epoch:  1460     LossContext: 0.00009342
    Epoch:  1470     LossContext: 0.00009339
    Epoch:  1480     LossContext: 0.00009337
    Epoch:  1490     LossContext: 0.00009335
    Epoch:  1499     LossContext: 0.00009332

Gradient descent adaptation time: 0 hours 1 mins 27 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./05052024-183119/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 0.000111383386

==  Begining out-of-distribution visualisation ... ==
    Environment id: 0
    Trajectory id: 0
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./05052024-183119/adapt/results_ood.png
Testing finished. Figure saved in: ./05052024-183119/adapt/results_ood_uq.pdf

======================= DONE ======================== 

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

========= STARTING =========
Working with a context size of 2^7 = 128
Run folder created successfuly: ./05052024-202925/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 202942
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 202942
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 78864 


WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed


=== Beginning training with proximal alternating minimization ... ===
    Number of examples in a batch: 4
    Maximum number of steps per inner minimization: 25
    Maximum number of outer minimizations: 500
    Maximum total number of training steps: 12500

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Outer Step:     0      LossTrajs: 1.81049669     ContextsNorm: 0.00000000     ValIndCrit: 1.69898713
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.72e-06
        -DiffCxt:  inf
    Outer Step:     1      LossTrajs: 1.52693844     ContextsNorm: 0.00052718     ValIndCrit: 1.43165767
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.63e-06
        -DiffCxt:  7.11e-04
    Outer Step:     2      LossTrajs: 1.22199178     ContextsNorm: 0.00157289     ValIndCrit: 1.14370763
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.36e-06
        -DiffCxt:  3.57e-04
    Outer Step:     3      LossTrajs: 0.81440824     ContextsNorm: 0.00401465     ValIndCrit: 0.76263070
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.28e-06
        -DiffCxt:  4.01e-04
    Outer Step:    10      LossTrajs: 0.23275667     ContextsNorm: 0.01505202     ValIndCrit: 0.27561563
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.44e-06
        -DiffCxt:  1.65e-05
    Outer Step:    20      LossTrajs: 0.16544230     ContextsNorm: 0.01615356     ValIndCrit: 0.18978858
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.95e-07
        -DiffCxt:  1.63e-07
    Outer Step:    30      LossTrajs: 0.07613123     ContextsNorm: 0.01578039     ValIndCrit: 0.07819942
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.75e-07
        -DiffCxt:  8.60e-07
    Outer Step:    40      LossTrajs: 0.02997250     ContextsNorm: 0.02196190     ValIndCrit: 0.03116282
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.49e-07
        -DiffCxt:  6.61e-07
    Outer Step:    50      LossTrajs: 0.01299337     ContextsNorm: 0.02513473     ValIndCrit: 0.01537144
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.50e-08
        -DiffCxt:  1.64e-07
    Outer Step:    60      LossTrajs: 0.00874501     ContextsNorm: 0.02511735     ValIndCrit: 0.01110594
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.98e-08
        -DiffCxt:  3.32e-08
    Outer Step:    70      LossTrajs: 0.00632897     ContextsNorm: 0.02553423     ValIndCrit: 0.00865119
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.33e-08
        -DiffCxt:  1.86e-07
    Outer Step:    80      LossTrajs: 0.00483066     ContextsNorm: 0.02555767     ValIndCrit: 0.00725127
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.96e-08
        -DiffCxt:  3.07e-08
    Outer Step:    90      LossTrajs: 0.00384375     ContextsNorm: 0.02560178     ValIndCrit: 0.00598249
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.48e-08
        -DiffCxt:  2.59e-08
    Outer Step:   100      LossTrajs: 0.00287306     ContextsNorm: 0.02573512     ValIndCrit: 0.00467073
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.51e-08
        -DiffCxt:  6.82e-08
    Outer Step:   110      LossTrajs: 0.00201242     ContextsNorm: 0.02587816     ValIndCrit: 0.00351329
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.69e-09
        -DiffCxt:  7.39e-08
    Outer Step:   120      LossTrajs: 0.00146735     ContextsNorm: 0.02578144     ValIndCrit: 0.00271766
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.06e-08
        -DiffCxt:  3.63e-08
    Outer Step:   130      LossTrajs: 0.00108955     ContextsNorm: 0.02606074     ValIndCrit: 0.00210665
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.69e-08
        -DiffCxt:  6.45e-08
    Outer Step:   140      LossTrajs: 0.00084861     ContextsNorm: 0.02586349     ValIndCrit: 0.00165182
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.91e-08
        -DiffCxt:  4.03e-08
    Outer Step:   150      LossTrajs: 0.00064714     ContextsNorm: 0.02583998     ValIndCrit: 0.00127335
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.43e-08
        -DiffCxt:  3.15e-08
    Outer Step:   160      LossTrajs: 0.00049669     ContextsNorm: 0.02581313     ValIndCrit: 0.00101553
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.70e-09
        -DiffCxt:  1.25e-08
    Outer Step:   170      LossTrajs: 0.00042257     ContextsNorm: 0.02570177     ValIndCrit: 0.00080596
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.91e-08
        -DiffCxt:  3.65e-08
    Outer Step:   180      LossTrajs: 0.00032805     ContextsNorm: 0.02569322     ValIndCrit: 0.00061337
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.87e-09
        -DiffCxt:  1.12e-08
    Outer Step:   190      LossTrajs: 0.00024598     ContextsNorm: 0.02556759     ValIndCrit: 0.00047909
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.09e-09
        -DiffCxt:  2.30e-08
    Outer Step:   200      LossTrajs: 0.00020196     ContextsNorm: 0.02536015     ValIndCrit: 0.00038026
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.02e-09
        -DiffCxt:  2.07e-08
    Outer Step:   210      LossTrajs: 0.00016474     ContextsNorm: 0.02538880     ValIndCrit: 0.00031059
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.36e-08
        -DiffCxt:  2.92e-08
    Outer Step:   220      LossTrajs: 0.00013824     ContextsNorm: 0.02537048     ValIndCrit: 0.00026426
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.57e-09
        -DiffCxt:  3.12e-08
    Outer Step:   230      LossTrajs: 0.00011979     ContextsNorm: 0.02530986     ValIndCrit: 0.00022375
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.25e-09
        -DiffCxt:  4.88e-08
    Outer Step:   240      LossTrajs: 0.00010084     ContextsNorm: 0.02531789     ValIndCrit: 0.00019902
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.51e-09
        -DiffCxt:  9.34e-08
    Outer Step:   250      LossTrajs: 0.00009473     ContextsNorm: 0.02524289     ValIndCrit: 0.00017523
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.60e-08
        -DiffCxt:  1.44e-08
    Outer Step:   260      LossTrajs: 0.00008406     ContextsNorm: 0.02522918     ValIndCrit: 0.00016052
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.65e-09
        -DiffCxt:  2.83e-08
    Outer Step:   270      LossTrajs: 0.00007723     ContextsNorm: 0.02520430     ValIndCrit: 0.00014530
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.83e-09
        -DiffCxt:  4.50e-08
    Outer Step:   280      LossTrajs: 0.00007138     ContextsNorm: 0.02516812     ValIndCrit: 0.00013773
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.26e-09
        -DiffCxt:  1.02e-07
    Outer Step:   290      LossTrajs: 0.00006585     ContextsNorm: 0.02516495     ValIndCrit: 0.00012731
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.47e-09
        -DiffCxt:  3.15e-08
    Outer Step:   300      LossTrajs: 0.00006575     ContextsNorm: 0.02521844     ValIndCrit: 0.00012380
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.70e-08
        -DiffCxt:  1.92e-07
    Outer Step:   310      LossTrajs: 0.00006331     ContextsNorm: 0.02521079     ValIndCrit: 0.00011175
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.97e-09
        -DiffCxt:  7.96e-08
    Outer Step:   320      LossTrajs: 0.00006334     ContextsNorm: 0.02515949     ValIndCrit: 0.00010675
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.76e-08
        -DiffCxt:  1.48e-07
    Outer Step:   330      LossTrajs: 0.00005680     ContextsNorm: 0.02511424     ValIndCrit: 0.00010248
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.34e-09
        -DiffCxt:  2.68e-08
    Outer Step:   340      LossTrajs: 0.00005390     ContextsNorm: 0.02507062     ValIndCrit: 0.00009795
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.09e-09
        -DiffCxt:  6.36e-08
    Outer Step:   350      LossTrajs: 0.00005451     ContextsNorm: 0.02505418     ValIndCrit: 0.00009399
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.91e-09
        -DiffCxt:  5.88e-08
    Outer Step:   360      LossTrajs: 0.00005312     ContextsNorm: 0.02494068     ValIndCrit: 0.00009241
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.50e-08
        -DiffCxt:  8.82e-08
    Outer Step:   370      LossTrajs: 0.00005015     ContextsNorm: 0.02490492     ValIndCrit: 0.00008610
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.04e-09
        -DiffCxt:  1.83e-07
    Outer Step:   380      LossTrajs: 0.00005046     ContextsNorm: 0.02493661     ValIndCrit: 0.00008333
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.29e-08
        -DiffCxt:  9.30e-08
    Outer Step:   390      LossTrajs: 0.00005106     ContextsNorm: 0.02487967     ValIndCrit: 0.00007974
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.54e-09
        -DiffCxt:  2.07e-08
    Outer Step:   400      LossTrajs: 0.00005190     ContextsNorm: 0.02492472     ValIndCrit: 0.00007536
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.05e-08
        -DiffCxt:  7.02e-08
    Outer Step:   410      LossTrajs: 0.00004651     ContextsNorm: 0.02487417     ValIndCrit: 0.00007512
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.50e-09
        -DiffCxt:  1.30e-07
    Outer Step:   420      LossTrajs: 0.00004629     ContextsNorm: 0.02481179     ValIndCrit: 0.00007531
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.90e-09
        -DiffCxt:  1.06e-07
    Outer Step:   430      LossTrajs: 0.00004536     ContextsNorm: 0.02477480     ValIndCrit: 0.00007119
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.53e-09
        -DiffCxt:  2.71e-08
    Outer Step:   440      LossTrajs: 0.00004701     ContextsNorm: 0.02481657     ValIndCrit: 0.00006833
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.72e-08
        -DiffCxt:  6.87e-08
    Outer Step:   450      LossTrajs: 0.00004485     ContextsNorm: 0.02473502     ValIndCrit: 0.00006856
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.05e-09
        -DiffCxt:  8.41e-08
    Outer Step:   460      LossTrajs: 0.00004894     ContextsNorm: 0.02470895     ValIndCrit: 0.00006698
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.92e-08
        -DiffCxt:  1.98e-07
    Outer Step:   470      LossTrajs: 0.00004695     ContextsNorm: 0.02465576     ValIndCrit: 0.00006206
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.89e-09
        -DiffCxt:  1.83e-08
    Outer Step:   480      LossTrajs: 0.00004421     ContextsNorm: 0.02461733     ValIndCrit: 0.00006142
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.72e-09
        -DiffCxt:  4.24e-08
    Outer Step:   490      LossTrajs: 0.00004173     ContextsNorm: 0.02455260     ValIndCrit: 0.00005910
        Saving best model so far ...
        -NbInnerStepsNode:    3
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.65e-10
        -DiffCxt:  1.21e-08
    Outer Step:   499      LossTrajs: 0.00004195     ContextsNorm: 0.02448692     ValIndCrit: 0.00005757
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.55e-09
        -DiffCxt:  3.95e-08

Total gradient descent training time: 2 hours 2 mins 15 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 223158
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 5.75747e-05


############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]
==  Begining in-domain visualisation ... ==
    Environment id: 0
    Trajectory id: 5
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./05052024-202925/results_in_domain.png
Testing finished. Figure saved in: ./05052024-202925/results_in_domain_uq.pdf
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.04372479
    Epoch:     1     LossContext: 0.03211176
    Epoch:     2     LossContext: 0.02088051
    Epoch:     3     LossContext: 0.01116515
    Epoch:    10     LossContext: 0.00635961
    Epoch:    20     LossContext: 0.00260820
    Epoch:    30     LossContext: 0.00097509
    Epoch:    40     LossContext: 0.00038191
    Epoch:    50     LossContext: 0.00016445
    Epoch:    60     LossContext: 0.00007822
    Epoch:    70     LossContext: 0.00004272
    Epoch:    80     LossContext: 0.00003423
    Epoch:    90     LossContext: 0.00003510
    Epoch:   100     LossContext: 0.00003499
    Epoch:   110     LossContext: 0.00003422
    Epoch:   120     LossContext: 0.00003413
    Epoch:   130     LossContext: 0.00003413
    Epoch:   140     LossContext: 0.00003409
    Epoch:   150     LossContext: 0.00003408
    Epoch:   160     LossContext: 0.00003407
    Epoch:   170     LossContext: 0.00003406
    Epoch:   180     LossContext: 0.00003406
    Epoch:   190     LossContext: 0.00003406
    Epoch:   200     LossContext: 0.00003405
    Epoch:   210     LossContext: 0.00003405
    Epoch:   220     LossContext: 0.00003404
    Epoch:   230     LossContext: 0.00003404
    Epoch:   240     LossContext: 0.00003404
    Epoch:   250     LossContext: 0.00003403
    Epoch:   260     LossContext: 0.00003403
    Epoch:   270     LossContext: 0.00003402
    Epoch:   280     LossContext: 0.00003402
    Epoch:   290     LossContext: 0.00003401
    Epoch:   300     LossContext: 0.00003401
    Epoch:   310     LossContext: 0.00003400
    Epoch:   320     LossContext: 0.00003399
    Epoch:   330     LossContext: 0.00003399
    Epoch:   340     LossContext: 0.00003399
    Epoch:   350     LossContext: 0.00003398
    Epoch:   360     LossContext: 0.00003398
    Epoch:   370     LossContext: 0.00003398
    Epoch:   380     LossContext: 0.00003397
    Epoch:   390     LossContext: 0.00003397
    Epoch:   400     LossContext: 0.00003397
    Epoch:   410     LossContext: 0.00003397
    Epoch:   420     LossContext: 0.00003396
    Epoch:   430     LossContext: 0.00003396
    Epoch:   440     LossContext: 0.00003396
    Epoch:   450     LossContext: 0.00003395
    Epoch:   460     LossContext: 0.00003395
    Epoch:   470     LossContext: 0.00003395
    Epoch:   480     LossContext: 0.00003394
    Epoch:   490     LossContext: 0.00003394
    Epoch:   500     LossContext: 0.00003394
    Epoch:   510     LossContext: 0.00003393
    Epoch:   520     LossContext: 0.00003393
    Epoch:   530     LossContext: 0.00003392
    Epoch:   540     LossContext: 0.00003392
    Epoch:   550     LossContext: 0.00003392
    Epoch:   560     LossContext: 0.00003391
    Epoch:   570     LossContext: 0.00003391
    Epoch:   580     LossContext: 0.00003391
    Epoch:   590     LossContext: 0.00003390
    Epoch:   600     LossContext: 0.00003390
    Epoch:   610     LossContext: 0.00003389
    Epoch:   620     LossContext: 0.00003389
    Epoch:   630     LossContext: 0.00003389
    Epoch:   640     LossContext: 0.00003389
    Epoch:   650     LossContext: 0.00003388
    Epoch:   660     LossContext: 0.00003388
    Epoch:   670     LossContext: 0.00003388
    Epoch:   680     LossContext: 0.00003388
    Epoch:   690     LossContext: 0.00003387
    Epoch:   700     LossContext: 0.00003387
    Epoch:   710     LossContext: 0.00003387
    Epoch:   720     LossContext: 0.00003387
    Epoch:   730     LossContext: 0.00003386
    Epoch:   740     LossContext: 0.00003386
    Epoch:   750     LossContext: 0.00003386
    Epoch:   760     LossContext: 0.00003386
    Epoch:   770     LossContext: 0.00003386
    Epoch:   780     LossContext: 0.00003385
    Epoch:   790     LossContext: 0.00003385
    Epoch:   800     LossContext: 0.00003385
    Epoch:   810     LossContext: 0.00003385
    Epoch:   820     LossContext: 0.00003384
    Epoch:   830     LossContext: 0.00003384
    Epoch:   840     LossContext: 0.00003384
    Epoch:   850     LossContext: 0.00003383
    Epoch:   860     LossContext: 0.00003383
    Epoch:   870     LossContext: 0.00003383
    Epoch:   880     LossContext: 0.00003383
    Epoch:   890     LossContext: 0.00003382
    Epoch:   900     LossContext: 0.00003382
    Epoch:   910     LossContext: 0.00003382
    Epoch:   920     LossContext: 0.00003382
    Epoch:   930     LossContext: 0.00003381
    Epoch:   940     LossContext: 0.00003381
    Epoch:   950     LossContext: 0.00003381
    Epoch:   960     LossContext: 0.00003381
    Epoch:   970     LossContext: 0.00003380
    Epoch:   980     LossContext: 0.00003380
    Epoch:   990     LossContext: 0.00003380
    Epoch:  1000     LossContext: 0.00003379
    Epoch:  1010     LossContext: 0.00003379
    Epoch:  1020     LossContext: 0.00003379
    Epoch:  1030     LossContext: 0.00003378
    Epoch:  1040     LossContext: 0.00003378
    Epoch:  1050     LossContext: 0.00003378
    Epoch:  1060     LossContext: 0.00003378
    Epoch:  1070     LossContext: 0.00003377
    Epoch:  1080     LossContext: 0.00003377
    Epoch:  1090     LossContext: 0.00003377
    Epoch:  1100     LossContext: 0.00003376
    Epoch:  1110     LossContext: 0.00003376
    Epoch:  1120     LossContext: 0.00003376
    Epoch:  1130     LossContext: 0.00003375
    Epoch:  1140     LossContext: 0.00003375
    Epoch:  1150     LossContext: 0.00003375
    Epoch:  1160     LossContext: 0.00003374
    Epoch:  1170     LossContext: 0.00003374
    Epoch:  1180     LossContext: 0.00003374
    Epoch:  1190     LossContext: 0.00003373
    Epoch:  1200     LossContext: 0.00003373
    Epoch:  1210     LossContext: 0.00003373
    Epoch:  1220     LossContext: 0.00003372
    Epoch:  1230     LossContext: 0.00003372
    Epoch:  1240     LossContext: 0.00003372
    Epoch:  1250     LossContext: 0.00003371
    Epoch:  1260     LossContext: 0.00003371
    Epoch:  1270     LossContext: 0.00003371
    Epoch:  1280     LossContext: 0.00003370
    Epoch:  1290     LossContext: 0.00003370
    Epoch:  1300     LossContext: 0.00003369
    Epoch:  1310     LossContext: 0.00003369
    Epoch:  1320     LossContext: 0.00003369
    Epoch:  1330     LossContext: 0.00003368
    Epoch:  1340     LossContext: 0.00003368
    Epoch:  1350     LossContext: 0.00003368
    Epoch:  1360     LossContext: 0.00003367
    Epoch:  1370     LossContext: 0.00003367
    Epoch:  1380     LossContext: 0.00003366
    Epoch:  1390     LossContext: 0.00003366
    Epoch:  1400     LossContext: 0.00003366
    Epoch:  1410     LossContext: 0.00003365
    Epoch:  1420     LossContext: 0.00003365
    Epoch:  1430     LossContext: 0.00003365
    Epoch:  1440     LossContext: 0.00003364
    Epoch:  1450     LossContext: 0.00003364
    Epoch:  1460     LossContext: 0.00003363
    Epoch:  1470     LossContext: 0.00003363
    Epoch:  1480     LossContext: 0.00003363
    Epoch:  1490     LossContext: 0.00003362
    Epoch:  1499     LossContext: 0.00003362

Gradient descent adaptation time: 0 hours 1 mins 47 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.13669094
    Epoch:     1     LossContext: 0.11758873
    Epoch:     2     LossContext: 0.09799155
    Epoch:     3     LossContext: 0.07818487
    Epoch:    10     LossContext: 0.00615476
    Epoch:    20     LossContext: 0.00553145
    Epoch:    30     LossContext: 0.00210179
    Epoch:    40     LossContext: 0.00110356
    Epoch:    50     LossContext: 0.00053270
    Epoch:    60     LossContext: 0.00025255
    Epoch:    70     LossContext: 0.00013049
    Epoch:    80     LossContext: 0.00010253
    Epoch:    90     LossContext: 0.00008305
    Epoch:   100     LossContext: 0.00007996
    Epoch:   110     LossContext: 0.00007750
    Epoch:   120     LossContext: 0.00007677
    Epoch:   130     LossContext: 0.00007668
    Epoch:   140     LossContext: 0.00007660
    Epoch:   150     LossContext: 0.00007656
    Epoch:   160     LossContext: 0.00007651
    Epoch:   170     LossContext: 0.00007648
    Epoch:   180     LossContext: 0.00007645
    Epoch:   190     LossContext: 0.00007643
    Epoch:   200     LossContext: 0.00007641
    Epoch:   210     LossContext: 0.00007639
    Epoch:   220     LossContext: 0.00007636
    Epoch:   230     LossContext: 0.00007634
    Epoch:   240     LossContext: 0.00007632
    Epoch:   250     LossContext: 0.00007629
    Epoch:   260     LossContext: 0.00007626
    Epoch:   270     LossContext: 0.00007624
    Epoch:   280     LossContext: 0.00007621
    Epoch:   290     LossContext: 0.00007618
    Epoch:   300     LossContext: 0.00007615
    Epoch:   310     LossContext: 0.00007613
    Epoch:   320     LossContext: 0.00007610
    Epoch:   330     LossContext: 0.00007607
    Epoch:   340     LossContext: 0.00007605
    Epoch:   350     LossContext: 0.00007603
    Epoch:   360     LossContext: 0.00007602
    Epoch:   370     LossContext: 0.00007600
    Epoch:   380     LossContext: 0.00007599
    Epoch:   390     LossContext: 0.00007597
    Epoch:   400     LossContext: 0.00007595
    Epoch:   410     LossContext: 0.00007594
    Epoch:   420     LossContext: 0.00007592
    Epoch:   430     LossContext: 0.00007590
    Epoch:   440     LossContext: 0.00007588
    Epoch:   450     LossContext: 0.00007587
    Epoch:   460     LossContext: 0.00007585
    Epoch:   470     LossContext: 0.00007583
    Epoch:   480     LossContext: 0.00007581
    Epoch:   490     LossContext: 0.00007579
    Epoch:   500     LossContext: 0.00007577
    Epoch:   510     LossContext: 0.00007575
    Epoch:   520     LossContext: 0.00007573
    Epoch:   530     LossContext: 0.00007571
    Epoch:   540     LossContext: 0.00007569
    Epoch:   550     LossContext: 0.00007567
    Epoch:   560     LossContext: 0.00007565
    Epoch:   570     LossContext: 0.00007563
    Epoch:   580     LossContext: 0.00007561
    Epoch:   590     LossContext: 0.00007559
    Epoch:   600     LossContext: 0.00007557
    Epoch:   610     LossContext: 0.00007555
    Epoch:   620     LossContext: 0.00007553
    Epoch:   630     LossContext: 0.00007550
    Epoch:   640     LossContext: 0.00007548
    Epoch:   650     LossContext: 0.00007546
    Epoch:   660     LossContext: 0.00007544
    Epoch:   670     LossContext: 0.00007541
    Epoch:   680     LossContext: 0.00007539
    Epoch:   690     LossContext: 0.00007537
    Epoch:   700     LossContext: 0.00007534
    Epoch:   710     LossContext: 0.00007532
    Epoch:   720     LossContext: 0.00007529
    Epoch:   730     LossContext: 0.00007527
    Epoch:   740     LossContext: 0.00007525
    Epoch:   750     LossContext: 0.00007522
    Epoch:   760     LossContext: 0.00007520
    Epoch:   770     LossContext: 0.00007517
    Epoch:   780     LossContext: 0.00007514
    Epoch:   790     LossContext: 0.00007512
    Epoch:   800     LossContext: 0.00007509
    Epoch:   810     LossContext: 0.00007507
    Epoch:   820     LossContext: 0.00007504
    Epoch:   830     LossContext: 0.00007501
    Epoch:   840     LossContext: 0.00007499
    Epoch:   850     LossContext: 0.00007496
    Epoch:   860     LossContext: 0.00007493
    Epoch:   870     LossContext: 0.00007490
    Epoch:   880     LossContext: 0.00007488
    Epoch:   890     LossContext: 0.00007485
    Epoch:   900     LossContext: 0.00007482
    Epoch:   910     LossContext: 0.00007479
    Epoch:   920     LossContext: 0.00007476
    Epoch:   930     LossContext: 0.00007473
    Epoch:   940     LossContext: 0.00007471
    Epoch:   950     LossContext: 0.00007468
    Epoch:   960     LossContext: 0.00007465
    Epoch:   970     LossContext: 0.00007462
    Epoch:   980     LossContext: 0.00007459
    Epoch:   990     LossContext: 0.00007456
    Epoch:  1000     LossContext: 0.00007453
    Epoch:  1010     LossContext: 0.00007449
    Epoch:  1020     LossContext: 0.00007446
    Epoch:  1030     LossContext: 0.00007443
    Epoch:  1040     LossContext: 0.00007440
    Epoch:  1050     LossContext: 0.00007437
    Epoch:  1060     LossContext: 0.00007434
    Epoch:  1070     LossContext: 0.00007430
    Epoch:  1080     LossContext: 0.00007427
    Epoch:  1090     LossContext: 0.00007424
    Epoch:  1100     LossContext: 0.00007421
    Epoch:  1110     LossContext: 0.00007417
    Epoch:  1120     LossContext: 0.00007414
    Epoch:  1130     LossContext: 0.00007411
    Epoch:  1140     LossContext: 0.00007408
    Epoch:  1150     LossContext: 0.00007404
    Epoch:  1160     LossContext: 0.00007401
    Epoch:  1170     LossContext: 0.00007398
    Epoch:  1180     LossContext: 0.00007394
    Epoch:  1190     LossContext: 0.00007391
    Epoch:  1200     LossContext: 0.00007387
    Epoch:  1210     LossContext: 0.00007384
    Epoch:  1220     LossContext: 0.00007381
    Epoch:  1230     LossContext: 0.00007377
    Epoch:  1240     LossContext: 0.00007374
    Epoch:  1250     LossContext: 0.00007371
    Epoch:  1260     LossContext: 0.00007368
    Epoch:  1270     LossContext: 0.00007364
    Epoch:  1280     LossContext: 0.00007361
    Epoch:  1290     LossContext: 0.00007358
    Epoch:  1300     LossContext: 0.00007354
    Epoch:  1310     LossContext: 0.00007351
    Epoch:  1320     LossContext: 0.00007347
    Epoch:  1330     LossContext: 0.00007344
    Epoch:  1340     LossContext: 0.00007341
    Epoch:  1350     LossContext: 0.00007337
    Epoch:  1360     LossContext: 0.00007334
    Epoch:  1370     LossContext: 0.00007330
    Epoch:  1380     LossContext: 0.00007326
    Epoch:  1390     LossContext: 0.00007323
    Epoch:  1400     LossContext: 0.00007319
    Epoch:  1410     LossContext: 0.00007316
    Epoch:  1420     LossContext: 0.00007312
    Epoch:  1430     LossContext: 0.00007308
    Epoch:  1440     LossContext: 0.00007305
    Epoch:  1450     LossContext: 0.00007301
    Epoch:  1460     LossContext: 0.00007297
    Epoch:  1470     LossContext: 0.00007294
    Epoch:  1480     LossContext: 0.00007290
    Epoch:  1490     LossContext: 0.00007286
    Epoch:  1499     LossContext: 0.00007282

Gradient descent adaptation time: 0 hours 1 mins 33 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.03142947
    Epoch:     1     LossContext: 0.02314062
    Epoch:     2     LossContext: 0.01628523
    Epoch:     3     LossContext: 0.01084572
    Epoch:    10     LossContext: 0.00015484
    Epoch:    20     LossContext: 0.00212447
    Epoch:    30     LossContext: 0.00057512
    Epoch:    40     LossContext: 0.00015282
    Epoch:    50     LossContext: 0.00016342
    Epoch:    60     LossContext: 0.00007947
    Epoch:    70     LossContext: 0.00008211
    Epoch:    80     LossContext: 0.00007150
    Epoch:    90     LossContext: 0.00007049
    Epoch:   100     LossContext: 0.00006975
    Epoch:   110     LossContext: 0.00006901
    Epoch:   120     LossContext: 0.00006882
    Epoch:   130     LossContext: 0.00006847
    Epoch:   140     LossContext: 0.00006819
    Epoch:   150     LossContext: 0.00006790
    Epoch:   160     LossContext: 0.00006760
    Epoch:   170     LossContext: 0.00006737
    Epoch:   180     LossContext: 0.00006722
    Epoch:   190     LossContext: 0.00006706
    Epoch:   200     LossContext: 0.00006690
    Epoch:   210     LossContext: 0.00006674
    Epoch:   220     LossContext: 0.00006657
    Epoch:   230     LossContext: 0.00006640
    Epoch:   240     LossContext: 0.00006623
    Epoch:   250     LossContext: 0.00006606
    Epoch:   260     LossContext: 0.00006588
    Epoch:   270     LossContext: 0.00006571
    Epoch:   280     LossContext: 0.00006553
    Epoch:   290     LossContext: 0.00006535
    Epoch:   300     LossContext: 0.00006518
    Epoch:   310     LossContext: 0.00006503
    Epoch:   320     LossContext: 0.00006489
    Epoch:   330     LossContext: 0.00006474
    Epoch:   340     LossContext: 0.00006465
    Epoch:   350     LossContext: 0.00006458
    Epoch:   360     LossContext: 0.00006451
    Epoch:   370     LossContext: 0.00006443
    Epoch:   380     LossContext: 0.00006436
    Epoch:   390     LossContext: 0.00006429
    Epoch:   400     LossContext: 0.00006421
    Epoch:   410     LossContext: 0.00006414
    Epoch:   420     LossContext: 0.00006407
    Epoch:   430     LossContext: 0.00006400
    Epoch:   440     LossContext: 0.00006393
    Epoch:   450     LossContext: 0.00006386
    Epoch:   460     LossContext: 0.00006379
    Epoch:   470     LossContext: 0.00006372
    Epoch:   480     LossContext: 0.00006365
    Epoch:   490     LossContext: 0.00006357
    Epoch:   500     LossContext: 0.00006350
    Epoch:   510     LossContext: 0.00006342
    Epoch:   520     LossContext: 0.00006334
    Epoch:   530     LossContext: 0.00006326
    Epoch:   540     LossContext: 0.00006318
    Epoch:   550     LossContext: 0.00006311
    Epoch:   560     LossContext: 0.00006303
    Epoch:   570     LossContext: 0.00006294
    Epoch:   580     LossContext: 0.00006286
    Epoch:   590     LossContext: 0.00006278
    Epoch:   600     LossContext: 0.00006269
    Epoch:   610     LossContext: 0.00006261
    Epoch:   620     LossContext: 0.00006252
    Epoch:   630     LossContext: 0.00006244
    Epoch:   640     LossContext: 0.00006235
    Epoch:   650     LossContext: 0.00006226
    Epoch:   660     LossContext: 0.00006218
    Epoch:   670     LossContext: 0.00006209
    Epoch:   680     LossContext: 0.00006200
    Epoch:   690     LossContext: 0.00006192
    Epoch:   700     LossContext: 0.00006183
    Epoch:   710     LossContext: 0.00006175
    Epoch:   720     LossContext: 0.00006166
    Epoch:   730     LossContext: 0.00006158
    Epoch:   740     LossContext: 0.00006149
    Epoch:   750     LossContext: 0.00006141
    Epoch:   760     LossContext: 0.00006133
    Epoch:   770     LossContext: 0.00006125
    Epoch:   780     LossContext: 0.00006116
    Epoch:   790     LossContext: 0.00006108
    Epoch:   800     LossContext: 0.00006100
    Epoch:   810     LossContext: 0.00006091
    Epoch:   820     LossContext: 0.00006083
    Epoch:   830     LossContext: 0.00006074
    Epoch:   840     LossContext: 0.00006065
    Epoch:   850     LossContext: 0.00006056
    Epoch:   860     LossContext: 0.00006048
    Epoch:   870     LossContext: 0.00006039
    Epoch:   880     LossContext: 0.00006030
    Epoch:   890     LossContext: 0.00006021
    Epoch:   900     LossContext: 0.00006013
    Epoch:   910     LossContext: 0.00006005
    Epoch:   920     LossContext: 0.00005997
    Epoch:   930     LossContext: 0.00005990
    Epoch:   940     LossContext: 0.00005983
    Epoch:   950     LossContext: 0.00005975
    Epoch:   960     LossContext: 0.00005968
    Epoch:   970     LossContext: 0.00005960
    Epoch:   980     LossContext: 0.00005953
    Epoch:   990     LossContext: 0.00005945
    Epoch:  1000     LossContext: 0.00005937
    Epoch:  1010     LossContext: 0.00005930
    Epoch:  1020     LossContext: 0.00005923
    Epoch:  1030     LossContext: 0.00005917
    Epoch:  1040     LossContext: 0.00005910
    Epoch:  1050     LossContext: 0.00005903
    Epoch:  1060     LossContext: 0.00005896
    Epoch:  1070     LossContext: 0.00005890
    Epoch:  1080     LossContext: 0.00005883
    Epoch:  1090     LossContext: 0.00005876
    Epoch:  1100     LossContext: 0.00005869
    Epoch:  1110     LossContext: 0.00005863
    Epoch:  1120     LossContext: 0.00005857
    Epoch:  1130     LossContext: 0.00005850
    Epoch:  1140     LossContext: 0.00005844
    Epoch:  1150     LossContext: 0.00005837
    Epoch:  1160     LossContext: 0.00005831
    Epoch:  1170     LossContext: 0.00005824
    Epoch:  1180     LossContext: 0.00005817
    Epoch:  1190     LossContext: 0.00005811
    Epoch:  1200     LossContext: 0.00005804
    Epoch:  1210     LossContext: 0.00005798
    Epoch:  1220     LossContext: 0.00005791
    Epoch:  1230     LossContext: 0.00005785
    Epoch:  1240     LossContext: 0.00005779
    Epoch:  1250     LossContext: 0.00005772
    Epoch:  1260     LossContext: 0.00005766
    Epoch:  1270     LossContext: 0.00005760
    Epoch:  1280     LossContext: 0.00005753
    Epoch:  1290     LossContext: 0.00005747
    Epoch:  1300     LossContext: 0.00005741
    Epoch:  1310     LossContext: 0.00005735
    Epoch:  1320     LossContext: 0.00005728
    Epoch:  1330     LossContext: 0.00005722
    Epoch:  1340     LossContext: 0.00005716
    Epoch:  1350     LossContext: 0.00005710
    Epoch:  1360     LossContext: 0.00005703
    Epoch:  1370     LossContext: 0.00005697
    Epoch:  1380     LossContext: 0.00005691
    Epoch:  1390     LossContext: 0.00005685
    Epoch:  1400     LossContext: 0.00005679
    Epoch:  1410     LossContext: 0.00005673
    Epoch:  1420     LossContext: 0.00005667
    Epoch:  1430     LossContext: 0.00005661
    Epoch:  1440     LossContext: 0.00005655
    Epoch:  1450     LossContext: 0.00005649
    Epoch:  1460     LossContext: 0.00005643
    Epoch:  1470     LossContext: 0.00005637
    Epoch:  1480     LossContext: 0.00005630
    Epoch:  1490     LossContext: 0.00005624
    Epoch:  1499     LossContext: 0.00005619

Gradient descent adaptation time: 0 hours 1 mins 32 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.01100984
    Epoch:     1     LossContext: 0.00660944
    Epoch:     2     LossContext: 0.00400624
    Epoch:     3     LossContext: 0.00248324
    Epoch:    10     LossContext: 0.00023065
    Epoch:    20     LossContext: 0.00075007
    Epoch:    30     LossContext: 0.00023086
    Epoch:    40     LossContext: 0.00008945
    Epoch:    50     LossContext: 0.00008786
    Epoch:    60     LossContext: 0.00006152
    Epoch:    70     LossContext: 0.00006235
    Epoch:    80     LossContext: 0.00005840
    Epoch:    90     LossContext: 0.00005818
    Epoch:   100     LossContext: 0.00005772
    Epoch:   110     LossContext: 0.00005741
    Epoch:   120     LossContext: 0.00005723
    Epoch:   130     LossContext: 0.00005701
    Epoch:   140     LossContext: 0.00005681
    Epoch:   150     LossContext: 0.00005661
    Epoch:   160     LossContext: 0.00005643
    Epoch:   170     LossContext: 0.00005629
    Epoch:   180     LossContext: 0.00005620
    Epoch:   190     LossContext: 0.00005610
    Epoch:   200     LossContext: 0.00005600
    Epoch:   210     LossContext: 0.00005591
    Epoch:   220     LossContext: 0.00005581
    Epoch:   230     LossContext: 0.00005570
    Epoch:   240     LossContext: 0.00005560
    Epoch:   250     LossContext: 0.00005550
    Epoch:   260     LossContext: 0.00005541
    Epoch:   270     LossContext: 0.00005532
    Epoch:   280     LossContext: 0.00005523
    Epoch:   290     LossContext: 0.00005514
    Epoch:   300     LossContext: 0.00005505
    Epoch:   310     LossContext: 0.00005496
    Epoch:   320     LossContext: 0.00005487
    Epoch:   330     LossContext: 0.00005478
    Epoch:   340     LossContext: 0.00005473
    Epoch:   350     LossContext: 0.00005470
    Epoch:   360     LossContext: 0.00005466
    Epoch:   370     LossContext: 0.00005462
    Epoch:   380     LossContext: 0.00005458
    Epoch:   390     LossContext: 0.00005454
    Epoch:   400     LossContext: 0.00005450
    Epoch:   410     LossContext: 0.00005446
    Epoch:   420     LossContext: 0.00005442
    Epoch:   430     LossContext: 0.00005438
    Epoch:   440     LossContext: 0.00005434
    Epoch:   450     LossContext: 0.00005430
    Epoch:   460     LossContext: 0.00005426
    Epoch:   470     LossContext: 0.00005423
    Epoch:   480     LossContext: 0.00005419
    Epoch:   490     LossContext: 0.00005415
    Epoch:   500     LossContext: 0.00005411
    Epoch:   510     LossContext: 0.00005407
    Epoch:   520     LossContext: 0.00005403
    Epoch:   530     LossContext: 0.00005400
    Epoch:   540     LossContext: 0.00005396
    Epoch:   550     LossContext: 0.00005392
    Epoch:   560     LossContext: 0.00005388
    Epoch:   570     LossContext: 0.00005384
    Epoch:   580     LossContext: 0.00005380
    Epoch:   590     LossContext: 0.00005376
    Epoch:   600     LossContext: 0.00005372
    Epoch:   610     LossContext: 0.00005368
    Epoch:   620     LossContext: 0.00005365
    Epoch:   630     LossContext: 0.00005361
    Epoch:   640     LossContext: 0.00005357
    Epoch:   650     LossContext: 0.00005354
    Epoch:   660     LossContext: 0.00005350
    Epoch:   670     LossContext: 0.00005346
    Epoch:   680     LossContext: 0.00005343
    Epoch:   690     LossContext: 0.00005339
    Epoch:   700     LossContext: 0.00005335
    Epoch:   710     LossContext: 0.00005332
    Epoch:   720     LossContext: 0.00005328
    Epoch:   730     LossContext: 0.00005325
    Epoch:   740     LossContext: 0.00005321
    Epoch:   750     LossContext: 0.00005318
    Epoch:   760     LossContext: 0.00005314
    Epoch:   770     LossContext: 0.00005311
    Epoch:   780     LossContext: 0.00005307
    Epoch:   790     LossContext: 0.00005304
    Epoch:   800     LossContext: 0.00005301
    Epoch:   810     LossContext: 0.00005299
    Epoch:   820     LossContext: 0.00005296
    Epoch:   830     LossContext: 0.00005293
    Epoch:   840     LossContext: 0.00005291
    Epoch:   850     LossContext: 0.00005288
    Epoch:   860     LossContext: 0.00005286
    Epoch:   870     LossContext: 0.00005283
    Epoch:   880     LossContext: 0.00005281
    Epoch:   890     LossContext: 0.00005278
    Epoch:   900     LossContext: 0.00005275
    Epoch:   910     LossContext: 0.00005273
    Epoch:   920     LossContext: 0.00005270
    Epoch:   930     LossContext: 0.00005268
    Epoch:   940     LossContext: 0.00005265
    Epoch:   950     LossContext: 0.00005263
    Epoch:   960     LossContext: 0.00005260
    Epoch:   970     LossContext: 0.00005258
    Epoch:   980     LossContext: 0.00005255
    Epoch:   990     LossContext: 0.00005253
    Epoch:  1000     LossContext: 0.00005250
    Epoch:  1010     LossContext: 0.00005248
    Epoch:  1020     LossContext: 0.00005245
    Epoch:  1030     LossContext: 0.00005243
    Epoch:  1040     LossContext: 0.00005240
    Epoch:  1050     LossContext: 0.00005238
    Epoch:  1060     LossContext: 0.00005236
    Epoch:  1070     LossContext: 0.00005233
    Epoch:  1080     LossContext: 0.00005231
    Epoch:  1090     LossContext: 0.00005229
    Epoch:  1100     LossContext: 0.00005226
    Epoch:  1110     LossContext: 0.00005224
    Epoch:  1120     LossContext: 0.00005222
    Epoch:  1130     LossContext: 0.00005219
    Epoch:  1140     LossContext: 0.00005217
    Epoch:  1150     LossContext: 0.00005215
    Epoch:  1160     LossContext: 0.00005212
    Epoch:  1170     LossContext: 0.00005210
    Epoch:  1180     LossContext: 0.00005208
    Epoch:  1190     LossContext: 0.00005206
    Epoch:  1200     LossContext: 0.00005203
    Epoch:  1210     LossContext: 0.00005201
    Epoch:  1220     LossContext: 0.00005199
    Epoch:  1230     LossContext: 0.00005197
    Epoch:  1240     LossContext: 0.00005195
    Epoch:  1250     LossContext: 0.00005193
    Epoch:  1260     LossContext: 0.00005191
    Epoch:  1270     LossContext: 0.00005189
    Epoch:  1280     LossContext: 0.00005187
    Epoch:  1290     LossContext: 0.00005185
    Epoch:  1300     LossContext: 0.00005183
    Epoch:  1310     LossContext: 0.00005181
    Epoch:  1320     LossContext: 0.00005179
    Epoch:  1330     LossContext: 0.00005177
    Epoch:  1340     LossContext: 0.00005176
    Epoch:  1350     LossContext: 0.00005174
    Epoch:  1360     LossContext: 0.00005172
    Epoch:  1370     LossContext: 0.00005171
    Epoch:  1380     LossContext: 0.00005169
    Epoch:  1390     LossContext: 0.00005167
    Epoch:  1400     LossContext: 0.00005166
    Epoch:  1410     LossContext: 0.00005164
    Epoch:  1420     LossContext: 0.00005163
    Epoch:  1430     LossContext: 0.00005161
    Epoch:  1440     LossContext: 0.00005160
    Epoch:  1450     LossContext: 0.00005159
    Epoch:  1460     LossContext: 0.00005157
    Epoch:  1470     LossContext: 0.00005156
    Epoch:  1480     LossContext: 0.00005154
    Epoch:  1490     LossContext: 0.00005153
    Epoch:  1499     LossContext: 0.00005152

Gradient descent adaptation time: 0 hours 1 mins 33 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./05052024-202925/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 7.257318e-05

==  Begining out-of-distribution visualisation ... ==
    Environment id: 0
    Trajectory id: 0
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./05052024-202925/adapt/results_ood.png
Testing finished. Figure saved in: ./05052024-202925/adapt/results_ood_uq.pdf

======================= DONE ======================== 

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

========= STARTING =========
Working with a context size of 2^8 = 256
Run folder created successfuly: ./05052024-223846/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 223904
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 223904
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 111632 


WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed


=== Beginning training with proximal alternating minimization ... ===
    Number of examples in a batch: 4
    Maximum number of steps per inner minimization: 25
    Maximum number of outer minimizations: 500
    Maximum total number of training steps: 12500

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Outer Step:     0      LossTrajs: 1.81050646     ContextsNorm: 0.00000000     ValIndCrit: 1.69901979
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.71e-06
        -DiffCxt:  inf
    Outer Step:     1      LossTrajs: 1.52754009     ContextsNorm: 0.00071076     ValIndCrit: 1.43227768
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.63e-06
        -DiffCxt:  4.69e-04
    Outer Step:     2      LossTrajs: 1.22411585     ContextsNorm: 0.00221209     ValIndCrit: 1.14566207
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.55e-06
        -DiffCxt:  4.33e-04
    Outer Step:     3      LossTrajs: 0.81972432     ContextsNorm: 0.00539661     ValIndCrit: 0.76623744
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.06e-06
        -DiffCxt:  4.65e-04
    Outer Step:    10      LossTrajs: 0.22989938     ContextsNorm: 0.01222761     ValIndCrit: 0.27279189
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.58e-07
        -DiffCxt:  9.66e-06
    Outer Step:    20      LossTrajs: 0.16534674     ContextsNorm: 0.01271351     ValIndCrit: 0.18976597
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.07e-07
        -DiffCxt:  2.15e-07
    Outer Step:    30      LossTrajs: 0.07527899     ContextsNorm: 0.01285324     ValIndCrit: 0.07719173
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.51e-07
        -DiffCxt:  3.66e-06
    Outer Step:    40      LossTrajs: 0.02633974     ContextsNorm: 0.01735050     ValIndCrit: 0.02780777
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.53e-07
        -DiffCxt:  4.20e-07
    Outer Step:    50      LossTrajs: 0.01133383     ContextsNorm: 0.01993488     ValIndCrit: 0.01380732
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.40e-08
        -DiffCxt:  6.22e-08
    Outer Step:    60      LossTrajs: 0.00765258     ContextsNorm: 0.02008055     ValIndCrit: 0.00998411
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.46e-08
        -DiffCxt:  1.49e-08
    Outer Step:    70      LossTrajs: 0.00570936     ContextsNorm: 0.02042240     ValIndCrit: 0.00801855
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.31e-08
        -DiffCxt:  1.02e-07
    Outer Step:    80      LossTrajs: 0.00449315     ContextsNorm: 0.02048958     ValIndCrit: 0.00681272
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.85e-08
        -DiffCxt:  5.51e-08
    Outer Step:    90      LossTrajs: 0.00364015     ContextsNorm: 0.02047793     ValIndCrit: 0.00578274
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.21e-08
        -DiffCxt:  3.04e-08
    Outer Step:   100      LossTrajs: 0.00273717     ContextsNorm: 0.02046486     ValIndCrit: 0.00465268
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.32e-08
        -DiffCxt:  6.15e-08
    Outer Step:   110      LossTrajs: 0.00196846     ContextsNorm: 0.02056955     ValIndCrit: 0.00359226
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.78e-09
        -DiffCxt:  4.19e-08
    Outer Step:   120      LossTrajs: 0.00146733     ContextsNorm: 0.02039345     ValIndCrit: 0.00280694
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.27e-08
        -DiffCxt:  6.69e-08
    Outer Step:   130      LossTrajs: 0.00111395     ContextsNorm: 0.02061836     ValIndCrit: 0.00220082
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.40e-08
        -DiffCxt:  6.01e-08
    Outer Step:   140      LossTrajs: 0.00085451     ContextsNorm: 0.02052260     ValIndCrit: 0.00173390
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.49e-08
        -DiffCxt:  2.94e-08
    Outer Step:   150      LossTrajs: 0.00065047     ContextsNorm: 0.02027756     ValIndCrit: 0.00132266
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.83e-08
        -DiffCxt:  2.28e-08
    Outer Step:   160      LossTrajs: 0.00047895     ContextsNorm: 0.02015532     ValIndCrit: 0.00104091
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.45e-08
        -DiffCxt:  1.84e-08
    Outer Step:   170      LossTrajs: 0.00040047     ContextsNorm: 0.02009939     ValIndCrit: 0.00080477
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.77e-08
        -DiffCxt:  3.26e-08
    Outer Step:   180      LossTrajs: 0.00031054     ContextsNorm: 0.02011493     ValIndCrit: 0.00060937
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.45e-08
        -DiffCxt:  2.29e-08
    Outer Step:   190      LossTrajs: 0.00023085     ContextsNorm: 0.01997129     ValIndCrit: 0.00047493
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.70e-08
        -DiffCxt:  2.37e-08
    Outer Step:   200      LossTrajs: 0.00019011     ContextsNorm: 0.01987301     ValIndCrit: 0.00037242
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.08e-09
        -DiffCxt:  1.08e-08
    Outer Step:   210      LossTrajs: 0.00015428     ContextsNorm: 0.01980120     ValIndCrit: 0.00030443
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.10e-08
        -DiffCxt:  1.54e-08
    Outer Step:   220      LossTrajs: 0.00012874     ContextsNorm: 0.01983241     ValIndCrit: 0.00025761
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.32e-08
        -DiffCxt:  2.99e-08
    Outer Step:   230      LossTrajs: 0.00011167     ContextsNorm: 0.01976803     ValIndCrit: 0.00022300
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.35e-09
        -DiffCxt:  1.53e-08
    Outer Step:   240      LossTrajs: 0.00009254     ContextsNorm: 0.01978927     ValIndCrit: 0.00020077
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.51e-09
        -DiffCxt:  2.03e-08
    Outer Step:   250      LossTrajs: 0.00008600     ContextsNorm: 0.01974091     ValIndCrit: 0.00017428
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.15e-08
        -DiffCxt:  3.56e-08
    Outer Step:   260      LossTrajs: 0.00007789     ContextsNorm: 0.01967923     ValIndCrit: 0.00016214
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.53e-09
        -DiffCxt:  3.10e-08
    Outer Step:   270      LossTrajs: 0.00007118     ContextsNorm: 0.01967660     ValIndCrit: 0.00014874
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.46e-09
        -DiffCxt:  2.40e-08
    Outer Step:   280      LossTrajs: 0.00006894     ContextsNorm: 0.01960680     ValIndCrit: 0.00013622
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.00e-08
        -DiffCxt:  3.67e-08
    Outer Step:   290      LossTrajs: 0.00006391     ContextsNorm: 0.01959249     ValIndCrit: 0.00013040
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.54e-08
        -DiffCxt:  4.01e-08
    Outer Step:   300      LossTrajs: 0.00005866     ContextsNorm: 0.01951783     ValIndCrit: 0.00012039
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.61e-09
        -DiffCxt:  2.99e-08
    Outer Step:   310      LossTrajs: 0.00005529     ContextsNorm: 0.01946714     ValIndCrit: 0.00011902
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.04e-08
        -DiffCxt:  7.96e-08
    Outer Step:   320      LossTrajs: 0.00005547     ContextsNorm: 0.01933828     ValIndCrit: 0.00011238
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.07e-08
        -DiffCxt:  1.28e-07
    Outer Step:   330      LossTrajs: 0.00005172     ContextsNorm: 0.01934224     ValIndCrit: 0.00010405
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.63e-09
        -DiffCxt:  3.79e-08
    Outer Step:   340      LossTrajs: 0.00004835     ContextsNorm: 0.01930728     ValIndCrit: 0.00010093
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.08e-08
        -DiffCxt:  9.61e-08
    Outer Step:   350      LossTrajs: 0.00004682     ContextsNorm: 0.01919910     ValIndCrit: 0.00009345
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.84e-09
        -DiffCxt:  7.27e-08
    Outer Step:   360      LossTrajs: 0.00004837     ContextsNorm: 0.01914466     ValIndCrit: 0.00009135
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.11e-08
        -DiffCxt:  2.08e-07
    Outer Step:   370      LossTrajs: 0.00004441     ContextsNorm: 0.01910256     ValIndCrit: 0.00008724
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.56e-09
        -DiffCxt:  2.15e-08
    Outer Step:   380      LossTrajs: 0.00004245     ContextsNorm: 0.01900661     ValIndCrit: 0.00008826
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.53e-08
        -DiffCxt:  2.03e-07
    Outer Step:   390      LossTrajs: 0.00004109     ContextsNorm: 0.01895289     ValIndCrit: 0.00008013
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.83e-09
        -DiffCxt:  2.24e-08
    Outer Step:   400      LossTrajs: 0.00004601     ContextsNorm: 0.01900577     ValIndCrit: 0.00007873
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.47e-08
        -DiffCxt:  2.00e-07
    Outer Step:   410      LossTrajs: 0.00004018     ContextsNorm: 0.01890854     ValIndCrit: 0.00007427
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.73e-08
        -DiffCxt:  3.68e-07
    Outer Step:   420      LossTrajs: 0.00003757     ContextsNorm: 0.01886467     ValIndCrit: 0.00007419
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.68e-09
        -DiffCxt:  1.58e-07
    Outer Step:   430      LossTrajs: 0.00003690     ContextsNorm: 0.01875816     ValIndCrit: 0.00007031
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.90e-09
        -DiffCxt:  1.59e-07
    Outer Step:   440      LossTrajs: 0.00003622     ContextsNorm: 0.01878535     ValIndCrit: 0.00006931
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.76e-09
        -DiffCxt:  1.83e-07
    Outer Step:   450      LossTrajs: 0.00003606     ContextsNorm: 0.01871773     ValIndCrit: 0.00006548
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.65e-09
        -DiffCxt:  6.28e-08
    Outer Step:   460      LossTrajs: 0.00003458     ContextsNorm: 0.01860528     ValIndCrit: 0.00006381
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.35e-09
        -DiffCxt:  3.65e-08
    Outer Step:   470      LossTrajs: 0.00003609     ContextsNorm: 0.01854124     ValIndCrit: 0.00006287
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.25e-09
        -DiffCxt:  1.70e-08
    Outer Step:   480      LossTrajs: 0.00003344     ContextsNorm: 0.01851543     ValIndCrit: 0.00005954
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   22
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.54e-09
        -DiffCxt:  9.17e-09
    Outer Step:   490      LossTrajs: 0.00003535     ContextsNorm: 0.01849661     ValIndCrit: 0.00005968
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.76e-08
        -DiffCxt:  2.14e-07
    Outer Step:   499      LossTrajs: 0.00003530     ContextsNorm: 0.01852346     ValIndCrit: 0.00005822
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.01e-08
        -DiffCxt:  1.07e-07

Total gradient descent training time: 2 hours 1 mins 30 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 004035
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 5.821536e-05


############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]
==  Begining in-domain visualisation ... ==
    Environment id: 0
    Trajectory id: 27
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./05052024-223846/results_in_domain.png
Testing finished. Figure saved in: ./05052024-223846/results_in_domain_uq.pdf
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.04461709
    Epoch:     1     LossContext: 0.02968841
    Epoch:     2     LossContext: 0.01581356
    Epoch:     3     LossContext: 0.00554682
    Epoch:    10     LossContext: 0.00211499
    Epoch:    20     LossContext: 0.00115092
    Epoch:    30     LossContext: 0.00031219
    Epoch:    40     LossContext: 0.00011389
    Epoch:    50     LossContext: 0.00004659
    Epoch:    60     LossContext: 0.00005834
    Epoch:    70     LossContext: 0.00004786
    Epoch:    80     LossContext: 0.00003073
    Epoch:    90     LossContext: 0.00003099
    Epoch:   100     LossContext: 0.00003044
    Epoch:   110     LossContext: 0.00002983
    Epoch:   120     LossContext: 0.00002988
    Epoch:   130     LossContext: 0.00002977
    Epoch:   140     LossContext: 0.00002977
    Epoch:   150     LossContext: 0.00002975
    Epoch:   160     LossContext: 0.00002974
    Epoch:   170     LossContext: 0.00002973
    Epoch:   180     LossContext: 0.00002973
    Epoch:   190     LossContext: 0.00002972
    Epoch:   200     LossContext: 0.00002972
    Epoch:   210     LossContext: 0.00002971
    Epoch:   220     LossContext: 0.00002971
    Epoch:   230     LossContext: 0.00002970
    Epoch:   240     LossContext: 0.00002969
    Epoch:   250     LossContext: 0.00002969
    Epoch:   260     LossContext: 0.00002968
    Epoch:   270     LossContext: 0.00002968
    Epoch:   280     LossContext: 0.00002967
    Epoch:   290     LossContext: 0.00002966
    Epoch:   300     LossContext: 0.00002966
    Epoch:   310     LossContext: 0.00002965
    Epoch:   320     LossContext: 0.00002964
    Epoch:   330     LossContext: 0.00002964
    Epoch:   340     LossContext: 0.00002963
    Epoch:   350     LossContext: 0.00002963
    Epoch:   360     LossContext: 0.00002962
    Epoch:   370     LossContext: 0.00002962
    Epoch:   380     LossContext: 0.00002962
    Epoch:   390     LossContext: 0.00002961
    Epoch:   400     LossContext: 0.00002961
    Epoch:   410     LossContext: 0.00002960
    Epoch:   420     LossContext: 0.00002960
    Epoch:   430     LossContext: 0.00002960
    Epoch:   440     LossContext: 0.00002959
    Epoch:   450     LossContext: 0.00002959
    Epoch:   460     LossContext: 0.00002958
    Epoch:   470     LossContext: 0.00002958
    Epoch:   480     LossContext: 0.00002957
    Epoch:   490     LossContext: 0.00002957
    Epoch:   500     LossContext: 0.00002957
    Epoch:   510     LossContext: 0.00002956
    Epoch:   520     LossContext: 0.00002956
    Epoch:   530     LossContext: 0.00002955
    Epoch:   540     LossContext: 0.00002955
    Epoch:   550     LossContext: 0.00002954
    Epoch:   560     LossContext: 0.00002954
    Epoch:   570     LossContext: 0.00002953
    Epoch:   580     LossContext: 0.00002953
    Epoch:   590     LossContext: 0.00002952
    Epoch:   600     LossContext: 0.00002952
    Epoch:   610     LossContext: 0.00002951
    Epoch:   620     LossContext: 0.00002951
    Epoch:   630     LossContext: 0.00002950
    Epoch:   640     LossContext: 0.00002950
    Epoch:   650     LossContext: 0.00002949
    Epoch:   660     LossContext: 0.00002948
    Epoch:   670     LossContext: 0.00002948
    Epoch:   680     LossContext: 0.00002947
    Epoch:   690     LossContext: 0.00002947
    Epoch:   700     LossContext: 0.00002946
    Epoch:   710     LossContext: 0.00002946
    Epoch:   720     LossContext: 0.00002945
    Epoch:   730     LossContext: 0.00002945
    Epoch:   740     LossContext: 0.00002944
    Epoch:   750     LossContext: 0.00002943
    Epoch:   760     LossContext: 0.00002943
    Epoch:   770     LossContext: 0.00002942
    Epoch:   780     LossContext: 0.00002942
    Epoch:   790     LossContext: 0.00002941
    Epoch:   800     LossContext: 0.00002940
    Epoch:   810     LossContext: 0.00002940
    Epoch:   820     LossContext: 0.00002939
    Epoch:   830     LossContext: 0.00002938
    Epoch:   840     LossContext: 0.00002938
    Epoch:   850     LossContext: 0.00002937
    Epoch:   860     LossContext: 0.00002936
    Epoch:   870     LossContext: 0.00002936
    Epoch:   880     LossContext: 0.00002935
    Epoch:   890     LossContext: 0.00002935
    Epoch:   900     LossContext: 0.00002934
    Epoch:   910     LossContext: 0.00002933
    Epoch:   920     LossContext: 0.00002933
    Epoch:   930     LossContext: 0.00002932
    Epoch:   940     LossContext: 0.00002931
    Epoch:   950     LossContext: 0.00002930
    Epoch:   960     LossContext: 0.00002930
    Epoch:   970     LossContext: 0.00002929
    Epoch:   980     LossContext: 0.00002928
    Epoch:   990     LossContext: 0.00002928
    Epoch:  1000     LossContext: 0.00002927
    Epoch:  1010     LossContext: 0.00002926
    Epoch:  1020     LossContext: 0.00002925
    Epoch:  1030     LossContext: 0.00002925
    Epoch:  1040     LossContext: 0.00002924
    Epoch:  1050     LossContext: 0.00002923
    Epoch:  1060     LossContext: 0.00002922
    Epoch:  1070     LossContext: 0.00002922
    Epoch:  1080     LossContext: 0.00002921
    Epoch:  1090     LossContext: 0.00002920
    Epoch:  1100     LossContext: 0.00002919
    Epoch:  1110     LossContext: 0.00002919
    Epoch:  1120     LossContext: 0.00002918
    Epoch:  1130     LossContext: 0.00002917
    Epoch:  1140     LossContext: 0.00002916
    Epoch:  1150     LossContext: 0.00002915
    Epoch:  1160     LossContext: 0.00002915
    Epoch:  1170     LossContext: 0.00002914
    Epoch:  1180     LossContext: 0.00002913
    Epoch:  1190     LossContext: 0.00002912
    Epoch:  1200     LossContext: 0.00002911
    Epoch:  1210     LossContext: 0.00002910
    Epoch:  1220     LossContext: 0.00002910
    Epoch:  1230     LossContext: 0.00002909
    Epoch:  1240     LossContext: 0.00002908
    Epoch:  1250     LossContext: 0.00002907
    Epoch:  1260     LossContext: 0.00002906
    Epoch:  1270     LossContext: 0.00002905
    Epoch:  1280     LossContext: 0.00002904
    Epoch:  1290     LossContext: 0.00002904
    Epoch:  1300     LossContext: 0.00002903
    Epoch:  1310     LossContext: 0.00002902
    Epoch:  1320     LossContext: 0.00002901
    Epoch:  1330     LossContext: 0.00002900
    Epoch:  1340     LossContext: 0.00002899
    Epoch:  1350     LossContext: 0.00002898
    Epoch:  1360     LossContext: 0.00002897
    Epoch:  1370     LossContext: 0.00002896
    Epoch:  1380     LossContext: 0.00002895
    Epoch:  1390     LossContext: 0.00002895
    Epoch:  1400     LossContext: 0.00002894
    Epoch:  1410     LossContext: 0.00002893
    Epoch:  1420     LossContext: 0.00002892
    Epoch:  1430     LossContext: 0.00002891
    Epoch:  1440     LossContext: 0.00002890
    Epoch:  1450     LossContext: 0.00002889
    Epoch:  1460     LossContext: 0.00002888
    Epoch:  1470     LossContext: 0.00002887
    Epoch:  1480     LossContext: 0.00002886
    Epoch:  1490     LossContext: 0.00002885
    Epoch:  1499     LossContext: 0.00002884

Gradient descent adaptation time: 0 hours 1 mins 43 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.15309112
    Epoch:     1     LossContext: 0.12616535
    Epoch:     2     LossContext: 0.09875658
    Epoch:     3     LossContext: 0.07150793
    Epoch:    10     LossContext: 0.02262387
    Epoch:    20     LossContext: 0.00932247
    Epoch:    30     LossContext: 0.00040751
    Epoch:    40     LossContext: 0.00055535
    Epoch:    50     LossContext: 0.00008453
    Epoch:    60     LossContext: 0.00011094
    Epoch:    70     LossContext: 0.00008481
    Epoch:    80     LossContext: 0.00007322
    Epoch:    90     LossContext: 0.00006958
    Epoch:   100     LossContext: 0.00006314
    Epoch:   110     LossContext: 0.00006052
    Epoch:   120     LossContext: 0.00006034
    Epoch:   130     LossContext: 0.00006035
    Epoch:   140     LossContext: 0.00006021
    Epoch:   150     LossContext: 0.00006013
    Epoch:   160     LossContext: 0.00006009
    Epoch:   170     LossContext: 0.00006005
    Epoch:   180     LossContext: 0.00006002
    Epoch:   190     LossContext: 0.00006000
    Epoch:   200     LossContext: 0.00005997
    Epoch:   210     LossContext: 0.00005995
    Epoch:   220     LossContext: 0.00005992
    Epoch:   230     LossContext: 0.00005989
    Epoch:   240     LossContext: 0.00005987
    Epoch:   250     LossContext: 0.00005984
    Epoch:   260     LossContext: 0.00005981
    Epoch:   270     LossContext: 0.00005978
    Epoch:   280     LossContext: 0.00005975
    Epoch:   290     LossContext: 0.00005972
    Epoch:   300     LossContext: 0.00005969
    Epoch:   310     LossContext: 0.00005966
    Epoch:   320     LossContext: 0.00005963
    Epoch:   330     LossContext: 0.00005960
    Epoch:   340     LossContext: 0.00005957
    Epoch:   350     LossContext: 0.00005956
    Epoch:   360     LossContext: 0.00005954
    Epoch:   370     LossContext: 0.00005953
    Epoch:   380     LossContext: 0.00005951
    Epoch:   390     LossContext: 0.00005949
    Epoch:   400     LossContext: 0.00005947
    Epoch:   410     LossContext: 0.00005945
    Epoch:   420     LossContext: 0.00005944
    Epoch:   430     LossContext: 0.00005942
    Epoch:   440     LossContext: 0.00005940
    Epoch:   450     LossContext: 0.00005938
    Epoch:   460     LossContext: 0.00005936
    Epoch:   470     LossContext: 0.00005934
    Epoch:   480     LossContext: 0.00005932
    Epoch:   490     LossContext: 0.00005930
    Epoch:   500     LossContext: 0.00005928
    Epoch:   510     LossContext: 0.00005926
    Epoch:   520     LossContext: 0.00005924
    Epoch:   530     LossContext: 0.00005922
    Epoch:   540     LossContext: 0.00005920
    Epoch:   550     LossContext: 0.00005918
    Epoch:   560     LossContext: 0.00005916
    Epoch:   570     LossContext: 0.00005913
    Epoch:   580     LossContext: 0.00005911
    Epoch:   590     LossContext: 0.00005909
    Epoch:   600     LossContext: 0.00005907
    Epoch:   610     LossContext: 0.00005904
    Epoch:   620     LossContext: 0.00005902
    Epoch:   630     LossContext: 0.00005900
    Epoch:   640     LossContext: 0.00005898
    Epoch:   650     LossContext: 0.00005895
    Epoch:   660     LossContext: 0.00005893
    Epoch:   670     LossContext: 0.00005890
    Epoch:   680     LossContext: 0.00005888
    Epoch:   690     LossContext: 0.00005886
    Epoch:   700     LossContext: 0.00005883
    Epoch:   710     LossContext: 0.00005881
    Epoch:   720     LossContext: 0.00005878
    Epoch:   730     LossContext: 0.00005876
    Epoch:   740     LossContext: 0.00005873
    Epoch:   750     LossContext: 0.00005870
    Epoch:   760     LossContext: 0.00005868
    Epoch:   770     LossContext: 0.00005865
    Epoch:   780     LossContext: 0.00005863
    Epoch:   790     LossContext: 0.00005860
    Epoch:   800     LossContext: 0.00005857
    Epoch:   810     LossContext: 0.00005854
    Epoch:   820     LossContext: 0.00005852
    Epoch:   830     LossContext: 0.00005849
    Epoch:   840     LossContext: 0.00005846
    Epoch:   850     LossContext: 0.00005843
    Epoch:   860     LossContext: 0.00005841
    Epoch:   870     LossContext: 0.00005838
    Epoch:   880     LossContext: 0.00005835
    Epoch:   890     LossContext: 0.00005832
    Epoch:   900     LossContext: 0.00005829
    Epoch:   910     LossContext: 0.00005826
    Epoch:   920     LossContext: 0.00005823
    Epoch:   930     LossContext: 0.00005821
    Epoch:   940     LossContext: 0.00005817
    Epoch:   950     LossContext: 0.00005815
    Epoch:   960     LossContext: 0.00005812
    Epoch:   970     LossContext: 0.00005808
    Epoch:   980     LossContext: 0.00005805
    Epoch:   990     LossContext: 0.00005802
    Epoch:  1000     LossContext: 0.00005799
    Epoch:  1010     LossContext: 0.00005796
    Epoch:  1020     LossContext: 0.00005793
    Epoch:  1030     LossContext: 0.00005790
    Epoch:  1040     LossContext: 0.00005787
    Epoch:  1050     LossContext: 0.00005784
    Epoch:  1060     LossContext: 0.00005780
    Epoch:  1070     LossContext: 0.00005777
    Epoch:  1080     LossContext: 0.00005774
    Epoch:  1090     LossContext: 0.00005771
    Epoch:  1100     LossContext: 0.00005767
    Epoch:  1110     LossContext: 0.00005764
    Epoch:  1120     LossContext: 0.00005761
    Epoch:  1130     LossContext: 0.00005757
    Epoch:  1140     LossContext: 0.00005754
    Epoch:  1150     LossContext: 0.00005751
    Epoch:  1160     LossContext: 0.00005747
    Epoch:  1170     LossContext: 0.00005744
    Epoch:  1180     LossContext: 0.00005740
    Epoch:  1190     LossContext: 0.00005737
    Epoch:  1200     LossContext: 0.00005733
    Epoch:  1210     LossContext: 0.00005730
    Epoch:  1220     LossContext: 0.00005726
    Epoch:  1230     LossContext: 0.00005723
    Epoch:  1240     LossContext: 0.00005719
    Epoch:  1250     LossContext: 0.00005716
    Epoch:  1260     LossContext: 0.00005712
    Epoch:  1270     LossContext: 0.00005709
    Epoch:  1280     LossContext: 0.00005705
    Epoch:  1290     LossContext: 0.00005701
    Epoch:  1300     LossContext: 0.00005698
    Epoch:  1310     LossContext: 0.00005694
    Epoch:  1320     LossContext: 0.00005690
    Epoch:  1330     LossContext: 0.00005687
    Epoch:  1340     LossContext: 0.00005683
    Epoch:  1350     LossContext: 0.00005679
    Epoch:  1360     LossContext: 0.00005675
    Epoch:  1370     LossContext: 0.00005672
    Epoch:  1380     LossContext: 0.00005668
    Epoch:  1390     LossContext: 0.00005664
    Epoch:  1400     LossContext: 0.00005660
    Epoch:  1410     LossContext: 0.00005656
    Epoch:  1420     LossContext: 0.00005653
    Epoch:  1430     LossContext: 0.00005649
    Epoch:  1440     LossContext: 0.00005645
    Epoch:  1450     LossContext: 0.00005641
    Epoch:  1460     LossContext: 0.00005637
    Epoch:  1470     LossContext: 0.00005633
    Epoch:  1480     LossContext: 0.00005629
    Epoch:  1490     LossContext: 0.00005625
    Epoch:  1499     LossContext: 0.00005621

Gradient descent adaptation time: 0 hours 1 mins 29 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.02443630
    Epoch:     1     LossContext: 0.01531349
    Epoch:     2     LossContext: 0.00870962
    Epoch:     3     LossContext: 0.00435199
    Epoch:    10     LossContext: 0.00159458
    Epoch:    20     LossContext: 0.00162095
    Epoch:    30     LossContext: 0.00016867
    Epoch:    40     LossContext: 0.00017808
    Epoch:    50     LossContext: 0.00013165
    Epoch:    60     LossContext: 0.00005932
    Epoch:    70     LossContext: 0.00006759
    Epoch:    80     LossContext: 0.00006210
    Epoch:    90     LossContext: 0.00005837
    Epoch:   100     LossContext: 0.00005804
    Epoch:   110     LossContext: 0.00005772
    Epoch:   120     LossContext: 0.00005724
    Epoch:   130     LossContext: 0.00005681
    Epoch:   140     LossContext: 0.00005642
    Epoch:   150     LossContext: 0.00005607
    Epoch:   160     LossContext: 0.00005574
    Epoch:   170     LossContext: 0.00005548
    Epoch:   180     LossContext: 0.00005532
    Epoch:   190     LossContext: 0.00005516
    Epoch:   200     LossContext: 0.00005500
    Epoch:   210     LossContext: 0.00005484
    Epoch:   220     LossContext: 0.00005468
    Epoch:   230     LossContext: 0.00005451
    Epoch:   240     LossContext: 0.00005435
    Epoch:   250     LossContext: 0.00005418
    Epoch:   260     LossContext: 0.00005401
    Epoch:   270     LossContext: 0.00005384
    Epoch:   280     LossContext: 0.00005367
    Epoch:   290     LossContext: 0.00005350
    Epoch:   300     LossContext: 0.00005334
    Epoch:   310     LossContext: 0.00005317
    Epoch:   320     LossContext: 0.00005301
    Epoch:   330     LossContext: 0.00005285
    Epoch:   340     LossContext: 0.00005275
    Epoch:   350     LossContext: 0.00005266
    Epoch:   360     LossContext: 0.00005258
    Epoch:   370     LossContext: 0.00005250
    Epoch:   380     LossContext: 0.00005242
    Epoch:   390     LossContext: 0.00005234
    Epoch:   400     LossContext: 0.00005225
    Epoch:   410     LossContext: 0.00005217
    Epoch:   420     LossContext: 0.00005209
    Epoch:   430     LossContext: 0.00005200
    Epoch:   440     LossContext: 0.00005192
    Epoch:   450     LossContext: 0.00005183
    Epoch:   460     LossContext: 0.00005175
    Epoch:   470     LossContext: 0.00005166
    Epoch:   480     LossContext: 0.00005158
    Epoch:   490     LossContext: 0.00005150
    Epoch:   500     LossContext: 0.00005141
    Epoch:   510     LossContext: 0.00005133
    Epoch:   520     LossContext: 0.00005125
    Epoch:   530     LossContext: 0.00005117
    Epoch:   540     LossContext: 0.00005109
    Epoch:   550     LossContext: 0.00005102
    Epoch:   560     LossContext: 0.00005094
    Epoch:   570     LossContext: 0.00005087
    Epoch:   580     LossContext: 0.00005080
    Epoch:   590     LossContext: 0.00005074
    Epoch:   600     LossContext: 0.00005067
    Epoch:   610     LossContext: 0.00005061
    Epoch:   620     LossContext: 0.00005054
    Epoch:   630     LossContext: 0.00005048
    Epoch:   640     LossContext: 0.00005041
    Epoch:   650     LossContext: 0.00005035
    Epoch:   660     LossContext: 0.00005029
    Epoch:   670     LossContext: 0.00005022
    Epoch:   680     LossContext: 0.00005016
    Epoch:   690     LossContext: 0.00005010
    Epoch:   700     LossContext: 0.00005004
    Epoch:   710     LossContext: 0.00004997
    Epoch:   720     LossContext: 0.00004991
    Epoch:   730     LossContext: 0.00004985
    Epoch:   740     LossContext: 0.00004979
    Epoch:   750     LossContext: 0.00004972
    Epoch:   760     LossContext: 0.00004966
    Epoch:   770     LossContext: 0.00004960
    Epoch:   780     LossContext: 0.00004954
    Epoch:   790     LossContext: 0.00004948
    Epoch:   800     LossContext: 0.00004942
    Epoch:   810     LossContext: 0.00004935
    Epoch:   820     LossContext: 0.00004929
    Epoch:   830     LossContext: 0.00004923
    Epoch:   840     LossContext: 0.00004917
    Epoch:   850     LossContext: 0.00004911
    Epoch:   860     LossContext: 0.00004905
    Epoch:   870     LossContext: 0.00004899
    Epoch:   880     LossContext: 0.00004893
    Epoch:   890     LossContext: 0.00004887
    Epoch:   900     LossContext: 0.00004881
    Epoch:   910     LossContext: 0.00004875
    Epoch:   920     LossContext: 0.00004870
    Epoch:   930     LossContext: 0.00004864
    Epoch:   940     LossContext: 0.00004858
    Epoch:   950     LossContext: 0.00004852
    Epoch:   960     LossContext: 0.00004847
    Epoch:   970     LossContext: 0.00004841
    Epoch:   980     LossContext: 0.00004835
    Epoch:   990     LossContext: 0.00004830
    Epoch:  1000     LossContext: 0.00004824
    Epoch:  1010     LossContext: 0.00004819
    Epoch:  1020     LossContext: 0.00004813
    Epoch:  1030     LossContext: 0.00004808
    Epoch:  1040     LossContext: 0.00004802
    Epoch:  1050     LossContext: 0.00004797
    Epoch:  1060     LossContext: 0.00004791
    Epoch:  1070     LossContext: 0.00004786
    Epoch:  1080     LossContext: 0.00004781
    Epoch:  1090     LossContext: 0.00004775
    Epoch:  1100     LossContext: 0.00004770
    Epoch:  1110     LossContext: 0.00004765
    Epoch:  1120     LossContext: 0.00004760
    Epoch:  1130     LossContext: 0.00004755
    Epoch:  1140     LossContext: 0.00004750
    Epoch:  1150     LossContext: 0.00004746
    Epoch:  1160     LossContext: 0.00004741
    Epoch:  1170     LossContext: 0.00004737
    Epoch:  1180     LossContext: 0.00004732
    Epoch:  1190     LossContext: 0.00004728
    Epoch:  1200     LossContext: 0.00004724
    Epoch:  1210     LossContext: 0.00004720
    Epoch:  1220     LossContext: 0.00004715
    Epoch:  1230     LossContext: 0.00004711
    Epoch:  1240     LossContext: 0.00004707
    Epoch:  1250     LossContext: 0.00004703
    Epoch:  1260     LossContext: 0.00004698
    Epoch:  1270     LossContext: 0.00004694
    Epoch:  1280     LossContext: 0.00004690
    Epoch:  1290     LossContext: 0.00004686
    Epoch:  1300     LossContext: 0.00004682
    Epoch:  1310     LossContext: 0.00004678
    Epoch:  1320     LossContext: 0.00004674
    Epoch:  1330     LossContext: 0.00004670
    Epoch:  1340     LossContext: 0.00004666
    Epoch:  1350     LossContext: 0.00004662
    Epoch:  1360     LossContext: 0.00004658
    Epoch:  1370     LossContext: 0.00004655
    Epoch:  1380     LossContext: 0.00004651
    Epoch:  1390     LossContext: 0.00004647
    Epoch:  1400     LossContext: 0.00004643
    Epoch:  1410     LossContext: 0.00004639
    Epoch:  1420     LossContext: 0.00004635
    Epoch:  1430     LossContext: 0.00004632
    Epoch:  1440     LossContext: 0.00004628
    Epoch:  1450     LossContext: 0.00004624
    Epoch:  1460     LossContext: 0.00004620
    Epoch:  1470     LossContext: 0.00004616
    Epoch:  1480     LossContext: 0.00004612
    Epoch:  1490     LossContext: 0.00004608
    Epoch:  1499     LossContext: 0.00004605

Gradient descent adaptation time: 0 hours 1 mins 29 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.01159590
    Epoch:     1     LossContext: 0.00677149
    Epoch:     2     LossContext: 0.00384538
    Epoch:     3     LossContext: 0.00199914
    Epoch:    10     LossContext: 0.00039664
    Epoch:    20     LossContext: 0.00087743
    Epoch:    30     LossContext: 0.00012629
    Epoch:    40     LossContext: 0.00014010
    Epoch:    50     LossContext: 0.00006141
    Epoch:    60     LossContext: 0.00006752
    Epoch:    70     LossContext: 0.00005415
    Epoch:    80     LossContext: 0.00005553
    Epoch:    90     LossContext: 0.00005345
    Epoch:   100     LossContext: 0.00005325
    Epoch:   110     LossContext: 0.00005293
    Epoch:   120     LossContext: 0.00005261
    Epoch:   130     LossContext: 0.00005237
    Epoch:   140     LossContext: 0.00005213
    Epoch:   150     LossContext: 0.00005189
    Epoch:   160     LossContext: 0.00005167
    Epoch:   170     LossContext: 0.00005148
    Epoch:   180     LossContext: 0.00005137
    Epoch:   190     LossContext: 0.00005127
    Epoch:   200     LossContext: 0.00005116
    Epoch:   210     LossContext: 0.00005105
    Epoch:   220     LossContext: 0.00005094
    Epoch:   230     LossContext: 0.00005083
    Epoch:   240     LossContext: 0.00005072
    Epoch:   250     LossContext: 0.00005061
    Epoch:   260     LossContext: 0.00005051
    Epoch:   270     LossContext: 0.00005041
    Epoch:   280     LossContext: 0.00005031
    Epoch:   290     LossContext: 0.00005021
    Epoch:   300     LossContext: 0.00005011
    Epoch:   310     LossContext: 0.00005001
    Epoch:   320     LossContext: 0.00004991
    Epoch:   330     LossContext: 0.00004980
    Epoch:   340     LossContext: 0.00004973
    Epoch:   350     LossContext: 0.00004968
    Epoch:   360     LossContext: 0.00004963
    Epoch:   370     LossContext: 0.00004958
    Epoch:   380     LossContext: 0.00004952
    Epoch:   390     LossContext: 0.00004947
    Epoch:   400     LossContext: 0.00004942
    Epoch:   410     LossContext: 0.00004937
    Epoch:   420     LossContext: 0.00004931
    Epoch:   430     LossContext: 0.00004926
    Epoch:   440     LossContext: 0.00004921
    Epoch:   450     LossContext: 0.00004915
    Epoch:   460     LossContext: 0.00004910
    Epoch:   470     LossContext: 0.00004905
    Epoch:   480     LossContext: 0.00004899
    Epoch:   490     LossContext: 0.00004894
    Epoch:   500     LossContext: 0.00004889
    Epoch:   510     LossContext: 0.00004884
    Epoch:   520     LossContext: 0.00004879
    Epoch:   530     LossContext: 0.00004874
    Epoch:   540     LossContext: 0.00004869
    Epoch:   550     LossContext: 0.00004865
    Epoch:   560     LossContext: 0.00004860
    Epoch:   570     LossContext: 0.00004856
    Epoch:   580     LossContext: 0.00004851
    Epoch:   590     LossContext: 0.00004846
    Epoch:   600     LossContext: 0.00004841
    Epoch:   610     LossContext: 0.00004837
    Epoch:   620     LossContext: 0.00004832
    Epoch:   630     LossContext: 0.00004828
    Epoch:   640     LossContext: 0.00004823
    Epoch:   650     LossContext: 0.00004819
    Epoch:   660     LossContext: 0.00004814
    Epoch:   670     LossContext: 0.00004810
    Epoch:   680     LossContext: 0.00004805
    Epoch:   690     LossContext: 0.00004801
    Epoch:   700     LossContext: 0.00004797
    Epoch:   710     LossContext: 0.00004792
    Epoch:   720     LossContext: 0.00004788
    Epoch:   730     LossContext: 0.00004784
    Epoch:   740     LossContext: 0.00004780
    Epoch:   750     LossContext: 0.00004776
    Epoch:   760     LossContext: 0.00004773
    Epoch:   770     LossContext: 0.00004769
    Epoch:   780     LossContext: 0.00004766
    Epoch:   790     LossContext: 0.00004762
    Epoch:   800     LossContext: 0.00004759
    Epoch:   810     LossContext: 0.00004755
    Epoch:   820     LossContext: 0.00004752
    Epoch:   830     LossContext: 0.00004748
    Epoch:   840     LossContext: 0.00004745
    Epoch:   850     LossContext: 0.00004742
    Epoch:   860     LossContext: 0.00004738
    Epoch:   870     LossContext: 0.00004735
    Epoch:   880     LossContext: 0.00004732
    Epoch:   890     LossContext: 0.00004729
    Epoch:   900     LossContext: 0.00004726
    Epoch:   910     LossContext: 0.00004723
    Epoch:   920     LossContext: 0.00004720
    Epoch:   930     LossContext: 0.00004716
    Epoch:   940     LossContext: 0.00004713
    Epoch:   950     LossContext: 0.00004710
    Epoch:   960     LossContext: 0.00004707
    Epoch:   970     LossContext: 0.00004704
    Epoch:   980     LossContext: 0.00004701
    Epoch:   990     LossContext: 0.00004699
    Epoch:  1000     LossContext: 0.00004696
    Epoch:  1010     LossContext: 0.00004693
    Epoch:  1020     LossContext: 0.00004690
    Epoch:  1030     LossContext: 0.00004687
    Epoch:  1040     LossContext: 0.00004684
    Epoch:  1050     LossContext: 0.00004682
    Epoch:  1060     LossContext: 0.00004679
    Epoch:  1070     LossContext: 0.00004677
    Epoch:  1080     LossContext: 0.00004675
    Epoch:  1090     LossContext: 0.00004673
    Epoch:  1100     LossContext: 0.00004671
    Epoch:  1110     LossContext: 0.00004669
    Epoch:  1120     LossContext: 0.00004667
    Epoch:  1130     LossContext: 0.00004665
    Epoch:  1140     LossContext: 0.00004663
    Epoch:  1150     LossContext: 0.00004661
    Epoch:  1160     LossContext: 0.00004660
    Epoch:  1170     LossContext: 0.00004658
    Epoch:  1180     LossContext: 0.00004657
    Epoch:  1190     LossContext: 0.00004655
    Epoch:  1200     LossContext: 0.00004654
    Epoch:  1210     LossContext: 0.00004652
    Epoch:  1220     LossContext: 0.00004651
    Epoch:  1230     LossContext: 0.00004649
    Epoch:  1240     LossContext: 0.00004648
    Epoch:  1250     LossContext: 0.00004646
    Epoch:  1260     LossContext: 0.00004645
    Epoch:  1270     LossContext: 0.00004643
    Epoch:  1280     LossContext: 0.00004641
    Epoch:  1290     LossContext: 0.00004640
    Epoch:  1300     LossContext: 0.00004638
    Epoch:  1310     LossContext: 0.00004637
    Epoch:  1320     LossContext: 0.00004635
    Epoch:  1330     LossContext: 0.00004634
    Epoch:  1340     LossContext: 0.00004632
    Epoch:  1350     LossContext: 0.00004631
    Epoch:  1360     LossContext: 0.00004630
    Epoch:  1370     LossContext: 0.00004628
    Epoch:  1380     LossContext: 0.00004627
    Epoch:  1390     LossContext: 0.00004626
    Epoch:  1400     LossContext: 0.00004625
    Epoch:  1410     LossContext: 0.00004623
    Epoch:  1420     LossContext: 0.00004622
    Epoch:  1430     LossContext: 0.00004621
    Epoch:  1440     LossContext: 0.00004620
    Epoch:  1450     LossContext: 0.00004619
    Epoch:  1460     LossContext: 0.00004617
    Epoch:  1470     LossContext: 0.00004616
    Epoch:  1480     LossContext: 0.00004615
    Epoch:  1490     LossContext: 0.00004614
    Epoch:  1499     LossContext: 0.00004613

Gradient descent adaptation time: 0 hours 1 mins 29 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./05052024-223846/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 7.768692e-05

