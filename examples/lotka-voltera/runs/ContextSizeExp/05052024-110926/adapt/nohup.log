
############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

========= STARTING =========
Working with a context size of 2^1 = 2
Run folder created successfuly: ./05052024-092750/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 092807
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 092808
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 46608 


WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed


=== Beginning training with proximal alternating minimization ... ===
    Number of examples in a batch: 4
    Maximum number of steps per inner minimization: 25
    Maximum number of outer minimizations: 500
    Maximum total number of training steps: 12500

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Outer Step:     0      LossTrajs: 1.81044996     ContextsNorm: 0.00000000     ValIndCrit: 1.69829845
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.49e-06
        -DiffCxt:  inf
    Outer Step:     1      LossTrajs: 1.50677943     ContextsNorm: 0.00012276     ValIndCrit: 1.41141903
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.46e-06
        -DiffCxt:  3.88e-03
    Outer Step:     2      LossTrajs: 1.14373624     ContextsNorm: 0.00031573     ValIndCrit: 1.06682456
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.25e-06
        -DiffCxt:  2.56e-03
    Outer Step:     3      LossTrajs: 0.59868580     ContextsNorm: 0.00066125     ValIndCrit: 0.56421119
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.90e-06
        -DiffCxt:  7.78e-04
    Outer Step:    10      LossTrajs: 0.25790417     ContextsNorm: 0.00133178     ValIndCrit: 0.30458218
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.26e-07
        -DiffCxt:  5.22e-05
    Outer Step:    20      LossTrajs: 0.16468894     ContextsNorm: 0.00239894     ValIndCrit: 0.17729028
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.07e-07
        -DiffCxt:  2.82e-05
    Outer Step:    30      LossTrajs: 0.08489937     ContextsNorm: 0.00409630     ValIndCrit: 0.08879507
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.17e-08
        -DiffCxt:  7.26e-06
    Outer Step:    40      LossTrajs: 0.07555290     ContextsNorm: 0.00562518     ValIndCrit: 0.07920280
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.22e-09
        -DiffCxt:  4.28e-06
    Outer Step:    50      LossTrajs: 0.07451831     ContextsNorm: 0.00781399     ValIndCrit: 0.07816203
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.48e-09
        -DiffCxt:  5.11e-06
    Outer Step:    60      LossTrajs: 0.07335620     ContextsNorm: 0.01165273     ValIndCrit: 0.07697055
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.73e-08
        -DiffCxt:  6.05e-06
    Outer Step:    70      LossTrajs: 0.06688280     ContextsNorm: 0.02409797     ValIndCrit: 0.06959999
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.93e-07
        -DiffCxt:  3.54e-06
    Outer Step:    80      LossTrajs: 0.03909003     ContextsNorm: 0.03499429     ValIndCrit: 0.04393270
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.81e-08
        -DiffCxt:  1.36e-07
    Outer Step:    90      LossTrajs: 0.03122975     ContextsNorm: 0.03768751     ValIndCrit: 0.03761768
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.72e-08
        -DiffCxt:  1.54e-07
    Outer Step:   100      LossTrajs: 0.02904842     ContextsNorm: 0.03898310     ValIndCrit: 0.03616863
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.76e-09
        -DiffCxt:  4.47e-08
    Outer Step:   110      LossTrajs: 0.02817778     ContextsNorm: 0.03939323     ValIndCrit: 0.03647576
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.02e-09
        -DiffCxt:  1.31e-07
    Outer Step:   120      LossTrajs: 0.02743174     ContextsNorm: 0.03910310     ValIndCrit: 0.03696220
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.76e-08
        -DiffCxt:  1.13e-07
    Outer Step:   130      LossTrajs: 0.02640934     ContextsNorm: 0.04044576     ValIndCrit: 0.03746767
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.10e-08
        -DiffCxt:  3.18e-07
    Outer Step:   140      LossTrajs: 0.02317197     ContextsNorm: 0.04312186     ValIndCrit: 0.03576146
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.57e-07
        -DiffCxt:  5.05e-07
    Outer Step:   150      LossTrajs: 0.00645207     ContextsNorm: 0.04913782     ValIndCrit: 0.01263865
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.96e-08
        -DiffCxt:  1.08e-07
    Outer Step:   160      LossTrajs: 0.00431281     ContextsNorm: 0.04945314     ValIndCrit: 0.00724582
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.71e-08
        -DiffCxt:  5.44e-08
    Outer Step:   170      LossTrajs: 0.00159951     ContextsNorm: 0.04901071     ValIndCrit: 0.00227597
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.33e-08
        -DiffCxt:  7.65e-08
    Outer Step:   180      LossTrajs: 0.00089688     ContextsNorm: 0.04874317     ValIndCrit: 0.00191422
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.79e-09
        -DiffCxt:  8.72e-08
    Outer Step:   190      LossTrajs: 0.00068645     ContextsNorm: 0.04900306     ValIndCrit: 0.00138373
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.94e-08
        -DiffCxt:  1.27e-07
    Outer Step:   200      LossTrajs: 0.00057647     ContextsNorm: 0.04902410     ValIndCrit: 0.00118417
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.62e-09
        -DiffCxt:  1.20e-08
    Outer Step:   210      LossTrajs: 0.00049002     ContextsNorm: 0.04875042     ValIndCrit: 0.00092099
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.41e-09
        -DiffCxt:  3.61e-08
    Outer Step:   220      LossTrajs: 0.00044297     ContextsNorm: 0.04879423     ValIndCrit: 0.00092900
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.90e-09
        -DiffCxt:  1.09e-07
    Outer Step:   230      LossTrajs: 0.00037311     ContextsNorm: 0.04866502     ValIndCrit: 0.00074383
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   12
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.62e-10
        -DiffCxt:  9.98e-09
    Outer Step:   240      LossTrajs: 0.00035597     ContextsNorm: 0.04873635     ValIndCrit: 0.00067592
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   13
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.21e-08
        -DiffCxt:  9.77e-09
    Outer Step:   250      LossTrajs: 0.00031833     ContextsNorm: 0.04841272     ValIndCrit: 0.00061209
        Saving best model so far ...
        -NbInnerStepsNode:    4
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.83e-10
        -DiffCxt:  1.51e-08
    Outer Step:   260      LossTrajs: 0.00030662     ContextsNorm: 0.04833229     ValIndCrit: 0.00052955
        Saving best model so far ...
        -NbInnerStepsNode:   14
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.91e-10
        -DiffCxt:  1.59e-08
    Outer Step:   270      LossTrajs: 0.00029965     ContextsNorm: 0.04822287     ValIndCrit: 0.00054660
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   11
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.91e-10
        -DiffCxt:  5.47e-09
    Outer Step:   280      LossTrajs: 0.00030076     ContextsNorm: 0.04822177     ValIndCrit: 0.00050042
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.12e-08
        -DiffCxt:  2.27e-08
    Outer Step:   290      LossTrajs: 0.00027214     ContextsNorm: 0.04794296     ValIndCrit: 0.00049300
        Saving best model so far ...
        -NbInnerStepsNode:    2
        -NbInnerStepsCxt:   22
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.18e-10
        -DiffCxt:  9.63e-09
    Outer Step:   300      LossTrajs: 0.00025019     ContextsNorm: 0.04803377     ValIndCrit: 0.00043577
        Saving best model so far ...
        -NbInnerStepsNode:   17
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.33e-10
        -DiffCxt:  1.06e-08
    Outer Step:   310      LossTrajs: 0.00023189     ContextsNorm: 0.04801112     ValIndCrit: 0.00042103
        Saving best model so far ...
        -NbInnerStepsNode:   21
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.04e-10
        -DiffCxt:  1.31e-08
    Outer Step:   320      LossTrajs: 0.00023181     ContextsNorm: 0.04783260     ValIndCrit: 0.00039928
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.96e-09
        -DiffCxt:  3.84e-08
    Outer Step:   330      LossTrajs: 0.00022380     ContextsNorm: 0.04773809     ValIndCrit: 0.00038860
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.25e-10
        -DiffCxt:  9.77e-09
    Outer Step:   340      LossTrajs: 0.00022147     ContextsNorm: 0.04770055     ValIndCrit: 0.00035530
        Saving best model so far ...
        -NbInnerStepsNode:   19
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.22e-10
        -DiffCxt:  3.07e-08
    Outer Step:   350      LossTrajs: 0.00023278     ContextsNorm: 0.04811838     ValIndCrit: 0.00035818
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.61e-08
        -DiffCxt:  2.81e-08
    Outer Step:   360      LossTrajs: 0.00019958     ContextsNorm: 0.04808743     ValIndCrit: 0.00032330
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   21
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.16e-09
        -DiffCxt:  7.59e-09
    Outer Step:   370      LossTrajs: 0.00020059     ContextsNorm: 0.04800534     ValIndCrit: 0.00032409
        -NbInnerStepsNode:    2
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.20e-10
        -DiffCxt:  7.82e-09
    Outer Step:   380      LossTrajs: 0.00019171     ContextsNorm: 0.04773479     ValIndCrit: 0.00032134
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   23
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.58e-10
        -DiffCxt:  9.11e-09
    Outer Step:   390      LossTrajs: 0.00018528     ContextsNorm: 0.04791500     ValIndCrit: 0.00030539
        Saving best model so far ...
        -NbInnerStepsNode:   19
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.65e-10
        -DiffCxt:  6.14e-08
    Outer Step:   400      LossTrajs: 0.00017323     ContextsNorm: 0.04770062     ValIndCrit: 0.00029606
        Saving best model so far ...
        -NbInnerStepsNode:    2
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.91e-10
        -DiffCxt:  1.90e-08
    Outer Step:   410      LossTrajs: 0.00017670     ContextsNorm: 0.04773903     ValIndCrit: 0.00030002
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.99e-09
        -DiffCxt:  6.39e-08
    Outer Step:   420      LossTrajs: 0.00017221     ContextsNorm: 0.04776692     ValIndCrit: 0.00029049
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.94e-10
        -DiffCxt:  3.40e-08
    Outer Step:   430      LossTrajs: 0.00017293     ContextsNorm: 0.04792991     ValIndCrit: 0.00026488
        Saving best model so far ...
        -NbInnerStepsNode:   15
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.72e-10
        -DiffCxt:  6.16e-08
    Outer Step:   440      LossTrajs: 0.00014952     ContextsNorm: 0.04806184     ValIndCrit: 0.00023202
        Saving best model so far ...
        -NbInnerStepsNode:    3
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.12e-10
        -DiffCxt:  1.94e-08
    Outer Step:   450      LossTrajs: 0.00016362     ContextsNorm: 0.04786354     ValIndCrit: 0.00023735
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.38e-09
        -DiffCxt:  1.52e-08
    Outer Step:   460      LossTrajs: 0.00015874     ContextsNorm: 0.04795495     ValIndCrit: 0.00023333
        -NbInnerStepsNode:    9
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.27e-10
        -DiffCxt:  1.74e-08
    Outer Step:   470      LossTrajs: 0.00015666     ContextsNorm: 0.04803444     ValIndCrit: 0.00023048
        Saving best model so far ...
        -NbInnerStepsNode:    6
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.39e-10
        -DiffCxt:  2.39e-08
    Outer Step:   480      LossTrajs: 0.00014695     ContextsNorm: 0.04792618     ValIndCrit: 0.00022213
        Saving best model so far ...
        -NbInnerStepsNode:    2
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.51e-10
        -DiffCxt:  9.28e-09
    Outer Step:   490      LossTrajs: 0.00014976     ContextsNorm: 0.04834351     ValIndCrit: 0.00021246
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.80e-09
        -DiffCxt:  3.38e-08
    Outer Step:   499      LossTrajs: 0.00014572     ContextsNorm: 0.04845441     ValIndCrit: 0.00021431
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.51e-10
        -DiffCxt:  1.93e-08

Total gradient descent training time: 1 hours 34 mins 28 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 110237
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 0.00021246144


############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]
==  Begining in-domain visualisation ... ==
    Environment id: 0
    Trajectory id: 17
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./05052024-092750/results_in_domain.png
Testing finished. Figure saved in: ./05052024-092750/results_in_domain_uq.pdf
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.02089248
    Epoch:     1     LossContext: 0.01652092
    Epoch:     2     LossContext: 0.01244161
    Epoch:     3     LossContext: 0.00897422
    Epoch:    10     LossContext: 0.00064107
    Epoch:    20     LossContext: 0.00045005
    Epoch:    30     LossContext: 0.00058595
    Epoch:    40     LossContext: 0.00014889
    Epoch:    50     LossContext: 0.00011752
    Epoch:    60     LossContext: 0.00012547
    Epoch:    70     LossContext: 0.00011250
    Epoch:    80     LossContext: 0.00010559
    Epoch:    90     LossContext: 0.00010393
    Epoch:   100     LossContext: 0.00010349
    Epoch:   110     LossContext: 0.00010349
    Epoch:   120     LossContext: 0.00010348
    Epoch:   130     LossContext: 0.00010348
    Epoch:   140     LossContext: 0.00010347
    Epoch:   150     LossContext: 0.00010347
    Epoch:   160     LossContext: 0.00010347
    Epoch:   170     LossContext: 0.00010347
    Epoch:   180     LossContext: 0.00010347
    Epoch:   190     LossContext: 0.00010347
    Epoch:   200     LossContext: 0.00010347
    Epoch:   210     LossContext: 0.00010347
    Epoch:   220     LossContext: 0.00010347
    Epoch:   230     LossContext: 0.00010347
    Epoch:   240     LossContext: 0.00010347
    Epoch:   250     LossContext: 0.00010347
    Epoch:   260     LossContext: 0.00010347
    Epoch:   270     LossContext: 0.00010347
    Epoch:   280     LossContext: 0.00010347
    Epoch:   290     LossContext: 0.00010347
    Epoch:   300     LossContext: 0.00010347
    Epoch:   310     LossContext: 0.00010347
    Epoch:   320     LossContext: 0.00010347
    Epoch:   330     LossContext: 0.00010347
    Epoch:   340     LossContext: 0.00010347
    Epoch:   350     LossContext: 0.00010347
    Epoch:   360     LossContext: 0.00010347
    Epoch:   370     LossContext: 0.00010347
    Epoch:   380     LossContext: 0.00010347
    Epoch:   390     LossContext: 0.00010347
    Epoch:   400     LossContext: 0.00010347
    Epoch:   410     LossContext: 0.00010347
    Epoch:   420     LossContext: 0.00010347
    Epoch:   430     LossContext: 0.00010347
    Epoch:   440     LossContext: 0.00010347
    Epoch:   450     LossContext: 0.00010347
    Epoch:   460     LossContext: 0.00010347
    Epoch:   470     LossContext: 0.00010347
    Epoch:   480     LossContext: 0.00010347
    Epoch:   490     LossContext: 0.00010347
    Epoch:   500     LossContext: 0.00010347
    Epoch:   510     LossContext: 0.00010347
    Epoch:   520     LossContext: 0.00010347
    Epoch:   530     LossContext: 0.00010347
    Epoch:   540     LossContext: 0.00010347
    Epoch:   550     LossContext: 0.00010347
    Epoch:   560     LossContext: 0.00010347
    Epoch:   570     LossContext: 0.00010347
    Epoch:   580     LossContext: 0.00010347
    Epoch:   590     LossContext: 0.00010347
    Epoch:   600     LossContext: 0.00010347
    Epoch:   610     LossContext: 0.00010347
    Epoch:   620     LossContext: 0.00010347
    Epoch:   630     LossContext: 0.00010347
    Epoch:   640     LossContext: 0.00010347
    Epoch:   650     LossContext: 0.00010347
    Epoch:   660     LossContext: 0.00010347
    Epoch:   670     LossContext: 0.00010347
    Epoch:   680     LossContext: 0.00010347
    Epoch:   690     LossContext: 0.00010347
    Epoch:   700     LossContext: 0.00010347
    Epoch:   710     LossContext: 0.00010347
    Epoch:   720     LossContext: 0.00010347
    Epoch:   730     LossContext: 0.00010347
    Epoch:   740     LossContext: 0.00010347
    Epoch:   750     LossContext: 0.00010347
    Epoch:   760     LossContext: 0.00010347
    Epoch:   770     LossContext: 0.00010347
    Epoch:   780     LossContext: 0.00010347
    Epoch:   790     LossContext: 0.00010347
    Epoch:   800     LossContext: 0.00010347
    Epoch:   810     LossContext: 0.00010347
    Epoch:   820     LossContext: 0.00010347
    Epoch:   830     LossContext: 0.00010347
    Epoch:   840     LossContext: 0.00010347
    Epoch:   850     LossContext: 0.00010347
    Epoch:   860     LossContext: 0.00010347
    Epoch:   870     LossContext: 0.00010347
    Epoch:   880     LossContext: 0.00010347
    Epoch:   890     LossContext: 0.00010347
    Epoch:   900     LossContext: 0.00010347
    Epoch:   910     LossContext: 0.00010347
    Epoch:   920     LossContext: 0.00010347
    Epoch:   930     LossContext: 0.00010347
    Epoch:   940     LossContext: 0.00010347
    Epoch:   950     LossContext: 0.00010347
    Epoch:   960     LossContext: 0.00010347
    Epoch:   970     LossContext: 0.00010347
    Epoch:   980     LossContext: 0.00010347
    Epoch:   990     LossContext: 0.00010347
    Epoch:  1000     LossContext: 0.00010347
    Epoch:  1010     LossContext: 0.00010347
    Epoch:  1020     LossContext: 0.00010347
    Epoch:  1030     LossContext: 0.00010347
    Epoch:  1040     LossContext: 0.00010347
    Epoch:  1050     LossContext: 0.00010347
    Epoch:  1060     LossContext: 0.00010347
    Epoch:  1070     LossContext: 0.00010347
    Epoch:  1080     LossContext: 0.00010347
    Epoch:  1090     LossContext: 0.00010347
    Epoch:  1100     LossContext: 0.00010347
    Epoch:  1110     LossContext: 0.00010347
    Epoch:  1120     LossContext: 0.00010347
    Epoch:  1130     LossContext: 0.00010347
    Epoch:  1140     LossContext: 0.00010347
    Epoch:  1150     LossContext: 0.00010347
    Epoch:  1160     LossContext: 0.00010347
    Epoch:  1170     LossContext: 0.00010347
    Epoch:  1180     LossContext: 0.00010347
    Epoch:  1190     LossContext: 0.00010347
    Epoch:  1200     LossContext: 0.00010347
    Epoch:  1210     LossContext: 0.00010347
    Epoch:  1220     LossContext: 0.00010347
    Epoch:  1230     LossContext: 0.00010347
    Epoch:  1240     LossContext: 0.00010347
    Epoch:  1250     LossContext: 0.00010347
    Epoch:  1260     LossContext: 0.00010347
    Epoch:  1270     LossContext: 0.00010347
    Epoch:  1280     LossContext: 0.00010347
    Epoch:  1290     LossContext: 0.00010347
    Epoch:  1300     LossContext: 0.00010347
    Epoch:  1310     LossContext: 0.00010347
    Epoch:  1320     LossContext: 0.00010347
    Epoch:  1330     LossContext: 0.00010347
    Epoch:  1340     LossContext: 0.00010347
    Epoch:  1350     LossContext: 0.00010347
    Epoch:  1360     LossContext: 0.00010347
    Epoch:  1370     LossContext: 0.00010347
    Epoch:  1380     LossContext: 0.00010347
    Epoch:  1390     LossContext: 0.00010347
    Epoch:  1400     LossContext: 0.00010347
    Epoch:  1410     LossContext: 0.00010347
    Epoch:  1420     LossContext: 0.00010347
    Epoch:  1430     LossContext: 0.00010347
    Epoch:  1440     LossContext: 0.00010347
    Epoch:  1450     LossContext: 0.00010347
    Epoch:  1460     LossContext: 0.00010347
    Epoch:  1470     LossContext: 0.00010347
    Epoch:  1480     LossContext: 0.00010347
    Epoch:  1490     LossContext: 0.00010347
    Epoch:  1499     LossContext: 0.00010347

Gradient descent adaptation time: 0 hours 1 mins 47 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.12934403
    Epoch:     1     LossContext: 0.11780426
    Epoch:     2     LossContext: 0.10555889
    Epoch:     3     LossContext: 0.09268703
    Epoch:    10     LossContext: 0.02202835
    Epoch:    20     LossContext: 0.00224022
    Epoch:    30     LossContext: 0.00129920
    Epoch:    40     LossContext: 0.00079673
    Epoch:    50     LossContext: 0.00052119
    Epoch:    60     LossContext: 0.00039112
    Epoch:    70     LossContext: 0.00028016
    Epoch:    80     LossContext: 0.00023065
    Epoch:    90     LossContext: 0.00022134
    Epoch:   100     LossContext: 0.00022064
    Epoch:   110     LossContext: 0.00021942
    Epoch:   120     LossContext: 0.00021855
    Epoch:   130     LossContext: 0.00021836
    Epoch:   140     LossContext: 0.00021836
    Epoch:   150     LossContext: 0.00021835
    Epoch:   160     LossContext: 0.00021834
    Epoch:   170     LossContext: 0.00021834
    Epoch:   180     LossContext: 0.00021834
    Epoch:   190     LossContext: 0.00021834
    Epoch:   200     LossContext: 0.00021834
    Epoch:   210     LossContext: 0.00021834
    Epoch:   220     LossContext: 0.00021834
    Epoch:   230     LossContext: 0.00021834
    Epoch:   240     LossContext: 0.00021834
    Epoch:   250     LossContext: 0.00021834
    Epoch:   260     LossContext: 0.00021834
    Epoch:   270     LossContext: 0.00021834
    Epoch:   280     LossContext: 0.00021834
    Epoch:   290     LossContext: 0.00021834
    Epoch:   300     LossContext: 0.00021834
    Epoch:   310     LossContext: 0.00021834
    Epoch:   320     LossContext: 0.00021834
    Epoch:   330     LossContext: 0.00021834
    Epoch:   340     LossContext: 0.00021834
    Epoch:   350     LossContext: 0.00021834
    Epoch:   360     LossContext: 0.00021834
    Epoch:   370     LossContext: 0.00021834
    Epoch:   380     LossContext: 0.00021834
    Epoch:   390     LossContext: 0.00021834
    Epoch:   400     LossContext: 0.00021834
    Epoch:   410     LossContext: 0.00021834
    Epoch:   420     LossContext: 0.00021834
    Epoch:   430     LossContext: 0.00021834
    Epoch:   440     LossContext: 0.00021834
    Epoch:   450     LossContext: 0.00021834
    Epoch:   460     LossContext: 0.00021834
    Epoch:   470     LossContext: 0.00021834
    Epoch:   480     LossContext: 0.00021834
    Epoch:   490     LossContext: 0.00021834
    Epoch:   500     LossContext: 0.00021834
    Epoch:   510     LossContext: 0.00021834
    Epoch:   520     LossContext: 0.00021834
    Epoch:   530     LossContext: 0.00021834
    Epoch:   540     LossContext: 0.00021834
    Epoch:   550     LossContext: 0.00021834
    Epoch:   560     LossContext: 0.00021834
    Epoch:   570     LossContext: 0.00021834
    Epoch:   580     LossContext: 0.00021834
    Epoch:   590     LossContext: 0.00021834
    Epoch:   600     LossContext: 0.00021834
    Epoch:   610     LossContext: 0.00021834
    Epoch:   620     LossContext: 0.00021834
    Epoch:   630     LossContext: 0.00021834
    Epoch:   640     LossContext: 0.00021834
    Epoch:   650     LossContext: 0.00021834
    Epoch:   660     LossContext: 0.00021834
    Epoch:   670     LossContext: 0.00021834
    Epoch:   680     LossContext: 0.00021834
    Epoch:   690     LossContext: 0.00021834
    Epoch:   700     LossContext: 0.00021834
    Epoch:   710     LossContext: 0.00021834
    Epoch:   720     LossContext: 0.00021834
    Epoch:   730     LossContext: 0.00021834
    Epoch:   740     LossContext: 0.00021834
    Epoch:   750     LossContext: 0.00021834
    Epoch:   760     LossContext: 0.00021834
    Epoch:   770     LossContext: 0.00021834
    Epoch:   780     LossContext: 0.00021834
    Epoch:   790     LossContext: 0.00021834
    Epoch:   800     LossContext: 0.00021834
    Epoch:   810     LossContext: 0.00021834
    Epoch:   820     LossContext: 0.00021834
    Epoch:   830     LossContext: 0.00021834
    Epoch:   840     LossContext: 0.00021834
    Epoch:   850     LossContext: 0.00021834
    Epoch:   860     LossContext: 0.00021834
    Epoch:   870     LossContext: 0.00021834
    Epoch:   880     LossContext: 0.00021834
    Epoch:   890     LossContext: 0.00021834
    Epoch:   900     LossContext: 0.00021834
    Epoch:   910     LossContext: 0.00021834
    Epoch:   920     LossContext: 0.00021834
    Epoch:   930     LossContext: 0.00021834
    Epoch:   940     LossContext: 0.00021834
    Epoch:   950     LossContext: 0.00021834
    Epoch:   960     LossContext: 0.00021834
    Epoch:   970     LossContext: 0.00021834
    Epoch:   980     LossContext: 0.00021834
    Epoch:   990     LossContext: 0.00021834
    Epoch:  1000     LossContext: 0.00021834
    Epoch:  1010     LossContext: 0.00021834
    Epoch:  1020     LossContext: 0.00021834
    Epoch:  1030     LossContext: 0.00021834
    Epoch:  1040     LossContext: 0.00021834
    Epoch:  1050     LossContext: 0.00021834
    Epoch:  1060     LossContext: 0.00021834
    Epoch:  1070     LossContext: 0.00021834
    Epoch:  1080     LossContext: 0.00021834
    Epoch:  1090     LossContext: 0.00021834
    Epoch:  1100     LossContext: 0.00021834
    Epoch:  1110     LossContext: 0.00021834
    Epoch:  1120     LossContext: 0.00021834
    Epoch:  1130     LossContext: 0.00021834
    Epoch:  1140     LossContext: 0.00021834
    Epoch:  1150     LossContext: 0.00021834
    Epoch:  1160     LossContext: 0.00021834
    Epoch:  1170     LossContext: 0.00021834
    Epoch:  1180     LossContext: 0.00021834
    Epoch:  1190     LossContext: 0.00021834
    Epoch:  1200     LossContext: 0.00021834
    Epoch:  1210     LossContext: 0.00021834
    Epoch:  1220     LossContext: 0.00021834
    Epoch:  1230     LossContext: 0.00021834
    Epoch:  1240     LossContext: 0.00021834
    Epoch:  1250     LossContext: 0.00021834
    Epoch:  1260     LossContext: 0.00021834
    Epoch:  1270     LossContext: 0.00021834
    Epoch:  1280     LossContext: 0.00021834
    Epoch:  1290     LossContext: 0.00021834
    Epoch:  1300     LossContext: 0.00021834
    Epoch:  1310     LossContext: 0.00021834
    Epoch:  1320     LossContext: 0.00021834
    Epoch:  1330     LossContext: 0.00021834
    Epoch:  1340     LossContext: 0.00021834
    Epoch:  1350     LossContext: 0.00021834
    Epoch:  1360     LossContext: 0.00021834
    Epoch:  1370     LossContext: 0.00021834
    Epoch:  1380     LossContext: 0.00021834
    Epoch:  1390     LossContext: 0.00021834
    Epoch:  1400     LossContext: 0.00021834
    Epoch:  1410     LossContext: 0.00021834
    Epoch:  1420     LossContext: 0.00021834
    Epoch:  1430     LossContext: 0.00021834
    Epoch:  1440     LossContext: 0.00021834
    Epoch:  1450     LossContext: 0.00021834
    Epoch:  1460     LossContext: 0.00021834
    Epoch:  1470     LossContext: 0.00021834
    Epoch:  1480     LossContext: 0.00021834
    Epoch:  1490     LossContext: 0.00021834
    Epoch:  1499     LossContext: 0.00021834

Gradient descent adaptation time: 0 hours 1 mins 33 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.04386384
    Epoch:     1     LossContext: 0.03689199
    Epoch:     2     LossContext: 0.03063339
    Epoch:     3     LossContext: 0.02509645
    Epoch:    10     LossContext: 0.00408063
    Epoch:    20     LossContext: 0.00028091
    Epoch:    30     LossContext: 0.00069987
    Epoch:    40     LossContext: 0.00064221
    Epoch:    50     LossContext: 0.00033090
    Epoch:    60     LossContext: 0.00019335
    Epoch:    70     LossContext: 0.00019146
    Epoch:    80     LossContext: 0.00019351
    Epoch:    90     LossContext: 0.00018641
    Epoch:   100     LossContext: 0.00018523
    Epoch:   110     LossContext: 0.00018549
    Epoch:   120     LossContext: 0.00018521
    Epoch:   130     LossContext: 0.00018515
    Epoch:   140     LossContext: 0.00018516
    Epoch:   150     LossContext: 0.00018514
    Epoch:   160     LossContext: 0.00018514
    Epoch:   170     LossContext: 0.00018514
    Epoch:   180     LossContext: 0.00018514
    Epoch:   190     LossContext: 0.00018514
    Epoch:   200     LossContext: 0.00018514
    Epoch:   210     LossContext: 0.00018514
    Epoch:   220     LossContext: 0.00018514
    Epoch:   230     LossContext: 0.00018514
    Epoch:   240     LossContext: 0.00018514
    Epoch:   250     LossContext: 0.00018514
    Epoch:   260     LossContext: 0.00018514
    Epoch:   270     LossContext: 0.00018514
    Epoch:   280     LossContext: 0.00018514
    Epoch:   290     LossContext: 0.00018514
    Epoch:   300     LossContext: 0.00018514
    Epoch:   310     LossContext: 0.00018514
    Epoch:   320     LossContext: 0.00018514
    Epoch:   330     LossContext: 0.00018514
    Epoch:   340     LossContext: 0.00018514
    Epoch:   350     LossContext: 0.00018514
    Epoch:   360     LossContext: 0.00018514
    Epoch:   370     LossContext: 0.00018514
    Epoch:   380     LossContext: 0.00018514
    Epoch:   390     LossContext: 0.00018514
    Epoch:   400     LossContext: 0.00018514
    Epoch:   410     LossContext: 0.00018514
    Epoch:   420     LossContext: 0.00018514
    Epoch:   430     LossContext: 0.00018514
    Epoch:   440     LossContext: 0.00018514
    Epoch:   450     LossContext: 0.00018514
    Epoch:   460     LossContext: 0.00018514
    Epoch:   470     LossContext: 0.00018514
    Epoch:   480     LossContext: 0.00018514
    Epoch:   490     LossContext: 0.00018514
    Epoch:   500     LossContext: 0.00018514
    Epoch:   510     LossContext: 0.00018514
    Epoch:   520     LossContext: 0.00018514
    Epoch:   530     LossContext: 0.00018514
    Epoch:   540     LossContext: 0.00018514
    Epoch:   550     LossContext: 0.00018514
    Epoch:   560     LossContext: 0.00018514
    Epoch:   570     LossContext: 0.00018514
    Epoch:   580     LossContext: 0.00018514
    Epoch:   590     LossContext: 0.00018514
    Epoch:   600     LossContext: 0.00018514
    Epoch:   610     LossContext: 0.00018514
    Epoch:   620     LossContext: 0.00018514
    Epoch:   630     LossContext: 0.00018514
    Epoch:   640     LossContext: 0.00018514
    Epoch:   650     LossContext: 0.00018514
    Epoch:   660     LossContext: 0.00018514
    Epoch:   670     LossContext: 0.00018514
    Epoch:   680     LossContext: 0.00018514
    Epoch:   690     LossContext: 0.00018514
    Epoch:   700     LossContext: 0.00018514
    Epoch:   710     LossContext: 0.00018514
    Epoch:   720     LossContext: 0.00018514
    Epoch:   730     LossContext: 0.00018514
    Epoch:   740     LossContext: 0.00018514
    Epoch:   750     LossContext: 0.00018514
    Epoch:   760     LossContext: 0.00018514
    Epoch:   770     LossContext: 0.00018514
    Epoch:   780     LossContext: 0.00018514
    Epoch:   790     LossContext: 0.00018514
    Epoch:   800     LossContext: 0.00018514
    Epoch:   810     LossContext: 0.00018514
    Epoch:   820     LossContext: 0.00018514
    Epoch:   830     LossContext: 0.00018514
    Epoch:   840     LossContext: 0.00018514
    Epoch:   850     LossContext: 0.00018514
    Epoch:   860     LossContext: 0.00018514
    Epoch:   870     LossContext: 0.00018514
    Epoch:   880     LossContext: 0.00018514
    Epoch:   890     LossContext: 0.00018514
    Epoch:   900     LossContext: 0.00018514
    Epoch:   910     LossContext: 0.00018514
    Epoch:   920     LossContext: 0.00018514
    Epoch:   930     LossContext: 0.00018514
    Epoch:   940     LossContext: 0.00018514
    Epoch:   950     LossContext: 0.00018514
    Epoch:   960     LossContext: 0.00018514
    Epoch:   970     LossContext: 0.00018514
    Epoch:   980     LossContext: 0.00018514
    Epoch:   990     LossContext: 0.00018514
    Epoch:  1000     LossContext: 0.00018514
    Epoch:  1010     LossContext: 0.00018514
    Epoch:  1020     LossContext: 0.00018514
    Epoch:  1030     LossContext: 0.00018514
    Epoch:  1040     LossContext: 0.00018514
    Epoch:  1050     LossContext: 0.00018514
    Epoch:  1060     LossContext: 0.00018514
    Epoch:  1070     LossContext: 0.00018514
    Epoch:  1080     LossContext: 0.00018514
    Epoch:  1090     LossContext: 0.00018514
    Epoch:  1100     LossContext: 0.00018514
    Epoch:  1110     LossContext: 0.00018514
    Epoch:  1120     LossContext: 0.00018514
    Epoch:  1130     LossContext: 0.00018514
    Epoch:  1140     LossContext: 0.00018514
    Epoch:  1150     LossContext: 0.00018514
    Epoch:  1160     LossContext: 0.00018514
    Epoch:  1170     LossContext: 0.00018514
    Epoch:  1180     LossContext: 0.00018514
    Epoch:  1190     LossContext: 0.00018514
    Epoch:  1200     LossContext: 0.00018514
    Epoch:  1210     LossContext: 0.00018514
    Epoch:  1220     LossContext: 0.00018514
    Epoch:  1230     LossContext: 0.00018514
    Epoch:  1240     LossContext: 0.00018514
    Epoch:  1250     LossContext: 0.00018514
    Epoch:  1260     LossContext: 0.00018514
    Epoch:  1270     LossContext: 0.00018514
    Epoch:  1280     LossContext: 0.00018514
    Epoch:  1290     LossContext: 0.00018514
    Epoch:  1300     LossContext: 0.00018514
    Epoch:  1310     LossContext: 0.00018514
    Epoch:  1320     LossContext: 0.00018514
    Epoch:  1330     LossContext: 0.00018514
    Epoch:  1340     LossContext: 0.00018514
    Epoch:  1350     LossContext: 0.00018514
    Epoch:  1360     LossContext: 0.00018514
    Epoch:  1370     LossContext: 0.00018514
    Epoch:  1380     LossContext: 0.00018514
    Epoch:  1390     LossContext: 0.00018514
    Epoch:  1400     LossContext: 0.00018514
    Epoch:  1410     LossContext: 0.00018514
    Epoch:  1420     LossContext: 0.00018514
    Epoch:  1430     LossContext: 0.00018514
    Epoch:  1440     LossContext: 0.00018514
    Epoch:  1450     LossContext: 0.00018514
    Epoch:  1460     LossContext: 0.00018514
    Epoch:  1470     LossContext: 0.00018514
    Epoch:  1480     LossContext: 0.00018514
    Epoch:  1490     LossContext: 0.00018514
    Epoch:  1499     LossContext: 0.00018514

Gradient descent adaptation time: 0 hours 1 mins 33 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.02771924
    Epoch:     1     LossContext: 0.02319386
    Epoch:     2     LossContext: 0.01947586
    Epoch:     3     LossContext: 0.01643076
    Epoch:    10     LossContext: 0.00373353
    Epoch:    20     LossContext: 0.00056790
    Epoch:    30     LossContext: 0.00049099
    Epoch:    40     LossContext: 0.00055242
    Epoch:    50     LossContext: 0.00050947
    Epoch:    60     LossContext: 0.00044054
    Epoch:    70     LossContext: 0.00040539
    Epoch:    80     LossContext: 0.00039901
    Epoch:    90     LossContext: 0.00039978
    Epoch:   100     LossContext: 0.00039953
    Epoch:   110     LossContext: 0.00039897
    Epoch:   120     LossContext: 0.00039883
    Epoch:   130     LossContext: 0.00039885
    Epoch:   140     LossContext: 0.00039884
    Epoch:   150     LossContext: 0.00039883
    Epoch:   160     LossContext: 0.00039883
    Epoch:   170     LossContext: 0.00039883
    Epoch:   180     LossContext: 0.00039883
    Epoch:   190     LossContext: 0.00039883
    Epoch:   200     LossContext: 0.00039883
    Epoch:   210     LossContext: 0.00039883
    Epoch:   220     LossContext: 0.00039883
    Epoch:   230     LossContext: 0.00039883
    Epoch:   240     LossContext: 0.00039883
    Epoch:   250     LossContext: 0.00039883
    Epoch:   260     LossContext: 0.00039883
    Epoch:   270     LossContext: 0.00039883
    Epoch:   280     LossContext: 0.00039883
    Epoch:   290     LossContext: 0.00039883
    Epoch:   300     LossContext: 0.00039883
    Epoch:   310     LossContext: 0.00039883
    Epoch:   320     LossContext: 0.00039883
    Epoch:   330     LossContext: 0.00039883
    Epoch:   340     LossContext: 0.00039883
    Epoch:   350     LossContext: 0.00039883
    Epoch:   360     LossContext: 0.00039883
    Epoch:   370     LossContext: 0.00039883
    Epoch:   380     LossContext: 0.00039883
    Epoch:   390     LossContext: 0.00039883
    Epoch:   400     LossContext: 0.00039883
    Epoch:   410     LossContext: 0.00039883
    Epoch:   420     LossContext: 0.00039883
    Epoch:   430     LossContext: 0.00039883
    Epoch:   440     LossContext: 0.00039883
    Epoch:   450     LossContext: 0.00039883
    Epoch:   460     LossContext: 0.00039883
    Epoch:   470     LossContext: 0.00039883
    Epoch:   480     LossContext: 0.00039883
    Epoch:   490     LossContext: 0.00039883
    Epoch:   500     LossContext: 0.00039883
    Epoch:   510     LossContext: 0.00039883
    Epoch:   520     LossContext: 0.00039883
    Epoch:   530     LossContext: 0.00039883
    Epoch:   540     LossContext: 0.00039883
    Epoch:   550     LossContext: 0.00039883
    Epoch:   560     LossContext: 0.00039883
    Epoch:   570     LossContext: 0.00039883
    Epoch:   580     LossContext: 0.00039883
    Epoch:   590     LossContext: 0.00039883
    Epoch:   600     LossContext: 0.00039883
    Epoch:   610     LossContext: 0.00039883
    Epoch:   620     LossContext: 0.00039883
    Epoch:   630     LossContext: 0.00039883
    Epoch:   640     LossContext: 0.00039883
    Epoch:   650     LossContext: 0.00039883
    Epoch:   660     LossContext: 0.00039883
    Epoch:   670     LossContext: 0.00039883
    Epoch:   680     LossContext: 0.00039883
    Epoch:   690     LossContext: 0.00039883
    Epoch:   700     LossContext: 0.00039883
    Epoch:   710     LossContext: 0.00039883
    Epoch:   720     LossContext: 0.00039883
    Epoch:   730     LossContext: 0.00039883
    Epoch:   740     LossContext: 0.00039883
    Epoch:   750     LossContext: 0.00039883
    Epoch:   760     LossContext: 0.00039883
    Epoch:   770     LossContext: 0.00039883
    Epoch:   780     LossContext: 0.00039883
    Epoch:   790     LossContext: 0.00039883
    Epoch:   800     LossContext: 0.00039883
    Epoch:   810     LossContext: 0.00039883
    Epoch:   820     LossContext: 0.00039883
    Epoch:   830     LossContext: 0.00039883
    Epoch:   840     LossContext: 0.00039883
    Epoch:   850     LossContext: 0.00039883
    Epoch:   860     LossContext: 0.00039883
    Epoch:   870     LossContext: 0.00039883
    Epoch:   880     LossContext: 0.00039883
    Epoch:   890     LossContext: 0.00039883
    Epoch:   900     LossContext: 0.00039883
    Epoch:   910     LossContext: 0.00039883
    Epoch:   920     LossContext: 0.00039883
    Epoch:   930     LossContext: 0.00039883
    Epoch:   940     LossContext: 0.00039883
    Epoch:   950     LossContext: 0.00039883
    Epoch:   960     LossContext: 0.00039883
    Epoch:   970     LossContext: 0.00039883
    Epoch:   980     LossContext: 0.00039883
    Epoch:   990     LossContext: 0.00039883
    Epoch:  1000     LossContext: 0.00039883
    Epoch:  1010     LossContext: 0.00039883
    Epoch:  1020     LossContext: 0.00039883
    Epoch:  1030     LossContext: 0.00039883
    Epoch:  1040     LossContext: 0.00039883
    Epoch:  1050     LossContext: 0.00039883
    Epoch:  1060     LossContext: 0.00039883
    Epoch:  1070     LossContext: 0.00039883
    Epoch:  1080     LossContext: 0.00039883
    Epoch:  1090     LossContext: 0.00039883
    Epoch:  1100     LossContext: 0.00039883
    Epoch:  1110     LossContext: 0.00039883
    Epoch:  1120     LossContext: 0.00039883
    Epoch:  1130     LossContext: 0.00039883
    Epoch:  1140     LossContext: 0.00039883
    Epoch:  1150     LossContext: 0.00039883
    Epoch:  1160     LossContext: 0.00039883
    Epoch:  1170     LossContext: 0.00039883
    Epoch:  1180     LossContext: 0.00039883
    Epoch:  1190     LossContext: 0.00039883
    Epoch:  1200     LossContext: 0.00039883
    Epoch:  1210     LossContext: 0.00039883
    Epoch:  1220     LossContext: 0.00039883
    Epoch:  1230     LossContext: 0.00039883
    Epoch:  1240     LossContext: 0.00039883
    Epoch:  1250     LossContext: 0.00039883
    Epoch:  1260     LossContext: 0.00039883
    Epoch:  1270     LossContext: 0.00039883
    Epoch:  1280     LossContext: 0.00039883
    Epoch:  1290     LossContext: 0.00039883
    Epoch:  1300     LossContext: 0.00039883
    Epoch:  1310     LossContext: 0.00039883
    Epoch:  1320     LossContext: 0.00039883
    Epoch:  1330     LossContext: 0.00039883
    Epoch:  1340     LossContext: 0.00039883
    Epoch:  1350     LossContext: 0.00039883
    Epoch:  1360     LossContext: 0.00039883
    Epoch:  1370     LossContext: 0.00039883
    Epoch:  1380     LossContext: 0.00039883
    Epoch:  1390     LossContext: 0.00039883
    Epoch:  1400     LossContext: 0.00039883
    Epoch:  1410     LossContext: 0.00039883
    Epoch:  1420     LossContext: 0.00039883
    Epoch:  1430     LossContext: 0.00039883
    Epoch:  1440     LossContext: 0.00039883
    Epoch:  1450     LossContext: 0.00039883
    Epoch:  1460     LossContext: 0.00039883
    Epoch:  1470     LossContext: 0.00039883
    Epoch:  1480     LossContext: 0.00039883
    Epoch:  1490     LossContext: 0.00039883
    Epoch:  1499     LossContext: 0.00039883

Gradient descent adaptation time: 0 hours 1 mins 33 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./05052024-092750/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 0.00026362634

==  Begining out-of-distribution visualisation ... ==
    Environment id: 0
    Trajectory id: 0
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./05052024-092750/adapt/results_ood.png
Testing finished. Figure saved in: ./05052024-092750/adapt/results_ood_uq.pdf

======================= DONE ======================== 

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

========= STARTING =========
Working with a context size of 2^2 = 4
Run folder created successfuly: ./05052024-110926/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 110943
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 110943
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 47120 


WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed


=== Beginning training with proximal alternating minimization ... ===
    Number of examples in a batch: 4
    Maximum number of steps per inner minimization: 25
    Maximum number of outer minimizations: 500
    Maximum total number of training steps: 12500

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Outer Step:     0      LossTrajs: 1.81043029     ContextsNorm: 0.00000000     ValIndCrit: 1.69849944
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.59e-06
        -DiffCxt:  inf
    Outer Step:     1      LossTrajs: 1.51415193     ContextsNorm: 0.00010805     ValIndCrit: 1.41881227
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.54e-06
        -DiffCxt:  3.24e-03
    Outer Step:     2      LossTrajs: 1.17332292     ContextsNorm: 0.00014515     ValIndCrit: 1.09586036
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.38e-06
        -DiffCxt:  2.21e-03
    Outer Step:     3      LossTrajs: 0.67210549     ContextsNorm: 0.00047194     ValIndCrit: 0.63062692
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.22e-06
        -DiffCxt:  1.08e-03
    Outer Step:    10      LossTrajs: 0.25470787     ContextsNorm: 0.00165054     ValIndCrit: 0.30099317
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.19e-07
        -DiffCxt:  2.34e-05
    Outer Step:    20      LossTrajs: 0.17007416     ContextsNorm: 0.00310088     ValIndCrit: 0.18600045
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.25e-07
        -DiffCxt:  1.19e-05
    Outer Step:    30      LossTrajs: 0.08711986     ContextsNorm: 0.00612873     ValIndCrit: 0.09109799
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.89e-08
        -DiffCxt:  5.52e-06
    Outer Step:    40      LossTrajs: 0.07495676     ContextsNorm: 0.01044829     ValIndCrit: 0.07856613
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.88e-08
        -DiffCxt:  3.68e-06
    Outer Step:    50      LossTrajs: 0.06770040     ContextsNorm: 0.02246431     ValIndCrit: 0.07045320
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.15e-07
        -DiffCxt:  4.17e-06
    Outer Step:    60      LossTrajs: 0.04613422     ContextsNorm: 0.03416202     ValIndCrit: 0.04962738
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.14e-08
        -DiffCxt:  3.18e-07
    Outer Step:    70      LossTrajs: 0.03598613     ContextsNorm: 0.03524419     ValIndCrit: 0.04105024
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.64e-08
        -DiffCxt:  1.40e-07
    Outer Step:    80      LossTrajs: 0.03056845     ContextsNorm: 0.03762168     ValIndCrit: 0.03637799
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.40e-08
        -DiffCxt:  1.39e-07
    Outer Step:    90      LossTrajs: 0.02930985     ContextsNorm: 0.03857268     ValIndCrit: 0.03578998
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.73e-09
        -DiffCxt:  6.22e-08
    Outer Step:   100      LossTrajs: 0.02822630     ContextsNorm: 0.03848614     ValIndCrit: 0.03578777
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.87e-08
        -DiffCxt:  9.30e-08
    Outer Step:   110      LossTrajs: 0.02668450     ContextsNorm: 0.04041416     ValIndCrit: 0.03519382
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.18e-08
        -DiffCxt:  1.71e-07
    Outer Step:   120      LossTrajs: 0.02467854     ContextsNorm: 0.04226590     ValIndCrit: 0.03312362
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.21e-08
        -DiffCxt:  2.33e-07
    Outer Step:   130      LossTrajs: 0.01860185     ContextsNorm: 0.04811957     ValIndCrit: 0.02708156
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.61e-07
        -DiffCxt:  1.02e-06
    Outer Step:   140      LossTrajs: 0.00651908     ContextsNorm: 0.04852538     ValIndCrit: 0.01017032
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.06e-08
        -DiffCxt:  1.85e-07
    Outer Step:   150      LossTrajs: 0.00355140     ContextsNorm: 0.04815024     ValIndCrit: 0.00530740
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.44e-08
        -DiffCxt:  1.40e-07
    Outer Step:   160      LossTrajs: 0.00140958     ContextsNorm: 0.04832740     ValIndCrit: 0.00226832
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.19e-08
        -DiffCxt:  3.17e-08
    Outer Step:   170      LossTrajs: 0.00099838     ContextsNorm: 0.04813069     ValIndCrit: 0.00170708
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.43e-09
        -DiffCxt:  1.71e-08
    Outer Step:   180      LossTrajs: 0.00078489     ContextsNorm: 0.04801989     ValIndCrit: 0.00135783
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.42e-09
        -DiffCxt:  3.09e-08
    Outer Step:   190      LossTrajs: 0.00064253     ContextsNorm: 0.04741718     ValIndCrit: 0.00111989
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.30e-09
        -DiffCxt:  4.26e-08
    Outer Step:   200      LossTrajs: 0.00050789     ContextsNorm: 0.04718906     ValIndCrit: 0.00089642
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.26e-09
        -DiffCxt:  1.58e-08
    Outer Step:   210      LossTrajs: 0.00041881     ContextsNorm: 0.04714472     ValIndCrit: 0.00070195
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   24
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.08e-09
        -DiffCxt:  9.90e-09
    Outer Step:   220      LossTrajs: 0.00036677     ContextsNorm: 0.04695657     ValIndCrit: 0.00059807
        Saving best model so far ...
        -NbInnerStepsNode:    4
        -NbInnerStepsCxt:   12
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.38e-10
        -DiffCxt:  7.33e-09
    Outer Step:   230      LossTrajs: 0.00031377     ContextsNorm: 0.04693515     ValIndCrit: 0.00050127
        Saving best model so far ...
        -NbInnerStepsNode:   24
        -NbInnerStepsCxt:   21
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.35e-10
        -DiffCxt:  8.83e-09
    Outer Step:   240      LossTrajs: 0.00029765     ContextsNorm: 0.04688539     ValIndCrit: 0.00045630
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.58e-08
        -DiffCxt:  2.47e-08
    Outer Step:   250      LossTrajs: 0.00023804     ContextsNorm: 0.04691922     ValIndCrit: 0.00038818
        Saving best model so far ...
        -NbInnerStepsNode:   17
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.58e-10
        -DiffCxt:  4.48e-08
    Outer Step:   260      LossTrajs: 0.00022646     ContextsNorm: 0.04677820     ValIndCrit: 0.00035186
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.64e-09
        -DiffCxt:  2.08e-08
    Outer Step:   270      LossTrajs: 0.00021497     ContextsNorm: 0.04675507     ValIndCrit: 0.00032427
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.46e-09
        -DiffCxt:  2.22e-08
    Outer Step:   280      LossTrajs: 0.00020829     ContextsNorm: 0.04677726     ValIndCrit: 0.00028976
        Saving best model so far ...
        -NbInnerStepsNode:   14
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.14e-10
        -DiffCxt:  5.21e-08
    Outer Step:   290      LossTrajs: 0.00017515     ContextsNorm: 0.04663479     ValIndCrit: 0.00028023
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.74e-09
        -DiffCxt:  6.81e-08
    Outer Step:   300      LossTrajs: 0.00015613     ContextsNorm: 0.04647801     ValIndCrit: 0.00024642
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.63e-09
        -DiffCxt:  5.27e-08
    Outer Step:   310      LossTrajs: 0.00015650     ContextsNorm: 0.04629156     ValIndCrit: 0.00022768
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   14
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.46e-10
        -DiffCxt:  9.07e-09
    Outer Step:   320      LossTrajs: 0.00015224     ContextsNorm: 0.04624905     ValIndCrit: 0.00021646
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.23e-08
        -DiffCxt:  8.61e-08
    Outer Step:   330      LossTrajs: 0.00014650     ContextsNorm: 0.04628234     ValIndCrit: 0.00019919
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.86e-09
        -DiffCxt:  3.73e-08
    Outer Step:   340      LossTrajs: 0.00013587     ContextsNorm: 0.04615128     ValIndCrit: 0.00018035
        Saving best model so far ...
        -NbInnerStepsNode:    2
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.23e-10
        -DiffCxt:  9.45e-09
    Outer Step:   350      LossTrajs: 0.00014156     ContextsNorm: 0.04617110     ValIndCrit: 0.00017420
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.16e-09
        -DiffCxt:  3.80e-08
    Outer Step:   360      LossTrajs: 0.00011687     ContextsNorm: 0.04612114     ValIndCrit: 0.00015962
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    7
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.73e-10
        -DiffCxt:  9.21e-09
    Outer Step:   370      LossTrajs: 0.00014666     ContextsNorm: 0.04611736     ValIndCrit: 0.00015671
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.34e-08
        -DiffCxt:  3.66e-08
    Outer Step:   380      LossTrajs: 0.00011805     ContextsNorm: 0.04623451     ValIndCrit: 0.00015751
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.51e-09
        -DiffCxt:  3.93e-08
    Outer Step:   390      LossTrajs: 0.00010851     ContextsNorm: 0.04611554     ValIndCrit: 0.00013872
        Saving best model so far ...
        -NbInnerStepsNode:    3
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.62e-10
        -DiffCxt:  1.10e-08
    Outer Step:   400      LossTrajs: 0.00011226     ContextsNorm: 0.04588396     ValIndCrit: 0.00014309
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.39e-09
        -DiffCxt:  5.63e-08
    Outer Step:   410      LossTrajs: 0.00011459     ContextsNorm: 0.04583319     ValIndCrit: 0.00013145
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.99e-09
        -DiffCxt:  3.34e-08
    Outer Step:   420      LossTrajs: 0.00010748     ContextsNorm: 0.04584833     ValIndCrit: 0.00013183
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.87e-09
        -DiffCxt:  4.75e-08
    Outer Step:   430      LossTrajs: 0.00010167     ContextsNorm: 0.04584850     ValIndCrit: 0.00012048
        Saving best model so far ...
        -NbInnerStepsNode:    2
        -NbInnerStepsCxt:   23
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.87e-10
        -DiffCxt:  7.94e-09
    Outer Step:   440      LossTrajs: 0.00011122     ContextsNorm: 0.04570092     ValIndCrit: 0.00011763
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.43e-08
        -DiffCxt:  2.42e-08
    Outer Step:   450      LossTrajs: 0.00010513     ContextsNorm: 0.04545989     ValIndCrit: 0.00011864
        -NbInnerStepsNode:    6
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.53e-10
        -DiffCxt:  4.83e-08
    Outer Step:   460      LossTrajs: 0.00009671     ContextsNorm: 0.04533369     ValIndCrit: 0.00011331
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   23
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.12e-10
        -DiffCxt:  6.49e-09
    Outer Step:   470      LossTrajs: 0.00010871     ContextsNorm: 0.04534143     ValIndCrit: 0.00011296
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   23
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.24e-09
        -DiffCxt:  7.74e-09
    Outer Step:   480      LossTrajs: 0.00010503     ContextsNorm: 0.04522581     ValIndCrit: 0.00010524
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.41e-08
        -DiffCxt:  1.42e-08
    Outer Step:   490      LossTrajs: 0.00009015     ContextsNorm: 0.04519952     ValIndCrit: 0.00010137
        Saving best model so far ...
        -NbInnerStepsNode:   15
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.44e-10
        -DiffCxt:  1.06e-08
    Outer Step:   499      LossTrajs: 0.00009051     ContextsNorm: 0.04508351     ValIndCrit: 0.00009778
        Saving best model so far ...
        -NbInnerStepsNode:    3
        -NbInnerStepsCxt:   23
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.12e-10
        -DiffCxt:  9.70e-09

Total gradient descent training time: 1 hours 45 mins 52 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 125536
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 9.777795e-05


############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]
==  Begining in-domain visualisation ... ==
    Environment id: 0
    Trajectory id: 5
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./05052024-110926/results_in_domain.png
Testing finished. Figure saved in: ./05052024-110926/results_in_domain_uq.pdf
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.01996643
    Epoch:     1     LossContext: 0.01545664
    Epoch:     2     LossContext: 0.01107727
    Epoch:     3     LossContext: 0.00705447
    Epoch:    10     LossContext: 0.00294975
    Epoch:    20     LossContext: 0.00025552
    Epoch:    30     LossContext: 0.00040792
    Epoch:    40     LossContext: 0.00025819
    Epoch:    50     LossContext: 0.00011011
    Epoch:    60     LossContext: 0.00008050
    Epoch:    70     LossContext: 0.00008046
    Epoch:    80     LossContext: 0.00008047
    Epoch:    90     LossContext: 0.00007960
    Epoch:   100     LossContext: 0.00007885
    Epoch:   110     LossContext: 0.00007847
    Epoch:   120     LossContext: 0.00007825
    Epoch:   130     LossContext: 0.00007807
    Epoch:   140     LossContext: 0.00007791
    Epoch:   150     LossContext: 0.00007774
    Epoch:   160     LossContext: 0.00007758
    Epoch:   170     LossContext: 0.00007745
    Epoch:   180     LossContext: 0.00007736
    Epoch:   190     LossContext: 0.00007727
    Epoch:   200     LossContext: 0.00007718
    Epoch:   210     LossContext: 0.00007708
    Epoch:   220     LossContext: 0.00007699
    Epoch:   230     LossContext: 0.00007689
    Epoch:   240     LossContext: 0.00007678
    Epoch:   250     LossContext: 0.00007668
    Epoch:   260     LossContext: 0.00007657
    Epoch:   270     LossContext: 0.00007647
    Epoch:   280     LossContext: 0.00007635
    Epoch:   290     LossContext: 0.00007624
    Epoch:   300     LossContext: 0.00007612
    Epoch:   310     LossContext: 0.00007601
    Epoch:   320     LossContext: 0.00007589
    Epoch:   330     LossContext: 0.00007576
    Epoch:   340     LossContext: 0.00007568
    Epoch:   350     LossContext: 0.00007562
    Epoch:   360     LossContext: 0.00007556
    Epoch:   370     LossContext: 0.00007549
    Epoch:   380     LossContext: 0.00007542
    Epoch:   390     LossContext: 0.00007536
    Epoch:   400     LossContext: 0.00007529
    Epoch:   410     LossContext: 0.00007522
    Epoch:   420     LossContext: 0.00007515
    Epoch:   430     LossContext: 0.00007508
    Epoch:   440     LossContext: 0.00007500
    Epoch:   450     LossContext: 0.00007493
    Epoch:   460     LossContext: 0.00007485
    Epoch:   470     LossContext: 0.00007478
    Epoch:   480     LossContext: 0.00007470
    Epoch:   490     LossContext: 0.00007462
    Epoch:   500     LossContext: 0.00007455
    Epoch:   510     LossContext: 0.00007447
    Epoch:   520     LossContext: 0.00007439
    Epoch:   530     LossContext: 0.00007430
    Epoch:   540     LossContext: 0.00007422
    Epoch:   550     LossContext: 0.00007414
    Epoch:   560     LossContext: 0.00007405
    Epoch:   570     LossContext: 0.00007397
    Epoch:   580     LossContext: 0.00007388
    Epoch:   590     LossContext: 0.00007379
    Epoch:   600     LossContext: 0.00007370
    Epoch:   610     LossContext: 0.00007362
    Epoch:   620     LossContext: 0.00007353
    Epoch:   630     LossContext: 0.00007343
    Epoch:   640     LossContext: 0.00007334
    Epoch:   650     LossContext: 0.00007325
    Epoch:   660     LossContext: 0.00007315
    Epoch:   670     LossContext: 0.00007306
    Epoch:   680     LossContext: 0.00007296
    Epoch:   690     LossContext: 0.00007287
    Epoch:   700     LossContext: 0.00007277
    Epoch:   710     LossContext: 0.00007267
    Epoch:   720     LossContext: 0.00007257
    Epoch:   730     LossContext: 0.00007247
    Epoch:   740     LossContext: 0.00007237
    Epoch:   750     LossContext: 0.00007226
    Epoch:   760     LossContext: 0.00007216
    Epoch:   770     LossContext: 0.00007205
    Epoch:   780     LossContext: 0.00007195
    Epoch:   790     LossContext: 0.00007184
    Epoch:   800     LossContext: 0.00007173
    Epoch:   810     LossContext: 0.00007163
    Epoch:   820     LossContext: 0.00007152
    Epoch:   830     LossContext: 0.00007141
    Epoch:   840     LossContext: 0.00007129
    Epoch:   850     LossContext: 0.00007118
    Epoch:   860     LossContext: 0.00007107
    Epoch:   870     LossContext: 0.00007095
    Epoch:   880     LossContext: 0.00007084
    Epoch:   890     LossContext: 0.00007072
    Epoch:   900     LossContext: 0.00007061
    Epoch:   910     LossContext: 0.00007049
    Epoch:   920     LossContext: 0.00007037
    Epoch:   930     LossContext: 0.00007025
    Epoch:   940     LossContext: 0.00007013
    Epoch:   950     LossContext: 0.00007001
    Epoch:   960     LossContext: 0.00006988
    Epoch:   970     LossContext: 0.00006976
    Epoch:   980     LossContext: 0.00006963
    Epoch:   990     LossContext: 0.00006951
    Epoch:  1000     LossContext: 0.00006938
    Epoch:  1010     LossContext: 0.00006925
    Epoch:  1020     LossContext: 0.00006913
    Epoch:  1030     LossContext: 0.00006900
    Epoch:  1040     LossContext: 0.00006887
    Epoch:  1050     LossContext: 0.00006873
    Epoch:  1060     LossContext: 0.00006860
    Epoch:  1070     LossContext: 0.00006847
    Epoch:  1080     LossContext: 0.00006833
    Epoch:  1090     LossContext: 0.00006820
    Epoch:  1100     LossContext: 0.00006806
    Epoch:  1110     LossContext: 0.00006792
    Epoch:  1120     LossContext: 0.00006779
    Epoch:  1130     LossContext: 0.00006765
    Epoch:  1140     LossContext: 0.00006751
    Epoch:  1150     LossContext: 0.00006736
    Epoch:  1160     LossContext: 0.00006722
    Epoch:  1170     LossContext: 0.00006708
    Epoch:  1180     LossContext: 0.00006693
    Epoch:  1190     LossContext: 0.00006687
    Epoch:  1200     LossContext: 0.00006685
    Epoch:  1210     LossContext: 0.00006685
    Epoch:  1220     LossContext: 0.00006685
    Epoch:  1230     LossContext: 0.00006685
    Epoch:  1240     LossContext: 0.00006686
    Epoch:  1250     LossContext: 0.00006685
    Epoch:  1260     LossContext: 0.00006685
    Epoch:  1270     LossContext: 0.00006684
    Epoch:  1280     LossContext: 0.00006684
    Epoch:  1290     LossContext: 0.00006685
    Epoch:  1300     LossContext: 0.00006684
    Epoch:  1310     LossContext: 0.00006684
    Epoch:  1320     LossContext: 0.00006684
    Epoch:  1330     LossContext: 0.00006684
    Epoch:  1340     LossContext: 0.00006684
    Epoch:  1350     LossContext: 0.00006684
    Epoch:  1360     LossContext: 0.00006684
    Epoch:  1370     LossContext: 0.00006684
    Epoch:  1380     LossContext: 0.00006684
    Epoch:  1390     LossContext: 0.00006684
    Epoch:  1400     LossContext: 0.00006684
    Epoch:  1410     LossContext: 0.00006684
    Epoch:  1420     LossContext: 0.00006684
    Epoch:  1430     LossContext: 0.00006684
    Epoch:  1440     LossContext: 0.00006684
    Epoch:  1450     LossContext: 0.00006684
    Epoch:  1460     LossContext: 0.00006683
    Epoch:  1470     LossContext: 0.00006683
    Epoch:  1480     LossContext: 0.00006683
    Epoch:  1490     LossContext: 0.00006683
    Epoch:  1499     LossContext: 0.00006683

Gradient descent adaptation time: 0 hours 1 mins 47 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.12451039
    Epoch:     1     LossContext: 0.11383460
    Epoch:     2     LossContext: 0.10256419
    Epoch:     3     LossContext: 0.09077326
    Epoch:    10     LossContext: 0.02086947
    Epoch:    20     LossContext: 0.00219301
    Epoch:    30     LossContext: 0.00133425
    Epoch:    40     LossContext: 0.00081293
    Epoch:    50     LossContext: 0.00062849
    Epoch:    60     LossContext: 0.00040777
    Epoch:    70     LossContext: 0.00024750
    Epoch:    80     LossContext: 0.00020700
    Epoch:    90     LossContext: 0.00020681
    Epoch:   100     LossContext: 0.00020527
    Epoch:   110     LossContext: 0.00020293
    Epoch:   120     LossContext: 0.00020240
    Epoch:   130     LossContext: 0.00020243
    Epoch:   140     LossContext: 0.00020235
    Epoch:   150     LossContext: 0.00020226
    Epoch:   160     LossContext: 0.00020221
    Epoch:   170     LossContext: 0.00020217
    Epoch:   180     LossContext: 0.00020214
    Epoch:   190     LossContext: 0.00020211
    Epoch:   200     LossContext: 0.00020209
    Epoch:   210     LossContext: 0.00020206
    Epoch:   220     LossContext: 0.00020203
    Epoch:   230     LossContext: 0.00020200
    Epoch:   240     LossContext: 0.00020197
    Epoch:   250     LossContext: 0.00020194
    Epoch:   260     LossContext: 0.00020191
    Epoch:   270     LossContext: 0.00020187
    Epoch:   280     LossContext: 0.00020184
    Epoch:   290     LossContext: 0.00020181
    Epoch:   300     LossContext: 0.00020177
    Epoch:   310     LossContext: 0.00020174
    Epoch:   320     LossContext: 0.00020170
    Epoch:   330     LossContext: 0.00020166
    Epoch:   340     LossContext: 0.00020164
    Epoch:   350     LossContext: 0.00020162
    Epoch:   360     LossContext: 0.00020160
    Epoch:   370     LossContext: 0.00020158
    Epoch:   380     LossContext: 0.00020156
    Epoch:   390     LossContext: 0.00020154
    Epoch:   400     LossContext: 0.00020152
    Epoch:   410     LossContext: 0.00020150
    Epoch:   420     LossContext: 0.00020148
    Epoch:   430     LossContext: 0.00020146
    Epoch:   440     LossContext: 0.00020143
    Epoch:   450     LossContext: 0.00020141
    Epoch:   460     LossContext: 0.00020139
    Epoch:   470     LossContext: 0.00020137
    Epoch:   480     LossContext: 0.00020134
    Epoch:   490     LossContext: 0.00020132
    Epoch:   500     LossContext: 0.00020130
    Epoch:   510     LossContext: 0.00020127
    Epoch:   520     LossContext: 0.00020125
    Epoch:   530     LossContext: 0.00020122
    Epoch:   540     LossContext: 0.00020120
    Epoch:   550     LossContext: 0.00020117
    Epoch:   560     LossContext: 0.00020115
    Epoch:   570     LossContext: 0.00020112
    Epoch:   580     LossContext: 0.00020110
    Epoch:   590     LossContext: 0.00020107
    Epoch:   600     LossContext: 0.00020104
    Epoch:   610     LossContext: 0.00020102
    Epoch:   620     LossContext: 0.00020099
    Epoch:   630     LossContext: 0.00020096
    Epoch:   640     LossContext: 0.00020093
    Epoch:   650     LossContext: 0.00020090
    Epoch:   660     LossContext: 0.00020088
    Epoch:   670     LossContext: 0.00020085
    Epoch:   680     LossContext: 0.00020082
    Epoch:   690     LossContext: 0.00020079
    Epoch:   700     LossContext: 0.00020076
    Epoch:   710     LossContext: 0.00020073
    Epoch:   720     LossContext: 0.00020070
    Epoch:   730     LossContext: 0.00020067
    Epoch:   740     LossContext: 0.00020064
    Epoch:   750     LossContext: 0.00020061
    Epoch:   760     LossContext: 0.00020058
    Epoch:   770     LossContext: 0.00020054
    Epoch:   780     LossContext: 0.00020051
    Epoch:   790     LossContext: 0.00020048
    Epoch:   800     LossContext: 0.00020045
    Epoch:   810     LossContext: 0.00020041
    Epoch:   820     LossContext: 0.00020038
    Epoch:   830     LossContext: 0.00020035
    Epoch:   840     LossContext: 0.00020031
    Epoch:   850     LossContext: 0.00020028
    Epoch:   860     LossContext: 0.00020024
    Epoch:   870     LossContext: 0.00020021
    Epoch:   880     LossContext: 0.00020017
    Epoch:   890     LossContext: 0.00020014
    Epoch:   900     LossContext: 0.00020011
    Epoch:   910     LossContext: 0.00020007
    Epoch:   920     LossContext: 0.00020003
    Epoch:   930     LossContext: 0.00020000
    Epoch:   940     LossContext: 0.00019996
    Epoch:   950     LossContext: 0.00019992
    Epoch:   960     LossContext: 0.00019988
    Epoch:   970     LossContext: 0.00019985
    Epoch:   980     LossContext: 0.00019981
    Epoch:   990     LossContext: 0.00019977
    Epoch:  1000     LossContext: 0.00019973
    Epoch:  1010     LossContext: 0.00019969
    Epoch:  1020     LossContext: 0.00019965
    Epoch:  1030     LossContext: 0.00019962
    Epoch:  1040     LossContext: 0.00019958
    Epoch:  1050     LossContext: 0.00019953
    Epoch:  1060     LossContext: 0.00019950
    Epoch:  1070     LossContext: 0.00019945
    Epoch:  1080     LossContext: 0.00019941
    Epoch:  1090     LossContext: 0.00019937
    Epoch:  1100     LossContext: 0.00019933
    Epoch:  1110     LossContext: 0.00019929
    Epoch:  1120     LossContext: 0.00019924
    Epoch:  1130     LossContext: 0.00019920
    Epoch:  1140     LossContext: 0.00019916
    Epoch:  1150     LossContext: 0.00019912
    Epoch:  1160     LossContext: 0.00019907
    Epoch:  1170     LossContext: 0.00019903
    Epoch:  1180     LossContext: 0.00019898
    Epoch:  1190     LossContext: 0.00019894
    Epoch:  1200     LossContext: 0.00019889
    Epoch:  1210     LossContext: 0.00019885
    Epoch:  1220     LossContext: 0.00019881
    Epoch:  1230     LossContext: 0.00019876
    Epoch:  1240     LossContext: 0.00019871
    Epoch:  1250     LossContext: 0.00019867
    Epoch:  1260     LossContext: 0.00019862
    Epoch:  1270     LossContext: 0.00019857
    Epoch:  1280     LossContext: 0.00019852
    Epoch:  1290     LossContext: 0.00019848
    Epoch:  1300     LossContext: 0.00019843
    Epoch:  1310     LossContext: 0.00019838
    Epoch:  1320     LossContext: 0.00019833
    Epoch:  1330     LossContext: 0.00019828
    Epoch:  1340     LossContext: 0.00019823
    Epoch:  1350     LossContext: 0.00019818
    Epoch:  1360     LossContext: 0.00019813
    Epoch:  1370     LossContext: 0.00019808
    Epoch:  1380     LossContext: 0.00019803
    Epoch:  1390     LossContext: 0.00019798
    Epoch:  1400     LossContext: 0.00019793
    Epoch:  1410     LossContext: 0.00019788
    Epoch:  1420     LossContext: 0.00019783
    Epoch:  1430     LossContext: 0.00019777
    Epoch:  1440     LossContext: 0.00019772
    Epoch:  1450     LossContext: 0.00019767
    Epoch:  1460     LossContext: 0.00019761
    Epoch:  1470     LossContext: 0.00019756
    Epoch:  1480     LossContext: 0.00019751
    Epoch:  1490     LossContext: 0.00019745
    Epoch:  1499     LossContext: 0.00019740

Gradient descent adaptation time: 0 hours 1 mins 33 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.04711356
    Epoch:     1     LossContext: 0.04031361
    Epoch:     2     LossContext: 0.03412138
    Epoch:     3     LossContext: 0.02855806
    Epoch:    10     LossContext: 0.00596342
    Epoch:    20     LossContext: 0.00021523
    Epoch:    30     LossContext: 0.00035690
    Epoch:    40     LossContext: 0.00049598
    Epoch:    50     LossContext: 0.00034010
    Epoch:    60     LossContext: 0.00018500
    Epoch:    70     LossContext: 0.00013605
    Epoch:    80     LossContext: 0.00013689
    Epoch:    90     LossContext: 0.00013764
    Epoch:   100     LossContext: 0.00013521
    Epoch:   110     LossContext: 0.00013439
    Epoch:   120     LossContext: 0.00013444
    Epoch:   130     LossContext: 0.00013439
    Epoch:   140     LossContext: 0.00013432
    Epoch:   150     LossContext: 0.00013429
    Epoch:   160     LossContext: 0.00013426
    Epoch:   170     LossContext: 0.00013423
    Epoch:   180     LossContext: 0.00013422
    Epoch:   190     LossContext: 0.00013420
    Epoch:   200     LossContext: 0.00013418
    Epoch:   210     LossContext: 0.00013417
    Epoch:   220     LossContext: 0.00013415
    Epoch:   230     LossContext: 0.00013413
    Epoch:   240     LossContext: 0.00013411
    Epoch:   250     LossContext: 0.00013409
    Epoch:   260     LossContext: 0.00013407
    Epoch:   270     LossContext: 0.00013406
    Epoch:   280     LossContext: 0.00013403
    Epoch:   290     LossContext: 0.00013401
    Epoch:   300     LossContext: 0.00013399
    Epoch:   310     LossContext: 0.00013397
    Epoch:   320     LossContext: 0.00013395
    Epoch:   330     LossContext: 0.00013393
    Epoch:   340     LossContext: 0.00013391
    Epoch:   350     LossContext: 0.00013390
    Epoch:   360     LossContext: 0.00013389
    Epoch:   370     LossContext: 0.00013388
    Epoch:   380     LossContext: 0.00013387
    Epoch:   390     LossContext: 0.00013386
    Epoch:   400     LossContext: 0.00013384
    Epoch:   410     LossContext: 0.00013383
    Epoch:   420     LossContext: 0.00013382
    Epoch:   430     LossContext: 0.00013381
    Epoch:   440     LossContext: 0.00013379
    Epoch:   450     LossContext: 0.00013378
    Epoch:   460     LossContext: 0.00013377
    Epoch:   470     LossContext: 0.00013375
    Epoch:   480     LossContext: 0.00013374
    Epoch:   490     LossContext: 0.00013372
    Epoch:   500     LossContext: 0.00013371
    Epoch:   510     LossContext: 0.00013370
    Epoch:   520     LossContext: 0.00013368
    Epoch:   530     LossContext: 0.00013367
    Epoch:   540     LossContext: 0.00013365
    Epoch:   550     LossContext: 0.00013364
    Epoch:   560     LossContext: 0.00013362
    Epoch:   570     LossContext: 0.00013361
    Epoch:   580     LossContext: 0.00013359
    Epoch:   590     LossContext: 0.00013357
    Epoch:   600     LossContext: 0.00013356
    Epoch:   610     LossContext: 0.00013354
    Epoch:   620     LossContext: 0.00013353
    Epoch:   630     LossContext: 0.00013351
    Epoch:   640     LossContext: 0.00013349
    Epoch:   650     LossContext: 0.00013348
    Epoch:   660     LossContext: 0.00013346
    Epoch:   670     LossContext: 0.00013344
    Epoch:   680     LossContext: 0.00013342
    Epoch:   690     LossContext: 0.00013341
    Epoch:   700     LossContext: 0.00013339
    Epoch:   710     LossContext: 0.00013337
    Epoch:   720     LossContext: 0.00013335
    Epoch:   730     LossContext: 0.00013334
    Epoch:   740     LossContext: 0.00013332
    Epoch:   750     LossContext: 0.00013330
    Epoch:   760     LossContext: 0.00013328
    Epoch:   770     LossContext: 0.00013326
    Epoch:   780     LossContext: 0.00013324
    Epoch:   790     LossContext: 0.00013322
    Epoch:   800     LossContext: 0.00013320
    Epoch:   810     LossContext: 0.00013318
    Epoch:   820     LossContext: 0.00013316
    Epoch:   830     LossContext: 0.00013314
    Epoch:   840     LossContext: 0.00013312
    Epoch:   850     LossContext: 0.00013310
    Epoch:   860     LossContext: 0.00013308
    Epoch:   870     LossContext: 0.00013306
    Epoch:   880     LossContext: 0.00013304
    Epoch:   890     LossContext: 0.00013302
    Epoch:   900     LossContext: 0.00013300
    Epoch:   910     LossContext: 0.00013298
    Epoch:   920     LossContext: 0.00013296
    Epoch:   930     LossContext: 0.00013293
    Epoch:   940     LossContext: 0.00013291
    Epoch:   950     LossContext: 0.00013289
    Epoch:   960     LossContext: 0.00013287
    Epoch:   970     LossContext: 0.00013284
    Epoch:   980     LossContext: 0.00013282
    Epoch:   990     LossContext: 0.00013280
    Epoch:  1000     LossContext: 0.00013278
    Epoch:  1010     LossContext: 0.00013275
    Epoch:  1020     LossContext: 0.00013273
    Epoch:  1030     LossContext: 0.00013271
    Epoch:  1040     LossContext: 0.00013268
    Epoch:  1050     LossContext: 0.00013266
    Epoch:  1060     LossContext: 0.00013263
    Epoch:  1070     LossContext: 0.00013261
    Epoch:  1080     LossContext: 0.00013259
    Epoch:  1090     LossContext: 0.00013256
    Epoch:  1100     LossContext: 0.00013254
    Epoch:  1110     LossContext: 0.00013251
    Epoch:  1120     LossContext: 0.00013249
    Epoch:  1130     LossContext: 0.00013246
    Epoch:  1140     LossContext: 0.00013243
    Epoch:  1150     LossContext: 0.00013241
    Epoch:  1160     LossContext: 0.00013238
    Epoch:  1170     LossContext: 0.00013236
    Epoch:  1180     LossContext: 0.00013233
    Epoch:  1190     LossContext: 0.00013230
    Epoch:  1200     LossContext: 0.00013228
    Epoch:  1210     LossContext: 0.00013225
    Epoch:  1220     LossContext: 0.00013222
    Epoch:  1230     LossContext: 0.00013220
    Epoch:  1240     LossContext: 0.00013217
    Epoch:  1250     LossContext: 0.00013214
    Epoch:  1260     LossContext: 0.00013211
    Epoch:  1270     LossContext: 0.00013208
    Epoch:  1280     LossContext: 0.00013206
    Epoch:  1290     LossContext: 0.00013203
    Epoch:  1300     LossContext: 0.00013200
    Epoch:  1310     LossContext: 0.00013197
    Epoch:  1320     LossContext: 0.00013194
    Epoch:  1330     LossContext: 0.00013191
    Epoch:  1340     LossContext: 0.00013188
    Epoch:  1350     LossContext: 0.00013185
    Epoch:  1360     LossContext: 0.00013182
    Epoch:  1370     LossContext: 0.00013179
    Epoch:  1380     LossContext: 0.00013176
    Epoch:  1390     LossContext: 0.00013173
    Epoch:  1400     LossContext: 0.00013170
    Epoch:  1410     LossContext: 0.00013167
    Epoch:  1420     LossContext: 0.00013164
    Epoch:  1430     LossContext: 0.00013161
    Epoch:  1440     LossContext: 0.00013158
    Epoch:  1450     LossContext: 0.00013155
    Epoch:  1460     LossContext: 0.00013151
    Epoch:  1470     LossContext: 0.00013148
    Epoch:  1480     LossContext: 0.00013145
    Epoch:  1490     LossContext: 0.00013142
    Epoch:  1499     LossContext: 0.00013139

Gradient descent adaptation time: 0 hours 1 mins 34 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.02918768
    Epoch:     1     LossContext: 0.02425969
    Epoch:     2     LossContext: 0.01993799
    Epoch:     3     LossContext: 0.01621814
    Epoch:    10     LossContext: 0.00312637
    Epoch:    20     LossContext: 0.00032015
    Epoch:    30     LossContext: 0.00022351
    Epoch:    40     LossContext: 0.00028255
    Epoch:    50     LossContext: 0.00027475
    Epoch:    60     LossContext: 0.00023953
    Epoch:    70     LossContext: 0.00021069
    Epoch:    80     LossContext: 0.00019809
    Epoch:    90     LossContext: 0.00019503
    Epoch:   100     LossContext: 0.00019487
    Epoch:   110     LossContext: 0.00019484
    Epoch:   120     LossContext: 0.00019462
    Epoch:   130     LossContext: 0.00019441
    Epoch:   140     LossContext: 0.00019425
    Epoch:   150     LossContext: 0.00019410
    Epoch:   160     LossContext: 0.00019395
    Epoch:   170     LossContext: 0.00019382
    Epoch:   180     LossContext: 0.00019374
    Epoch:   190     LossContext: 0.00019366
    Epoch:   200     LossContext: 0.00019357
    Epoch:   210     LossContext: 0.00019348
    Epoch:   220     LossContext: 0.00019339
    Epoch:   230     LossContext: 0.00019330
    Epoch:   240     LossContext: 0.00019320
    Epoch:   250     LossContext: 0.00019310
    Epoch:   260     LossContext: 0.00019300
    Epoch:   270     LossContext: 0.00019290
    Epoch:   280     LossContext: 0.00019280
    Epoch:   290     LossContext: 0.00019269
    Epoch:   300     LossContext: 0.00019258
    Epoch:   310     LossContext: 0.00019247
    Epoch:   320     LossContext: 0.00019236
    Epoch:   330     LossContext: 0.00019224
    Epoch:   340     LossContext: 0.00019217
    Epoch:   350     LossContext: 0.00019211
    Epoch:   360     LossContext: 0.00019205
    Epoch:   370     LossContext: 0.00019199
    Epoch:   380     LossContext: 0.00019193
    Epoch:   390     LossContext: 0.00019186
    Epoch:   400     LossContext: 0.00019180
    Epoch:   410     LossContext: 0.00019173
    Epoch:   420     LossContext: 0.00019167
    Epoch:   430     LossContext: 0.00019160
    Epoch:   440     LossContext: 0.00019153
    Epoch:   450     LossContext: 0.00019146
    Epoch:   460     LossContext: 0.00019139
    Epoch:   470     LossContext: 0.00019132
    Epoch:   480     LossContext: 0.00019125
    Epoch:   490     LossContext: 0.00019118
    Epoch:   500     LossContext: 0.00019110
    Epoch:   510     LossContext: 0.00019103
    Epoch:   520     LossContext: 0.00019095
    Epoch:   530     LossContext: 0.00019088
    Epoch:   540     LossContext: 0.00019080
    Epoch:   550     LossContext: 0.00019072
    Epoch:   560     LossContext: 0.00019064
    Epoch:   570     LossContext: 0.00019056
    Epoch:   580     LossContext: 0.00019048
    Epoch:   590     LossContext: 0.00019040
    Epoch:   600     LossContext: 0.00019031
    Epoch:   610     LossContext: 0.00019023
    Epoch:   620     LossContext: 0.00019015
    Epoch:   630     LossContext: 0.00019006
    Epoch:   640     LossContext: 0.00018997
    Epoch:   650     LossContext: 0.00018989
    Epoch:   660     LossContext: 0.00018980
    Epoch:   670     LossContext: 0.00018971
    Epoch:   680     LossContext: 0.00018962
    Epoch:   690     LossContext: 0.00018953
    Epoch:   700     LossContext: 0.00018944
    Epoch:   710     LossContext: 0.00018934
    Epoch:   720     LossContext: 0.00018925
    Epoch:   730     LossContext: 0.00018916
    Epoch:   740     LossContext: 0.00018906
    Epoch:   750     LossContext: 0.00018896
    Epoch:   760     LossContext: 0.00018887
    Epoch:   770     LossContext: 0.00018877
    Epoch:   780     LossContext: 0.00018867
    Epoch:   790     LossContext: 0.00018857
    Epoch:   800     LossContext: 0.00018847
    Epoch:   810     LossContext: 0.00018837
    Epoch:   820     LossContext: 0.00018827
    Epoch:   830     LossContext: 0.00018816
    Epoch:   840     LossContext: 0.00018806
    Epoch:   850     LossContext: 0.00018795
    Epoch:   860     LossContext: 0.00018785
    Epoch:   870     LossContext: 0.00018774
    Epoch:   880     LossContext: 0.00018763
    Epoch:   890     LossContext: 0.00018753
    Epoch:   900     LossContext: 0.00018742
    Epoch:   910     LossContext: 0.00018731
    Epoch:   920     LossContext: 0.00018719
    Epoch:   930     LossContext: 0.00018708
    Epoch:   940     LossContext: 0.00018697
    Epoch:   950     LossContext: 0.00018686
    Epoch:   960     LossContext: 0.00018674
    Epoch:   970     LossContext: 0.00018663
    Epoch:   980     LossContext: 0.00018651
    Epoch:   990     LossContext: 0.00018639
    Epoch:  1000     LossContext: 0.00018627
    Epoch:  1010     LossContext: 0.00018615
    Epoch:  1020     LossContext: 0.00018603
    Epoch:  1030     LossContext: 0.00018591
    Epoch:  1040     LossContext: 0.00018579
    Epoch:  1050     LossContext: 0.00018567
    Epoch:  1060     LossContext: 0.00018555
    Epoch:  1070     LossContext: 0.00018542
    Epoch:  1080     LossContext: 0.00018530
    Epoch:  1090     LossContext: 0.00018517
    Epoch:  1100     LossContext: 0.00018504
    Epoch:  1110     LossContext: 0.00018491
    Epoch:  1120     LossContext: 0.00018479
    Epoch:  1130     LossContext: 0.00018466
    Epoch:  1140     LossContext: 0.00018452
    Epoch:  1150     LossContext: 0.00018439
    Epoch:  1160     LossContext: 0.00018426
    Epoch:  1170     LossContext: 0.00018413
    Epoch:  1180     LossContext: 0.00018399
    Epoch:  1190     LossContext: 0.00018386
    Epoch:  1200     LossContext: 0.00018372
    Epoch:  1210     LossContext: 0.00018358
    Epoch:  1220     LossContext: 0.00018345
    Epoch:  1230     LossContext: 0.00018331
    Epoch:  1240     LossContext: 0.00018317
    Epoch:  1250     LossContext: 0.00018303
    Epoch:  1260     LossContext: 0.00018288
    Epoch:  1270     LossContext: 0.00018274
    Epoch:  1280     LossContext: 0.00018260
    Epoch:  1290     LossContext: 0.00018245
    Epoch:  1300     LossContext: 0.00018231
    Epoch:  1310     LossContext: 0.00018216
    Epoch:  1320     LossContext: 0.00018202
    Epoch:  1330     LossContext: 0.00018187
    Epoch:  1340     LossContext: 0.00018172
    Epoch:  1350     LossContext: 0.00018157
    Epoch:  1360     LossContext: 0.00018142
    Epoch:  1370     LossContext: 0.00018127
    Epoch:  1380     LossContext: 0.00018111
    Epoch:  1390     LossContext: 0.00018096
    Epoch:  1400     LossContext: 0.00018080
    Epoch:  1410     LossContext: 0.00018065
    Epoch:  1420     LossContext: 0.00018049
    Epoch:  1430     LossContext: 0.00018033
    Epoch:  1440     LossContext: 0.00018018
    Epoch:  1450     LossContext: 0.00018002
    Epoch:  1460     LossContext: 0.00017986
    Epoch:  1470     LossContext: 0.00017970
    Epoch:  1480     LossContext: 0.00017953
    Epoch:  1490     LossContext: 0.00017937
    Epoch:  1499     LossContext: 0.00017922

Gradient descent adaptation time: 0 hours 1 mins 33 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./05052024-110926/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 0.0001521809

