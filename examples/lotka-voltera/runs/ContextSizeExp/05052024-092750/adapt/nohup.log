
############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

========= STARTING =========
Working with a context size of 2^1 = 2
Run folder created successfuly: ./05052024-092750/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 092807
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 092808
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 46608 


WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed


=== Beginning training with proximal alternating minimization ... ===
    Number of examples in a batch: 4
    Maximum number of steps per inner minimization: 25
    Maximum number of outer minimizations: 500
    Maximum total number of training steps: 12500

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Outer Step:     0      LossTrajs: 1.81044996     ContextsNorm: 0.00000000     ValIndCrit: 1.69829845
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.49e-06
        -DiffCxt:  inf
    Outer Step:     1      LossTrajs: 1.50677943     ContextsNorm: 0.00012276     ValIndCrit: 1.41141903
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.46e-06
        -DiffCxt:  3.88e-03
    Outer Step:     2      LossTrajs: 1.14373624     ContextsNorm: 0.00031573     ValIndCrit: 1.06682456
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.25e-06
        -DiffCxt:  2.56e-03
    Outer Step:     3      LossTrajs: 0.59868580     ContextsNorm: 0.00066125     ValIndCrit: 0.56421119
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.90e-06
        -DiffCxt:  7.78e-04
    Outer Step:    10      LossTrajs: 0.25790417     ContextsNorm: 0.00133178     ValIndCrit: 0.30458218
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.26e-07
        -DiffCxt:  5.22e-05
    Outer Step:    20      LossTrajs: 0.16468894     ContextsNorm: 0.00239894     ValIndCrit: 0.17729028
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.07e-07
        -DiffCxt:  2.82e-05
    Outer Step:    30      LossTrajs: 0.08489937     ContextsNorm: 0.00409630     ValIndCrit: 0.08879507
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.17e-08
        -DiffCxt:  7.26e-06
    Outer Step:    40      LossTrajs: 0.07555290     ContextsNorm: 0.00562518     ValIndCrit: 0.07920280
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.22e-09
        -DiffCxt:  4.28e-06
    Outer Step:    50      LossTrajs: 0.07451831     ContextsNorm: 0.00781399     ValIndCrit: 0.07816203
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.48e-09
        -DiffCxt:  5.11e-06
    Outer Step:    60      LossTrajs: 0.07335620     ContextsNorm: 0.01165273     ValIndCrit: 0.07697055
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.73e-08
        -DiffCxt:  6.05e-06
    Outer Step:    70      LossTrajs: 0.06688280     ContextsNorm: 0.02409797     ValIndCrit: 0.06959999
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.93e-07
        -DiffCxt:  3.54e-06
    Outer Step:    80      LossTrajs: 0.03909003     ContextsNorm: 0.03499429     ValIndCrit: 0.04393270
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.81e-08
        -DiffCxt:  1.36e-07
    Outer Step:    90      LossTrajs: 0.03122975     ContextsNorm: 0.03768751     ValIndCrit: 0.03761768
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.72e-08
        -DiffCxt:  1.54e-07
    Outer Step:   100      LossTrajs: 0.02904842     ContextsNorm: 0.03898310     ValIndCrit: 0.03616863
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.76e-09
        -DiffCxt:  4.47e-08
    Outer Step:   110      LossTrajs: 0.02817778     ContextsNorm: 0.03939323     ValIndCrit: 0.03647576
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.02e-09
        -DiffCxt:  1.31e-07
    Outer Step:   120      LossTrajs: 0.02743174     ContextsNorm: 0.03910310     ValIndCrit: 0.03696220
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.76e-08
        -DiffCxt:  1.13e-07
    Outer Step:   130      LossTrajs: 0.02640934     ContextsNorm: 0.04044576     ValIndCrit: 0.03746767
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.10e-08
        -DiffCxt:  3.18e-07
    Outer Step:   140      LossTrajs: 0.02317197     ContextsNorm: 0.04312186     ValIndCrit: 0.03576146
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.57e-07
        -DiffCxt:  5.05e-07
    Outer Step:   150      LossTrajs: 0.00645207     ContextsNorm: 0.04913782     ValIndCrit: 0.01263865
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.96e-08
        -DiffCxt:  1.08e-07
    Outer Step:   160      LossTrajs: 0.00431281     ContextsNorm: 0.04945314     ValIndCrit: 0.00724582
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.71e-08
        -DiffCxt:  5.44e-08
    Outer Step:   170      LossTrajs: 0.00159951     ContextsNorm: 0.04901071     ValIndCrit: 0.00227597
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.33e-08
        -DiffCxt:  7.65e-08
    Outer Step:   180      LossTrajs: 0.00089688     ContextsNorm: 0.04874317     ValIndCrit: 0.00191422
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.79e-09
        -DiffCxt:  8.72e-08
    Outer Step:   190      LossTrajs: 0.00068645     ContextsNorm: 0.04900306     ValIndCrit: 0.00138373
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.94e-08
        -DiffCxt:  1.27e-07
    Outer Step:   200      LossTrajs: 0.00057647     ContextsNorm: 0.04902410     ValIndCrit: 0.00118417
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.62e-09
        -DiffCxt:  1.20e-08
    Outer Step:   210      LossTrajs: 0.00049002     ContextsNorm: 0.04875042     ValIndCrit: 0.00092099
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.41e-09
        -DiffCxt:  3.61e-08
    Outer Step:   220      LossTrajs: 0.00044297     ContextsNorm: 0.04879423     ValIndCrit: 0.00092900
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.90e-09
        -DiffCxt:  1.09e-07
    Outer Step:   230      LossTrajs: 0.00037311     ContextsNorm: 0.04866502     ValIndCrit: 0.00074383
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   12
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.62e-10
        -DiffCxt:  9.98e-09
    Outer Step:   240      LossTrajs: 0.00035597     ContextsNorm: 0.04873635     ValIndCrit: 0.00067592
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   13
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.21e-08
        -DiffCxt:  9.77e-09
    Outer Step:   250      LossTrajs: 0.00031833     ContextsNorm: 0.04841272     ValIndCrit: 0.00061209
        Saving best model so far ...
        -NbInnerStepsNode:    4
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.83e-10
        -DiffCxt:  1.51e-08
    Outer Step:   260      LossTrajs: 0.00030662     ContextsNorm: 0.04833229     ValIndCrit: 0.00052955
        Saving best model so far ...
        -NbInnerStepsNode:   14
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.91e-10
        -DiffCxt:  1.59e-08
    Outer Step:   270      LossTrajs: 0.00029965     ContextsNorm: 0.04822287     ValIndCrit: 0.00054660
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   11
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.91e-10
        -DiffCxt:  5.47e-09
    Outer Step:   280      LossTrajs: 0.00030076     ContextsNorm: 0.04822177     ValIndCrit: 0.00050042
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.12e-08
        -DiffCxt:  2.27e-08
    Outer Step:   290      LossTrajs: 0.00027214     ContextsNorm: 0.04794296     ValIndCrit: 0.00049300
        Saving best model so far ...
        -NbInnerStepsNode:    2
        -NbInnerStepsCxt:   22
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.18e-10
        -DiffCxt:  9.63e-09
    Outer Step:   300      LossTrajs: 0.00025019     ContextsNorm: 0.04803377     ValIndCrit: 0.00043577
        Saving best model so far ...
        -NbInnerStepsNode:   17
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.33e-10
        -DiffCxt:  1.06e-08
    Outer Step:   310      LossTrajs: 0.00023189     ContextsNorm: 0.04801112     ValIndCrit: 0.00042103
        Saving best model so far ...
        -NbInnerStepsNode:   21
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.04e-10
        -DiffCxt:  1.31e-08
    Outer Step:   320      LossTrajs: 0.00023181     ContextsNorm: 0.04783260     ValIndCrit: 0.00039928
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.96e-09
        -DiffCxt:  3.84e-08
    Outer Step:   330      LossTrajs: 0.00022380     ContextsNorm: 0.04773809     ValIndCrit: 0.00038860
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.25e-10
        -DiffCxt:  9.77e-09
    Outer Step:   340      LossTrajs: 0.00022147     ContextsNorm: 0.04770055     ValIndCrit: 0.00035530
        Saving best model so far ...
        -NbInnerStepsNode:   19
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.22e-10
        -DiffCxt:  3.07e-08
    Outer Step:   350      LossTrajs: 0.00023278     ContextsNorm: 0.04811838     ValIndCrit: 0.00035818
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.61e-08
        -DiffCxt:  2.81e-08
    Outer Step:   360      LossTrajs: 0.00019958     ContextsNorm: 0.04808743     ValIndCrit: 0.00032330
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   21
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.16e-09
        -DiffCxt:  7.59e-09
    Outer Step:   370      LossTrajs: 0.00020059     ContextsNorm: 0.04800534     ValIndCrit: 0.00032409
        -NbInnerStepsNode:    2
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.20e-10
        -DiffCxt:  7.82e-09
    Outer Step:   380      LossTrajs: 0.00019171     ContextsNorm: 0.04773479     ValIndCrit: 0.00032134
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   23
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.58e-10
        -DiffCxt:  9.11e-09
    Outer Step:   390      LossTrajs: 0.00018528     ContextsNorm: 0.04791500     ValIndCrit: 0.00030539
        Saving best model so far ...
        -NbInnerStepsNode:   19
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.65e-10
        -DiffCxt:  6.14e-08
    Outer Step:   400      LossTrajs: 0.00017323     ContextsNorm: 0.04770062     ValIndCrit: 0.00029606
        Saving best model so far ...
        -NbInnerStepsNode:    2
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.91e-10
        -DiffCxt:  1.90e-08
    Outer Step:   410      LossTrajs: 0.00017670     ContextsNorm: 0.04773903     ValIndCrit: 0.00030002
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.99e-09
        -DiffCxt:  6.39e-08
    Outer Step:   420      LossTrajs: 0.00017221     ContextsNorm: 0.04776692     ValIndCrit: 0.00029049
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.94e-10
        -DiffCxt:  3.40e-08
    Outer Step:   430      LossTrajs: 0.00017293     ContextsNorm: 0.04792991     ValIndCrit: 0.00026488
        Saving best model so far ...
        -NbInnerStepsNode:   15
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.72e-10
        -DiffCxt:  6.16e-08
    Outer Step:   440      LossTrajs: 0.00014952     ContextsNorm: 0.04806184     ValIndCrit: 0.00023202
        Saving best model so far ...
        -NbInnerStepsNode:    3
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.12e-10
        -DiffCxt:  1.94e-08
    Outer Step:   450      LossTrajs: 0.00016362     ContextsNorm: 0.04786354     ValIndCrit: 0.00023735
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.38e-09
        -DiffCxt:  1.52e-08
    Outer Step:   460      LossTrajs: 0.00015874     ContextsNorm: 0.04795495     ValIndCrit: 0.00023333
        -NbInnerStepsNode:    9
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.27e-10
        -DiffCxt:  1.74e-08
    Outer Step:   470      LossTrajs: 0.00015666     ContextsNorm: 0.04803444     ValIndCrit: 0.00023048
        Saving best model so far ...
        -NbInnerStepsNode:    6
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.39e-10
        -DiffCxt:  2.39e-08
    Outer Step:   480      LossTrajs: 0.00014695     ContextsNorm: 0.04792618     ValIndCrit: 0.00022213
        Saving best model so far ...
        -NbInnerStepsNode:    2
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.51e-10
        -DiffCxt:  9.28e-09
    Outer Step:   490      LossTrajs: 0.00014976     ContextsNorm: 0.04834351     ValIndCrit: 0.00021246
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.80e-09
        -DiffCxt:  3.38e-08
    Outer Step:   499      LossTrajs: 0.00014572     ContextsNorm: 0.04845441     ValIndCrit: 0.00021431
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.51e-10
        -DiffCxt:  1.93e-08

Total gradient descent training time: 1 hours 34 mins 28 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 110237
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 0.00021246144


############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]

############# Neural Context Flow #############

Jax version: 0.4.26
Available devices: [cuda(id=0)]
==  Begining in-domain visualisation ... ==
    Environment id: 0
    Trajectory id: 17
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./05052024-092750/results_in_domain.png
Testing finished. Figure saved in: ./05052024-092750/results_in_domain_uq.pdf
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.02089248
    Epoch:     1     LossContext: 0.01652092
    Epoch:     2     LossContext: 0.01244161
    Epoch:     3     LossContext: 0.00897422
    Epoch:    10     LossContext: 0.00064107
    Epoch:    20     LossContext: 0.00045005
    Epoch:    30     LossContext: 0.00058595
    Epoch:    40     LossContext: 0.00014889
    Epoch:    50     LossContext: 0.00011752
    Epoch:    60     LossContext: 0.00012547
    Epoch:    70     LossContext: 0.00011250
    Epoch:    80     LossContext: 0.00010559
    Epoch:    90     LossContext: 0.00010393
    Epoch:   100     LossContext: 0.00010349
    Epoch:   110     LossContext: 0.00010349
    Epoch:   120     LossContext: 0.00010348
    Epoch:   130     LossContext: 0.00010348
    Epoch:   140     LossContext: 0.00010347
    Epoch:   150     LossContext: 0.00010347
    Epoch:   160     LossContext: 0.00010347
    Epoch:   170     LossContext: 0.00010347
    Epoch:   180     LossContext: 0.00010347
    Epoch:   190     LossContext: 0.00010347
    Epoch:   200     LossContext: 0.00010347
    Epoch:   210     LossContext: 0.00010347
    Epoch:   220     LossContext: 0.00010347
    Epoch:   230     LossContext: 0.00010347
    Epoch:   240     LossContext: 0.00010347
    Epoch:   250     LossContext: 0.00010347
    Epoch:   260     LossContext: 0.00010347
    Epoch:   270     LossContext: 0.00010347
    Epoch:   280     LossContext: 0.00010347
    Epoch:   290     LossContext: 0.00010347
    Epoch:   300     LossContext: 0.00010347
    Epoch:   310     LossContext: 0.00010347
    Epoch:   320     LossContext: 0.00010347
    Epoch:   330     LossContext: 0.00010347
    Epoch:   340     LossContext: 0.00010347
    Epoch:   350     LossContext: 0.00010347
    Epoch:   360     LossContext: 0.00010347
    Epoch:   370     LossContext: 0.00010347
    Epoch:   380     LossContext: 0.00010347
    Epoch:   390     LossContext: 0.00010347
    Epoch:   400     LossContext: 0.00010347
    Epoch:   410     LossContext: 0.00010347
    Epoch:   420     LossContext: 0.00010347
    Epoch:   430     LossContext: 0.00010347
    Epoch:   440     LossContext: 0.00010347
    Epoch:   450     LossContext: 0.00010347
    Epoch:   460     LossContext: 0.00010347
    Epoch:   470     LossContext: 0.00010347
    Epoch:   480     LossContext: 0.00010347
    Epoch:   490     LossContext: 0.00010347
    Epoch:   500     LossContext: 0.00010347
    Epoch:   510     LossContext: 0.00010347
    Epoch:   520     LossContext: 0.00010347
    Epoch:   530     LossContext: 0.00010347
    Epoch:   540     LossContext: 0.00010347
    Epoch:   550     LossContext: 0.00010347
    Epoch:   560     LossContext: 0.00010347
    Epoch:   570     LossContext: 0.00010347
    Epoch:   580     LossContext: 0.00010347
    Epoch:   590     LossContext: 0.00010347
    Epoch:   600     LossContext: 0.00010347
    Epoch:   610     LossContext: 0.00010347
    Epoch:   620     LossContext: 0.00010347
    Epoch:   630     LossContext: 0.00010347
    Epoch:   640     LossContext: 0.00010347
    Epoch:   650     LossContext: 0.00010347
    Epoch:   660     LossContext: 0.00010347
    Epoch:   670     LossContext: 0.00010347
    Epoch:   680     LossContext: 0.00010347
    Epoch:   690     LossContext: 0.00010347
    Epoch:   700     LossContext: 0.00010347
    Epoch:   710     LossContext: 0.00010347
    Epoch:   720     LossContext: 0.00010347
    Epoch:   730     LossContext: 0.00010347
    Epoch:   740     LossContext: 0.00010347
    Epoch:   750     LossContext: 0.00010347
    Epoch:   760     LossContext: 0.00010347
    Epoch:   770     LossContext: 0.00010347
    Epoch:   780     LossContext: 0.00010347
    Epoch:   790     LossContext: 0.00010347
    Epoch:   800     LossContext: 0.00010347
    Epoch:   810     LossContext: 0.00010347
    Epoch:   820     LossContext: 0.00010347
    Epoch:   830     LossContext: 0.00010347
    Epoch:   840     LossContext: 0.00010347
    Epoch:   850     LossContext: 0.00010347
    Epoch:   860     LossContext: 0.00010347
    Epoch:   870     LossContext: 0.00010347
    Epoch:   880     LossContext: 0.00010347
    Epoch:   890     LossContext: 0.00010347
    Epoch:   900     LossContext: 0.00010347
    Epoch:   910     LossContext: 0.00010347
    Epoch:   920     LossContext: 0.00010347
    Epoch:   930     LossContext: 0.00010347
    Epoch:   940     LossContext: 0.00010347
    Epoch:   950     LossContext: 0.00010347
    Epoch:   960     LossContext: 0.00010347
    Epoch:   970     LossContext: 0.00010347
    Epoch:   980     LossContext: 0.00010347
    Epoch:   990     LossContext: 0.00010347
    Epoch:  1000     LossContext: 0.00010347
    Epoch:  1010     LossContext: 0.00010347
    Epoch:  1020     LossContext: 0.00010347
    Epoch:  1030     LossContext: 0.00010347
    Epoch:  1040     LossContext: 0.00010347
    Epoch:  1050     LossContext: 0.00010347
    Epoch:  1060     LossContext: 0.00010347
    Epoch:  1070     LossContext: 0.00010347
    Epoch:  1080     LossContext: 0.00010347
    Epoch:  1090     LossContext: 0.00010347
    Epoch:  1100     LossContext: 0.00010347
    Epoch:  1110     LossContext: 0.00010347
    Epoch:  1120     LossContext: 0.00010347
    Epoch:  1130     LossContext: 0.00010347
    Epoch:  1140     LossContext: 0.00010347
    Epoch:  1150     LossContext: 0.00010347
    Epoch:  1160     LossContext: 0.00010347
    Epoch:  1170     LossContext: 0.00010347
    Epoch:  1180     LossContext: 0.00010347
    Epoch:  1190     LossContext: 0.00010347
    Epoch:  1200     LossContext: 0.00010347
    Epoch:  1210     LossContext: 0.00010347
    Epoch:  1220     LossContext: 0.00010347
    Epoch:  1230     LossContext: 0.00010347
    Epoch:  1240     LossContext: 0.00010347
    Epoch:  1250     LossContext: 0.00010347
    Epoch:  1260     LossContext: 0.00010347
    Epoch:  1270     LossContext: 0.00010347
    Epoch:  1280     LossContext: 0.00010347
    Epoch:  1290     LossContext: 0.00010347
    Epoch:  1300     LossContext: 0.00010347
    Epoch:  1310     LossContext: 0.00010347
    Epoch:  1320     LossContext: 0.00010347
    Epoch:  1330     LossContext: 0.00010347
    Epoch:  1340     LossContext: 0.00010347
    Epoch:  1350     LossContext: 0.00010347
    Epoch:  1360     LossContext: 0.00010347
    Epoch:  1370     LossContext: 0.00010347
    Epoch:  1380     LossContext: 0.00010347
    Epoch:  1390     LossContext: 0.00010347
    Epoch:  1400     LossContext: 0.00010347
    Epoch:  1410     LossContext: 0.00010347
    Epoch:  1420     LossContext: 0.00010347
    Epoch:  1430     LossContext: 0.00010347
    Epoch:  1440     LossContext: 0.00010347
    Epoch:  1450     LossContext: 0.00010347
    Epoch:  1460     LossContext: 0.00010347
    Epoch:  1470     LossContext: 0.00010347
    Epoch:  1480     LossContext: 0.00010347
    Epoch:  1490     LossContext: 0.00010347
    Epoch:  1499     LossContext: 0.00010347

Gradient descent adaptation time: 0 hours 1 mins 47 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.12934403
    Epoch:     1     LossContext: 0.11780426
    Epoch:     2     LossContext: 0.10555889
    Epoch:     3     LossContext: 0.09268703
    Epoch:    10     LossContext: 0.02202835
    Epoch:    20     LossContext: 0.00224022
    Epoch:    30     LossContext: 0.00129920
    Epoch:    40     LossContext: 0.00079673
    Epoch:    50     LossContext: 0.00052119
    Epoch:    60     LossContext: 0.00039112
    Epoch:    70     LossContext: 0.00028016
    Epoch:    80     LossContext: 0.00023065
    Epoch:    90     LossContext: 0.00022134
    Epoch:   100     LossContext: 0.00022064
    Epoch:   110     LossContext: 0.00021942
    Epoch:   120     LossContext: 0.00021855
    Epoch:   130     LossContext: 0.00021836
    Epoch:   140     LossContext: 0.00021836
    Epoch:   150     LossContext: 0.00021835
    Epoch:   160     LossContext: 0.00021834
    Epoch:   170     LossContext: 0.00021834
    Epoch:   180     LossContext: 0.00021834
    Epoch:   190     LossContext: 0.00021834
    Epoch:   200     LossContext: 0.00021834
    Epoch:   210     LossContext: 0.00021834
    Epoch:   220     LossContext: 0.00021834
    Epoch:   230     LossContext: 0.00021834
    Epoch:   240     LossContext: 0.00021834
    Epoch:   250     LossContext: 0.00021834
    Epoch:   260     LossContext: 0.00021834
    Epoch:   270     LossContext: 0.00021834
    Epoch:   280     LossContext: 0.00021834
    Epoch:   290     LossContext: 0.00021834
    Epoch:   300     LossContext: 0.00021834
    Epoch:   310     LossContext: 0.00021834
    Epoch:   320     LossContext: 0.00021834
    Epoch:   330     LossContext: 0.00021834
    Epoch:   340     LossContext: 0.00021834
    Epoch:   350     LossContext: 0.00021834
    Epoch:   360     LossContext: 0.00021834
    Epoch:   370     LossContext: 0.00021834
    Epoch:   380     LossContext: 0.00021834
    Epoch:   390     LossContext: 0.00021834
    Epoch:   400     LossContext: 0.00021834
    Epoch:   410     LossContext: 0.00021834
    Epoch:   420     LossContext: 0.00021834
    Epoch:   430     LossContext: 0.00021834
    Epoch:   440     LossContext: 0.00021834
    Epoch:   450     LossContext: 0.00021834
    Epoch:   460     LossContext: 0.00021834
    Epoch:   470     LossContext: 0.00021834
    Epoch:   480     LossContext: 0.00021834
    Epoch:   490     LossContext: 0.00021834
    Epoch:   500     LossContext: 0.00021834
    Epoch:   510     LossContext: 0.00021834
    Epoch:   520     LossContext: 0.00021834
    Epoch:   530     LossContext: 0.00021834
    Epoch:   540     LossContext: 0.00021834
    Epoch:   550     LossContext: 0.00021834
    Epoch:   560     LossContext: 0.00021834
    Epoch:   570     LossContext: 0.00021834
    Epoch:   580     LossContext: 0.00021834
    Epoch:   590     LossContext: 0.00021834
    Epoch:   600     LossContext: 0.00021834
    Epoch:   610     LossContext: 0.00021834
    Epoch:   620     LossContext: 0.00021834
    Epoch:   630     LossContext: 0.00021834
    Epoch:   640     LossContext: 0.00021834
    Epoch:   650     LossContext: 0.00021834
    Epoch:   660     LossContext: 0.00021834
    Epoch:   670     LossContext: 0.00021834
    Epoch:   680     LossContext: 0.00021834
    Epoch:   690     LossContext: 0.00021834
    Epoch:   700     LossContext: 0.00021834
    Epoch:   710     LossContext: 0.00021834
    Epoch:   720     LossContext: 0.00021834
    Epoch:   730     LossContext: 0.00021834
    Epoch:   740     LossContext: 0.00021834
    Epoch:   750     LossContext: 0.00021834
    Epoch:   760     LossContext: 0.00021834
    Epoch:   770     LossContext: 0.00021834
    Epoch:   780     LossContext: 0.00021834
    Epoch:   790     LossContext: 0.00021834
    Epoch:   800     LossContext: 0.00021834
    Epoch:   810     LossContext: 0.00021834
    Epoch:   820     LossContext: 0.00021834
    Epoch:   830     LossContext: 0.00021834
    Epoch:   840     LossContext: 0.00021834
    Epoch:   850     LossContext: 0.00021834
    Epoch:   860     LossContext: 0.00021834
    Epoch:   870     LossContext: 0.00021834
    Epoch:   880     LossContext: 0.00021834
    Epoch:   890     LossContext: 0.00021834
    Epoch:   900     LossContext: 0.00021834
    Epoch:   910     LossContext: 0.00021834
    Epoch:   920     LossContext: 0.00021834
    Epoch:   930     LossContext: 0.00021834
    Epoch:   940     LossContext: 0.00021834
    Epoch:   950     LossContext: 0.00021834
    Epoch:   960     LossContext: 0.00021834
    Epoch:   970     LossContext: 0.00021834
    Epoch:   980     LossContext: 0.00021834
    Epoch:   990     LossContext: 0.00021834
    Epoch:  1000     LossContext: 0.00021834
    Epoch:  1010     LossContext: 0.00021834
    Epoch:  1020     LossContext: 0.00021834
    Epoch:  1030     LossContext: 0.00021834
    Epoch:  1040     LossContext: 0.00021834
    Epoch:  1050     LossContext: 0.00021834
    Epoch:  1060     LossContext: 0.00021834
    Epoch:  1070     LossContext: 0.00021834
    Epoch:  1080     LossContext: 0.00021834
    Epoch:  1090     LossContext: 0.00021834
    Epoch:  1100     LossContext: 0.00021834
    Epoch:  1110     LossContext: 0.00021834
    Epoch:  1120     LossContext: 0.00021834
    Epoch:  1130     LossContext: 0.00021834
    Epoch:  1140     LossContext: 0.00021834
    Epoch:  1150     LossContext: 0.00021834
    Epoch:  1160     LossContext: 0.00021834
    Epoch:  1170     LossContext: 0.00021834
    Epoch:  1180     LossContext: 0.00021834
    Epoch:  1190     LossContext: 0.00021834
    Epoch:  1200     LossContext: 0.00021834
    Epoch:  1210     LossContext: 0.00021834
    Epoch:  1220     LossContext: 0.00021834
    Epoch:  1230     LossContext: 0.00021834
    Epoch:  1240     LossContext: 0.00021834
    Epoch:  1250     LossContext: 0.00021834
    Epoch:  1260     LossContext: 0.00021834
    Epoch:  1270     LossContext: 0.00021834
    Epoch:  1280     LossContext: 0.00021834
    Epoch:  1290     LossContext: 0.00021834
    Epoch:  1300     LossContext: 0.00021834
    Epoch:  1310     LossContext: 0.00021834
    Epoch:  1320     LossContext: 0.00021834
    Epoch:  1330     LossContext: 0.00021834
    Epoch:  1340     LossContext: 0.00021834
    Epoch:  1350     LossContext: 0.00021834
    Epoch:  1360     LossContext: 0.00021834
    Epoch:  1370     LossContext: 0.00021834
    Epoch:  1380     LossContext: 0.00021834
    Epoch:  1390     LossContext: 0.00021834
    Epoch:  1400     LossContext: 0.00021834
    Epoch:  1410     LossContext: 0.00021834
    Epoch:  1420     LossContext: 0.00021834
    Epoch:  1430     LossContext: 0.00021834
    Epoch:  1440     LossContext: 0.00021834
    Epoch:  1450     LossContext: 0.00021834
    Epoch:  1460     LossContext: 0.00021834
    Epoch:  1470     LossContext: 0.00021834
    Epoch:  1480     LossContext: 0.00021834
    Epoch:  1490     LossContext: 0.00021834
    Epoch:  1499     LossContext: 0.00021834

Gradient descent adaptation time: 0 hours 1 mins 33 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.04386384
    Epoch:     1     LossContext: 0.03689199
    Epoch:     2     LossContext: 0.03063339
    Epoch:     3     LossContext: 0.02509645
    Epoch:    10     LossContext: 0.00408063
    Epoch:    20     LossContext: 0.00028091
    Epoch:    30     LossContext: 0.00069987
    Epoch:    40     LossContext: 0.00064221
    Epoch:    50     LossContext: 0.00033090
    Epoch:    60     LossContext: 0.00019335
    Epoch:    70     LossContext: 0.00019146
    Epoch:    80     LossContext: 0.00019351
    Epoch:    90     LossContext: 0.00018641
    Epoch:   100     LossContext: 0.00018523
    Epoch:   110     LossContext: 0.00018549
    Epoch:   120     LossContext: 0.00018521
    Epoch:   130     LossContext: 0.00018515
    Epoch:   140     LossContext: 0.00018516
    Epoch:   150     LossContext: 0.00018514
    Epoch:   160     LossContext: 0.00018514
    Epoch:   170     LossContext: 0.00018514
    Epoch:   180     LossContext: 0.00018514
    Epoch:   190     LossContext: 0.00018514
    Epoch:   200     LossContext: 0.00018514
    Epoch:   210     LossContext: 0.00018514
    Epoch:   220     LossContext: 0.00018514
    Epoch:   230     LossContext: 0.00018514
    Epoch:   240     LossContext: 0.00018514
    Epoch:   250     LossContext: 0.00018514
    Epoch:   260     LossContext: 0.00018514
    Epoch:   270     LossContext: 0.00018514
    Epoch:   280     LossContext: 0.00018514
    Epoch:   290     LossContext: 0.00018514
    Epoch:   300     LossContext: 0.00018514
    Epoch:   310     LossContext: 0.00018514
    Epoch:   320     LossContext: 0.00018514
    Epoch:   330     LossContext: 0.00018514
    Epoch:   340     LossContext: 0.00018514
    Epoch:   350     LossContext: 0.00018514
    Epoch:   360     LossContext: 0.00018514
    Epoch:   370     LossContext: 0.00018514
    Epoch:   380     LossContext: 0.00018514
    Epoch:   390     LossContext: 0.00018514
    Epoch:   400     LossContext: 0.00018514
    Epoch:   410     LossContext: 0.00018514
    Epoch:   420     LossContext: 0.00018514
    Epoch:   430     LossContext: 0.00018514
    Epoch:   440     LossContext: 0.00018514
    Epoch:   450     LossContext: 0.00018514
    Epoch:   460     LossContext: 0.00018514
    Epoch:   470     LossContext: 0.00018514
    Epoch:   480     LossContext: 0.00018514
    Epoch:   490     LossContext: 0.00018514
    Epoch:   500     LossContext: 0.00018514
    Epoch:   510     LossContext: 0.00018514
    Epoch:   520     LossContext: 0.00018514
    Epoch:   530     LossContext: 0.00018514
    Epoch:   540     LossContext: 0.00018514
    Epoch:   550     LossContext: 0.00018514
    Epoch:   560     LossContext: 0.00018514
    Epoch:   570     LossContext: 0.00018514
    Epoch:   580     LossContext: 0.00018514
    Epoch:   590     LossContext: 0.00018514
    Epoch:   600     LossContext: 0.00018514
    Epoch:   610     LossContext: 0.00018514
    Epoch:   620     LossContext: 0.00018514
    Epoch:   630     LossContext: 0.00018514
    Epoch:   640     LossContext: 0.00018514
    Epoch:   650     LossContext: 0.00018514
    Epoch:   660     LossContext: 0.00018514
    Epoch:   670     LossContext: 0.00018514
    Epoch:   680     LossContext: 0.00018514
    Epoch:   690     LossContext: 0.00018514
    Epoch:   700     LossContext: 0.00018514
    Epoch:   710     LossContext: 0.00018514
    Epoch:   720     LossContext: 0.00018514
    Epoch:   730     LossContext: 0.00018514
    Epoch:   740     LossContext: 0.00018514
    Epoch:   750     LossContext: 0.00018514
    Epoch:   760     LossContext: 0.00018514
    Epoch:   770     LossContext: 0.00018514
    Epoch:   780     LossContext: 0.00018514
    Epoch:   790     LossContext: 0.00018514
    Epoch:   800     LossContext: 0.00018514
    Epoch:   810     LossContext: 0.00018514
    Epoch:   820     LossContext: 0.00018514
    Epoch:   830     LossContext: 0.00018514
    Epoch:   840     LossContext: 0.00018514
    Epoch:   850     LossContext: 0.00018514
    Epoch:   860     LossContext: 0.00018514
    Epoch:   870     LossContext: 0.00018514
    Epoch:   880     LossContext: 0.00018514
    Epoch:   890     LossContext: 0.00018514
    Epoch:   900     LossContext: 0.00018514
    Epoch:   910     LossContext: 0.00018514
    Epoch:   920     LossContext: 0.00018514
    Epoch:   930     LossContext: 0.00018514
    Epoch:   940     LossContext: 0.00018514
    Epoch:   950     LossContext: 0.00018514
    Epoch:   960     LossContext: 0.00018514
    Epoch:   970     LossContext: 0.00018514
    Epoch:   980     LossContext: 0.00018514
    Epoch:   990     LossContext: 0.00018514
    Epoch:  1000     LossContext: 0.00018514
    Epoch:  1010     LossContext: 0.00018514
    Epoch:  1020     LossContext: 0.00018514
    Epoch:  1030     LossContext: 0.00018514
    Epoch:  1040     LossContext: 0.00018514
    Epoch:  1050     LossContext: 0.00018514
    Epoch:  1060     LossContext: 0.00018514
    Epoch:  1070     LossContext: 0.00018514
    Epoch:  1080     LossContext: 0.00018514
    Epoch:  1090     LossContext: 0.00018514
    Epoch:  1100     LossContext: 0.00018514
    Epoch:  1110     LossContext: 0.00018514
    Epoch:  1120     LossContext: 0.00018514
    Epoch:  1130     LossContext: 0.00018514
    Epoch:  1140     LossContext: 0.00018514
    Epoch:  1150     LossContext: 0.00018514
    Epoch:  1160     LossContext: 0.00018514
    Epoch:  1170     LossContext: 0.00018514
    Epoch:  1180     LossContext: 0.00018514
    Epoch:  1190     LossContext: 0.00018514
    Epoch:  1200     LossContext: 0.00018514
    Epoch:  1210     LossContext: 0.00018514
    Epoch:  1220     LossContext: 0.00018514
    Epoch:  1230     LossContext: 0.00018514
    Epoch:  1240     LossContext: 0.00018514
    Epoch:  1250     LossContext: 0.00018514
    Epoch:  1260     LossContext: 0.00018514
    Epoch:  1270     LossContext: 0.00018514
    Epoch:  1280     LossContext: 0.00018514
    Epoch:  1290     LossContext: 0.00018514
    Epoch:  1300     LossContext: 0.00018514
    Epoch:  1310     LossContext: 0.00018514
    Epoch:  1320     LossContext: 0.00018514
    Epoch:  1330     LossContext: 0.00018514
    Epoch:  1340     LossContext: 0.00018514
    Epoch:  1350     LossContext: 0.00018514
    Epoch:  1360     LossContext: 0.00018514
    Epoch:  1370     LossContext: 0.00018514
    Epoch:  1380     LossContext: 0.00018514
    Epoch:  1390     LossContext: 0.00018514
    Epoch:  1400     LossContext: 0.00018514
    Epoch:  1410     LossContext: 0.00018514
    Epoch:  1420     LossContext: 0.00018514
    Epoch:  1430     LossContext: 0.00018514
    Epoch:  1440     LossContext: 0.00018514
    Epoch:  1450     LossContext: 0.00018514
    Epoch:  1460     LossContext: 0.00018514
    Epoch:  1470     LossContext: 0.00018514
    Epoch:  1480     LossContext: 0.00018514
    Epoch:  1490     LossContext: 0.00018514
    Epoch:  1499     LossContext: 0.00018514

Gradient descent adaptation time: 0 hours 1 mins 33 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.02771924
    Epoch:     1     LossContext: 0.02319386
    Epoch:     2     LossContext: 0.01947586
    Epoch:     3     LossContext: 0.01643076
    Epoch:    10     LossContext: 0.00373353
    Epoch:    20     LossContext: 0.00056790
    Epoch:    30     LossContext: 0.00049099
    Epoch:    40     LossContext: 0.00055242
    Epoch:    50     LossContext: 0.00050947
    Epoch:    60     LossContext: 0.00044054
    Epoch:    70     LossContext: 0.00040539
    Epoch:    80     LossContext: 0.00039901
    Epoch:    90     LossContext: 0.00039978
    Epoch:   100     LossContext: 0.00039953
    Epoch:   110     LossContext: 0.00039897
    Epoch:   120     LossContext: 0.00039883
    Epoch:   130     LossContext: 0.00039885
    Epoch:   140     LossContext: 0.00039884
    Epoch:   150     LossContext: 0.00039883
    Epoch:   160     LossContext: 0.00039883
    Epoch:   170     LossContext: 0.00039883
    Epoch:   180     LossContext: 0.00039883
    Epoch:   190     LossContext: 0.00039883
    Epoch:   200     LossContext: 0.00039883
    Epoch:   210     LossContext: 0.00039883
    Epoch:   220     LossContext: 0.00039883
    Epoch:   230     LossContext: 0.00039883
    Epoch:   240     LossContext: 0.00039883
    Epoch:   250     LossContext: 0.00039883
    Epoch:   260     LossContext: 0.00039883
    Epoch:   270     LossContext: 0.00039883
    Epoch:   280     LossContext: 0.00039883
    Epoch:   290     LossContext: 0.00039883
    Epoch:   300     LossContext: 0.00039883
    Epoch:   310     LossContext: 0.00039883
    Epoch:   320     LossContext: 0.00039883
    Epoch:   330     LossContext: 0.00039883
    Epoch:   340     LossContext: 0.00039883
    Epoch:   350     LossContext: 0.00039883
    Epoch:   360     LossContext: 0.00039883
    Epoch:   370     LossContext: 0.00039883
    Epoch:   380     LossContext: 0.00039883
    Epoch:   390     LossContext: 0.00039883
    Epoch:   400     LossContext: 0.00039883
    Epoch:   410     LossContext: 0.00039883
    Epoch:   420     LossContext: 0.00039883
    Epoch:   430     LossContext: 0.00039883
    Epoch:   440     LossContext: 0.00039883
    Epoch:   450     LossContext: 0.00039883
    Epoch:   460     LossContext: 0.00039883
    Epoch:   470     LossContext: 0.00039883
    Epoch:   480     LossContext: 0.00039883
    Epoch:   490     LossContext: 0.00039883
    Epoch:   500     LossContext: 0.00039883
    Epoch:   510     LossContext: 0.00039883
    Epoch:   520     LossContext: 0.00039883
    Epoch:   530     LossContext: 0.00039883
    Epoch:   540     LossContext: 0.00039883
    Epoch:   550     LossContext: 0.00039883
    Epoch:   560     LossContext: 0.00039883
    Epoch:   570     LossContext: 0.00039883
    Epoch:   580     LossContext: 0.00039883
    Epoch:   590     LossContext: 0.00039883
    Epoch:   600     LossContext: 0.00039883
    Epoch:   610     LossContext: 0.00039883
    Epoch:   620     LossContext: 0.00039883
    Epoch:   630     LossContext: 0.00039883
    Epoch:   640     LossContext: 0.00039883
    Epoch:   650     LossContext: 0.00039883
    Epoch:   660     LossContext: 0.00039883
    Epoch:   670     LossContext: 0.00039883
    Epoch:   680     LossContext: 0.00039883
    Epoch:   690     LossContext: 0.00039883
    Epoch:   700     LossContext: 0.00039883
    Epoch:   710     LossContext: 0.00039883
    Epoch:   720     LossContext: 0.00039883
    Epoch:   730     LossContext: 0.00039883
    Epoch:   740     LossContext: 0.00039883
    Epoch:   750     LossContext: 0.00039883
    Epoch:   760     LossContext: 0.00039883
    Epoch:   770     LossContext: 0.00039883
    Epoch:   780     LossContext: 0.00039883
    Epoch:   790     LossContext: 0.00039883
    Epoch:   800     LossContext: 0.00039883
    Epoch:   810     LossContext: 0.00039883
    Epoch:   820     LossContext: 0.00039883
    Epoch:   830     LossContext: 0.00039883
    Epoch:   840     LossContext: 0.00039883
    Epoch:   850     LossContext: 0.00039883
    Epoch:   860     LossContext: 0.00039883
    Epoch:   870     LossContext: 0.00039883
    Epoch:   880     LossContext: 0.00039883
    Epoch:   890     LossContext: 0.00039883
    Epoch:   900     LossContext: 0.00039883
    Epoch:   910     LossContext: 0.00039883
    Epoch:   920     LossContext: 0.00039883
    Epoch:   930     LossContext: 0.00039883
    Epoch:   940     LossContext: 0.00039883
    Epoch:   950     LossContext: 0.00039883
    Epoch:   960     LossContext: 0.00039883
    Epoch:   970     LossContext: 0.00039883
    Epoch:   980     LossContext: 0.00039883
    Epoch:   990     LossContext: 0.00039883
    Epoch:  1000     LossContext: 0.00039883
    Epoch:  1010     LossContext: 0.00039883
    Epoch:  1020     LossContext: 0.00039883
    Epoch:  1030     LossContext: 0.00039883
    Epoch:  1040     LossContext: 0.00039883
    Epoch:  1050     LossContext: 0.00039883
    Epoch:  1060     LossContext: 0.00039883
    Epoch:  1070     LossContext: 0.00039883
    Epoch:  1080     LossContext: 0.00039883
    Epoch:  1090     LossContext: 0.00039883
    Epoch:  1100     LossContext: 0.00039883
    Epoch:  1110     LossContext: 0.00039883
    Epoch:  1120     LossContext: 0.00039883
    Epoch:  1130     LossContext: 0.00039883
    Epoch:  1140     LossContext: 0.00039883
    Epoch:  1150     LossContext: 0.00039883
    Epoch:  1160     LossContext: 0.00039883
    Epoch:  1170     LossContext: 0.00039883
    Epoch:  1180     LossContext: 0.00039883
    Epoch:  1190     LossContext: 0.00039883
    Epoch:  1200     LossContext: 0.00039883
    Epoch:  1210     LossContext: 0.00039883
    Epoch:  1220     LossContext: 0.00039883
    Epoch:  1230     LossContext: 0.00039883
    Epoch:  1240     LossContext: 0.00039883
    Epoch:  1250     LossContext: 0.00039883
    Epoch:  1260     LossContext: 0.00039883
    Epoch:  1270     LossContext: 0.00039883
    Epoch:  1280     LossContext: 0.00039883
    Epoch:  1290     LossContext: 0.00039883
    Epoch:  1300     LossContext: 0.00039883
    Epoch:  1310     LossContext: 0.00039883
    Epoch:  1320     LossContext: 0.00039883
    Epoch:  1330     LossContext: 0.00039883
    Epoch:  1340     LossContext: 0.00039883
    Epoch:  1350     LossContext: 0.00039883
    Epoch:  1360     LossContext: 0.00039883
    Epoch:  1370     LossContext: 0.00039883
    Epoch:  1380     LossContext: 0.00039883
    Epoch:  1390     LossContext: 0.00039883
    Epoch:  1400     LossContext: 0.00039883
    Epoch:  1410     LossContext: 0.00039883
    Epoch:  1420     LossContext: 0.00039883
    Epoch:  1430     LossContext: 0.00039883
    Epoch:  1440     LossContext: 0.00039883
    Epoch:  1450     LossContext: 0.00039883
    Epoch:  1460     LossContext: 0.00039883
    Epoch:  1470     LossContext: 0.00039883
    Epoch:  1480     LossContext: 0.00039883
    Epoch:  1490     LossContext: 0.00039883
    Epoch:  1499     LossContext: 0.00039883

Gradient descent adaptation time: 0 hours 1 mins 33 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./05052024-092750/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 0.00026362634

