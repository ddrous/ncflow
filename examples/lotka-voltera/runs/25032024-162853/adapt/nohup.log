
############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
No training. Loading data and results from: ./runs/25032024-162853/
WARNING: You did not provide a dataloader id. A new one has been generated: 193558
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 193558
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 308240 


WARNING: No key provided for the context initialization. Initializing at 0.

No training, loading model and results from ./runs/25032024-162853/ folder ...

WARNING: You did not provide a dataloader id. A new one has been generated: 193559
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 2.277585e-05


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./runs/25032024-162853/adapt/
 Seed: 6078

==  Begining in-domain visualisation ... ==
    Environment id: 6
    Trajectory id: 10
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/25032024-162853/results_in_domain.png
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.03106722
    Epoch:     1     LossContext: 0.01689889
    Epoch:     2     LossContext: 0.00606469
    Epoch:     3     LossContext: 0.00054452
    Epoch:    10     LossContext: 0.00085692
    Epoch:    20     LossContext: 0.00092404
    Epoch:    30     LossContext: 0.00069578
    Epoch:    40     LossContext: 0.00011339
    Epoch:    50     LossContext: 0.00002834
    Epoch:    60     LossContext: 0.00004657
    Epoch:    70     LossContext: 0.00001434
    Epoch:    80     LossContext: 0.00001806
    Epoch:    90     LossContext: 0.00001441
    Epoch:   100     LossContext: 0.00001408
    Epoch:   110     LossContext: 0.00001410
    Epoch:   120     LossContext: 0.00001390
    Epoch:   130     LossContext: 0.00001384
    Epoch:   140     LossContext: 0.00001382
    Epoch:   150     LossContext: 0.00001380
    Epoch:   160     LossContext: 0.00001377
    Epoch:   170     LossContext: 0.00001374
    Epoch:   180     LossContext: 0.00001371
    Epoch:   190     LossContext: 0.00001369
    Epoch:   200     LossContext: 0.00001366
    Epoch:   210     LossContext: 0.00001364
    Epoch:   220     LossContext: 0.00001361
    Epoch:   230     LossContext: 0.00001358
    Epoch:   240     LossContext: 0.00001356
    Epoch:   250     LossContext: 0.00001353
    Epoch:   260     LossContext: 0.00001351
    Epoch:   270     LossContext: 0.00001349
    Epoch:   280     LossContext: 0.00001346
    Epoch:   290     LossContext: 0.00001344
    Epoch:   300     LossContext: 0.00001342
    Epoch:   310     LossContext: 0.00001339
    Epoch:   320     LossContext: 0.00001337
    Epoch:   330     LossContext: 0.00001335
    Epoch:   340     LossContext: 0.00001332
    Epoch:   350     LossContext: 0.00001330
    Epoch:   360     LossContext: 0.00001328
    Epoch:   370     LossContext: 0.00001326
    Epoch:   380     LossContext: 0.00001324
    Epoch:   390     LossContext: 0.00001323
    Epoch:   400     LossContext: 0.00001321
    Epoch:   410     LossContext: 0.00001320
    Epoch:   420     LossContext: 0.00001318
    Epoch:   430     LossContext: 0.00001317
    Epoch:   440     LossContext: 0.00001315
    Epoch:   450     LossContext: 0.00001314
    Epoch:   460     LossContext: 0.00001313
    Epoch:   470     LossContext: 0.00001311
    Epoch:   480     LossContext: 0.00001310
    Epoch:   490     LossContext: 0.00001309
    Epoch:   500     LossContext: 0.00001307
    Epoch:   510     LossContext: 0.00001306
    Epoch:   520     LossContext: 0.00001305
    Epoch:   530     LossContext: 0.00001303
    Epoch:   540     LossContext: 0.00001302
    Epoch:   550     LossContext: 0.00001300
    Epoch:   560     LossContext: 0.00001299
    Epoch:   570     LossContext: 0.00001298
    Epoch:   580     LossContext: 0.00001296
    Epoch:   590     LossContext: 0.00001295
    Epoch:   600     LossContext: 0.00001293
    Epoch:   610     LossContext: 0.00001292
    Epoch:   620     LossContext: 0.00001291
    Epoch:   630     LossContext: 0.00001289
    Epoch:   640     LossContext: 0.00001288
    Epoch:   650     LossContext: 0.00001286
    Epoch:   660     LossContext: 0.00001285
    Epoch:   670     LossContext: 0.00001284
    Epoch:   680     LossContext: 0.00001283
    Epoch:   690     LossContext: 0.00001282
    Epoch:   700     LossContext: 0.00001281
    Epoch:   710     LossContext: 0.00001281
    Epoch:   720     LossContext: 0.00001280
    Epoch:   730     LossContext: 0.00001279
    Epoch:   740     LossContext: 0.00001278
    Epoch:   750     LossContext: 0.00001277
    Epoch:   760     LossContext: 0.00001277
    Epoch:   770     LossContext: 0.00001276
    Epoch:   780     LossContext: 0.00001275
    Epoch:   790     LossContext: 0.00001274
    Epoch:   800     LossContext: 0.00001273
    Epoch:   810     LossContext: 0.00001273
    Epoch:   820     LossContext: 0.00001272
    Epoch:   830     LossContext: 0.00001271
    Epoch:   840     LossContext: 0.00001270
    Epoch:   850     LossContext: 0.00001269
    Epoch:   860     LossContext: 0.00001268
    Epoch:   870     LossContext: 0.00001268
    Epoch:   880     LossContext: 0.00001267
    Epoch:   890     LossContext: 0.00001266
    Epoch:   900     LossContext: 0.00001265
    Epoch:   910     LossContext: 0.00001264
    Epoch:   920     LossContext: 0.00001263
    Epoch:   930     LossContext: 0.00001262
    Epoch:   940     LossContext: 0.00001262
    Epoch:   950     LossContext: 0.00001261
    Epoch:   960     LossContext: 0.00001260
    Epoch:   970     LossContext: 0.00001259
    Epoch:   980     LossContext: 0.00001258
    Epoch:   990     LossContext: 0.00001257
    Epoch:  1000     LossContext: 0.00001256
    Epoch:  1010     LossContext: 0.00001256
    Epoch:  1020     LossContext: 0.00001255
    Epoch:  1030     LossContext: 0.00001254
    Epoch:  1040     LossContext: 0.00001253
    Epoch:  1050     LossContext: 0.00001252
    Epoch:  1060     LossContext: 0.00001251
    Epoch:  1070     LossContext: 0.00001250
    Epoch:  1080     LossContext: 0.00001250
    Epoch:  1090     LossContext: 0.00001249
    Epoch:  1100     LossContext: 0.00001248
    Epoch:  1110     LossContext: 0.00001247
    Epoch:  1120     LossContext: 0.00001246
    Epoch:  1130     LossContext: 0.00001246
    Epoch:  1140     LossContext: 0.00001245
    Epoch:  1150     LossContext: 0.00001244
    Epoch:  1160     LossContext: 0.00001243
    Epoch:  1170     LossContext: 0.00001243
    Epoch:  1180     LossContext: 0.00001242
    Epoch:  1190     LossContext: 0.00001241
    Epoch:  1200     LossContext: 0.00001240
    Epoch:  1210     LossContext: 0.00001240
    Epoch:  1220     LossContext: 0.00001239
    Epoch:  1230     LossContext: 0.00001238
    Epoch:  1240     LossContext: 0.00001238
    Epoch:  1250     LossContext: 0.00001237
    Epoch:  1260     LossContext: 0.00001237
    Epoch:  1270     LossContext: 0.00001236
    Epoch:  1280     LossContext: 0.00001235
    Epoch:  1290     LossContext: 0.00001235
    Epoch:  1300     LossContext: 0.00001234
    Epoch:  1310     LossContext: 0.00001233
    Epoch:  1320     LossContext: 0.00001233
    Epoch:  1330     LossContext: 0.00001232
    Epoch:  1340     LossContext: 0.00001232
    Epoch:  1350     LossContext: 0.00001231
    Epoch:  1360     LossContext: 0.00001231
    Epoch:  1370     LossContext: 0.00001231
    Epoch:  1380     LossContext: 0.00001230
    Epoch:  1390     LossContext: 0.00001230
    Epoch:  1400     LossContext: 0.00001230
    Epoch:  1410     LossContext: 0.00001229
    Epoch:  1420     LossContext: 0.00001229
    Epoch:  1430     LossContext: 0.00001229
    Epoch:  1440     LossContext: 0.00001229
    Epoch:  1450     LossContext: 0.00001228
    Epoch:  1460     LossContext: 0.00001228
    Epoch:  1470     LossContext: 0.00001228
    Epoch:  1480     LossContext: 0.00001227
    Epoch:  1490     LossContext: 0.00001227
    Epoch:  1499     LossContext: 0.00001227

Gradient descent adaptation time: 0 hours 1 mins 38 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.12110929
    Epoch:     1     LossContext: 0.08149833
    Epoch:     2     LossContext: 0.04486332
    Epoch:     3     LossContext: 0.01572051
    Epoch:    10     LossContext: 0.00461885
    Epoch:    20     LossContext: 0.00279820
    Epoch:    30     LossContext: 0.00071477
    Epoch:    40     LossContext: 0.00041330
    Epoch:    50     LossContext: 0.00040922
    Epoch:    60     LossContext: 0.00006474
    Epoch:    70     LossContext: 0.00004544
    Epoch:    80     LossContext: 0.00004075
    Epoch:    90     LossContext: 0.00003181
    Epoch:   100     LossContext: 0.00002948
    Epoch:   110     LossContext: 0.00002852
    Epoch:   120     LossContext: 0.00002814
    Epoch:   130     LossContext: 0.00002819
    Epoch:   140     LossContext: 0.00002811
    Epoch:   150     LossContext: 0.00002807
    Epoch:   160     LossContext: 0.00002804
    Epoch:   170     LossContext: 0.00002802
    Epoch:   180     LossContext: 0.00002799
    Epoch:   190     LossContext: 0.00002797
    Epoch:   200     LossContext: 0.00002795
    Epoch:   210     LossContext: 0.00002792
    Epoch:   220     LossContext: 0.00002790
    Epoch:   230     LossContext: 0.00002787
    Epoch:   240     LossContext: 0.00002785
    Epoch:   250     LossContext: 0.00002782
    Epoch:   260     LossContext: 0.00002779
    Epoch:   270     LossContext: 0.00002776
    Epoch:   280     LossContext: 0.00002774
    Epoch:   290     LossContext: 0.00002771
    Epoch:   300     LossContext: 0.00002768
    Epoch:   310     LossContext: 0.00002765
    Epoch:   320     LossContext: 0.00002762
    Epoch:   330     LossContext: 0.00002759
    Epoch:   340     LossContext: 0.00002756
    Epoch:   350     LossContext: 0.00002753
    Epoch:   360     LossContext: 0.00002750
    Epoch:   370     LossContext: 0.00002746
    Epoch:   380     LossContext: 0.00002743
    Epoch:   390     LossContext: 0.00002740
    Epoch:   400     LossContext: 0.00002737
    Epoch:   410     LossContext: 0.00002733
    Epoch:   420     LossContext: 0.00002730
    Epoch:   430     LossContext: 0.00002727
    Epoch:   440     LossContext: 0.00002723
    Epoch:   450     LossContext: 0.00002720
    Epoch:   460     LossContext: 0.00002716
    Epoch:   470     LossContext: 0.00002713
    Epoch:   480     LossContext: 0.00002709
    Epoch:   490     LossContext: 0.00002706
    Epoch:   500     LossContext: 0.00002702
    Epoch:   510     LossContext: 0.00002699
    Epoch:   520     LossContext: 0.00002695
    Epoch:   530     LossContext: 0.00002691
    Epoch:   540     LossContext: 0.00002688
    Epoch:   550     LossContext: 0.00002684
    Epoch:   560     LossContext: 0.00002680
    Epoch:   570     LossContext: 0.00002676
    Epoch:   580     LossContext: 0.00002673
    Epoch:   590     LossContext: 0.00002669
    Epoch:   600     LossContext: 0.00002665
    Epoch:   610     LossContext: 0.00002661
    Epoch:   620     LossContext: 0.00002657
    Epoch:   630     LossContext: 0.00002653
    Epoch:   640     LossContext: 0.00002649
    Epoch:   650     LossContext: 0.00002645
    Epoch:   660     LossContext: 0.00002641
    Epoch:   670     LossContext: 0.00002638
    Epoch:   680     LossContext: 0.00002636
    Epoch:   690     LossContext: 0.00002634
    Epoch:   700     LossContext: 0.00002632
    Epoch:   710     LossContext: 0.00002629
    Epoch:   720     LossContext: 0.00002627
    Epoch:   730     LossContext: 0.00002625
    Epoch:   740     LossContext: 0.00002623
    Epoch:   750     LossContext: 0.00002621
    Epoch:   760     LossContext: 0.00002619
    Epoch:   770     LossContext: 0.00002616
    Epoch:   780     LossContext: 0.00002614
    Epoch:   790     LossContext: 0.00002612
    Epoch:   800     LossContext: 0.00002610
    Epoch:   810     LossContext: 0.00002607
    Epoch:   820     LossContext: 0.00002605
    Epoch:   830     LossContext: 0.00002603
    Epoch:   840     LossContext: 0.00002601
    Epoch:   850     LossContext: 0.00002598
    Epoch:   860     LossContext: 0.00002596
    Epoch:   870     LossContext: 0.00002593
    Epoch:   880     LossContext: 0.00002591
    Epoch:   890     LossContext: 0.00002589
    Epoch:   900     LossContext: 0.00002586
    Epoch:   910     LossContext: 0.00002584
    Epoch:   920     LossContext: 0.00002581
    Epoch:   930     LossContext: 0.00002579
    Epoch:   940     LossContext: 0.00002576
    Epoch:   950     LossContext: 0.00002574
    Epoch:   960     LossContext: 0.00002571
    Epoch:   970     LossContext: 0.00002569
    Epoch:   980     LossContext: 0.00002566
    Epoch:   990     LossContext: 0.00002564
    Epoch:  1000     LossContext: 0.00002561
    Epoch:  1010     LossContext: 0.00002559
    Epoch:  1020     LossContext: 0.00002556
    Epoch:  1030     LossContext: 0.00002553
    Epoch:  1040     LossContext: 0.00002551
    Epoch:  1050     LossContext: 0.00002548
    Epoch:  1060     LossContext: 0.00002546
    Epoch:  1070     LossContext: 0.00002543
    Epoch:  1080     LossContext: 0.00002540
    Epoch:  1090     LossContext: 0.00002538
    Epoch:  1100     LossContext: 0.00002535
    Epoch:  1110     LossContext: 0.00002532
    Epoch:  1120     LossContext: 0.00002529
    Epoch:  1130     LossContext: 0.00002527
    Epoch:  1140     LossContext: 0.00002524
    Epoch:  1150     LossContext: 0.00002521
    Epoch:  1160     LossContext: 0.00002518
    Epoch:  1170     LossContext: 0.00002516
    Epoch:  1180     LossContext: 0.00002513
    Epoch:  1190     LossContext: 0.00002510
    Epoch:  1200     LossContext: 0.00002507
    Epoch:  1210     LossContext: 0.00002504
    Epoch:  1220     LossContext: 0.00002501
    Epoch:  1230     LossContext: 0.00002499
    Epoch:  1240     LossContext: 0.00002496
    Epoch:  1250     LossContext: 0.00002493
    Epoch:  1260     LossContext: 0.00002490
    Epoch:  1270     LossContext: 0.00002487
    Epoch:  1280     LossContext: 0.00002484
    Epoch:  1290     LossContext: 0.00002481
    Epoch:  1300     LossContext: 0.00002478
    Epoch:  1310     LossContext: 0.00002475
    Epoch:  1320     LossContext: 0.00002472
    Epoch:  1330     LossContext: 0.00002469
    Epoch:  1340     LossContext: 0.00002467
    Epoch:  1350     LossContext: 0.00002466
    Epoch:  1360     LossContext: 0.00002464
    Epoch:  1370     LossContext: 0.00002463
    Epoch:  1380     LossContext: 0.00002461
    Epoch:  1390     LossContext: 0.00002459
    Epoch:  1400     LossContext: 0.00002458
    Epoch:  1410     LossContext: 0.00002456
    Epoch:  1420     LossContext: 0.00002455
    Epoch:  1430     LossContext: 0.00002453
    Epoch:  1440     LossContext: 0.00002452
    Epoch:  1450     LossContext: 0.00002450
    Epoch:  1460     LossContext: 0.00002448
    Epoch:  1470     LossContext: 0.00002447
    Epoch:  1480     LossContext: 0.00002445
    Epoch:  1490     LossContext: 0.00002444
    Epoch:  1499     LossContext: 0.00002442

Gradient descent adaptation time: 0 hours 1 mins 31 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.04224264
    Epoch:     1     LossContext: 0.02281691
    Epoch:     2     LossContext: 0.01068793
    Epoch:     3     LossContext: 0.00399050
    Epoch:    10     LossContext: 0.00405302
    Epoch:    20     LossContext: 0.00217023
    Epoch:    30     LossContext: 0.00059817
    Epoch:    40     LossContext: 0.00005541
    Epoch:    50     LossContext: 0.00017439
    Epoch:    60     LossContext: 0.00004849
    Epoch:    70     LossContext: 0.00002750
    Epoch:    80     LossContext: 0.00003192
    Epoch:    90     LossContext: 0.00002897
    Epoch:   100     LossContext: 0.00002676
    Epoch:   110     LossContext: 0.00002625
    Epoch:   120     LossContext: 0.00002614
    Epoch:   130     LossContext: 0.00002602
    Epoch:   140     LossContext: 0.00002588
    Epoch:   150     LossContext: 0.00002574
    Epoch:   160     LossContext: 0.00002561
    Epoch:   170     LossContext: 0.00002548
    Epoch:   180     LossContext: 0.00002536
    Epoch:   190     LossContext: 0.00002524
    Epoch:   200     LossContext: 0.00002512
    Epoch:   210     LossContext: 0.00002500
    Epoch:   220     LossContext: 0.00002489
    Epoch:   230     LossContext: 0.00002477
    Epoch:   240     LossContext: 0.00002465
    Epoch:   250     LossContext: 0.00002453
    Epoch:   260     LossContext: 0.00002442
    Epoch:   270     LossContext: 0.00002430
    Epoch:   280     LossContext: 0.00002418
    Epoch:   290     LossContext: 0.00002406
    Epoch:   300     LossContext: 0.00002394
    Epoch:   310     LossContext: 0.00002383
    Epoch:   320     LossContext: 0.00002372
    Epoch:   330     LossContext: 0.00002361
    Epoch:   340     LossContext: 0.00002351
    Epoch:   350     LossContext: 0.00002341
    Epoch:   360     LossContext: 0.00002331
    Epoch:   370     LossContext: 0.00002321
    Epoch:   380     LossContext: 0.00002312
    Epoch:   390     LossContext: 0.00002302
    Epoch:   400     LossContext: 0.00002293
    Epoch:   410     LossContext: 0.00002285
    Epoch:   420     LossContext: 0.00002276
    Epoch:   430     LossContext: 0.00002268
    Epoch:   440     LossContext: 0.00002260
    Epoch:   450     LossContext: 0.00002252
    Epoch:   460     LossContext: 0.00002244
    Epoch:   470     LossContext: 0.00002236
    Epoch:   480     LossContext: 0.00002229
    Epoch:   490     LossContext: 0.00002222
    Epoch:   500     LossContext: 0.00002214
    Epoch:   510     LossContext: 0.00002207
    Epoch:   520     LossContext: 0.00002199
    Epoch:   530     LossContext: 0.00002192
    Epoch:   540     LossContext: 0.00002185
    Epoch:   550     LossContext: 0.00002177
    Epoch:   560     LossContext: 0.00002170
    Epoch:   570     LossContext: 0.00002163
    Epoch:   580     LossContext: 0.00002156
    Epoch:   590     LossContext: 0.00002149
    Epoch:   600     LossContext: 0.00002142
    Epoch:   610     LossContext: 0.00002136
    Epoch:   620     LossContext: 0.00002129
    Epoch:   630     LossContext: 0.00002123
    Epoch:   640     LossContext: 0.00002117
    Epoch:   650     LossContext: 0.00002110
    Epoch:   660     LossContext: 0.00002104
    Epoch:   670     LossContext: 0.00002099
    Epoch:   680     LossContext: 0.00002096
    Epoch:   690     LossContext: 0.00002093
    Epoch:   700     LossContext: 0.00002090
    Epoch:   710     LossContext: 0.00002087
    Epoch:   720     LossContext: 0.00002084
    Epoch:   730     LossContext: 0.00002081
    Epoch:   740     LossContext: 0.00002078
    Epoch:   750     LossContext: 0.00002075
    Epoch:   760     LossContext: 0.00002071
    Epoch:   770     LossContext: 0.00002068
    Epoch:   780     LossContext: 0.00002065
    Epoch:   790     LossContext: 0.00002062
    Epoch:   800     LossContext: 0.00002059
    Epoch:   810     LossContext: 0.00002056
    Epoch:   820     LossContext: 0.00002054
    Epoch:   830     LossContext: 0.00002051
    Epoch:   840     LossContext: 0.00002048
    Epoch:   850     LossContext: 0.00002045
    Epoch:   860     LossContext: 0.00002042
    Epoch:   870     LossContext: 0.00002039
    Epoch:   880     LossContext: 0.00002037
    Epoch:   890     LossContext: 0.00002034
    Epoch:   900     LossContext: 0.00002031
    Epoch:   910     LossContext: 0.00002028
    Epoch:   920     LossContext: 0.00002026
    Epoch:   930     LossContext: 0.00002023
    Epoch:   940     LossContext: 0.00002020
    Epoch:   950     LossContext: 0.00002018
    Epoch:   960     LossContext: 0.00002015
    Epoch:   970     LossContext: 0.00002012
    Epoch:   980     LossContext: 0.00002010
    Epoch:   990     LossContext: 0.00002007
    Epoch:  1000     LossContext: 0.00002005
    Epoch:  1010     LossContext: 0.00002002
    Epoch:  1020     LossContext: 0.00002000
    Epoch:  1030     LossContext: 0.00001997
    Epoch:  1040     LossContext: 0.00001995
    Epoch:  1050     LossContext: 0.00001992
    Epoch:  1060     LossContext: 0.00001990
    Epoch:  1070     LossContext: 0.00001987
    Epoch:  1080     LossContext: 0.00001985
    Epoch:  1090     LossContext: 0.00001983
    Epoch:  1100     LossContext: 0.00001980
    Epoch:  1110     LossContext: 0.00001978
    Epoch:  1120     LossContext: 0.00001976
    Epoch:  1130     LossContext: 0.00001973
    Epoch:  1140     LossContext: 0.00001971
    Epoch:  1150     LossContext: 0.00001969
    Epoch:  1160     LossContext: 0.00001966
    Epoch:  1170     LossContext: 0.00001964
    Epoch:  1180     LossContext: 0.00001962
    Epoch:  1190     LossContext: 0.00001960
    Epoch:  1200     LossContext: 0.00001958
    Epoch:  1210     LossContext: 0.00001955
    Epoch:  1220     LossContext: 0.00001953
    Epoch:  1230     LossContext: 0.00001951
    Epoch:  1240     LossContext: 0.00001949
    Epoch:  1250     LossContext: 0.00001947
    Epoch:  1260     LossContext: 0.00001945
    Epoch:  1270     LossContext: 0.00001943
    Epoch:  1280     LossContext: 0.00001940
    Epoch:  1290     LossContext: 0.00001939
    Epoch:  1300     LossContext: 0.00001937
    Epoch:  1310     LossContext: 0.00001935
    Epoch:  1320     LossContext: 0.00001933
    Epoch:  1330     LossContext: 0.00001931
    Epoch:  1340     LossContext: 0.00001929
    Epoch:  1350     LossContext: 0.00001928
    Epoch:  1360     LossContext: 0.00001927
    Epoch:  1370     LossContext: 0.00001926
    Epoch:  1380     LossContext: 0.00001925
    Epoch:  1390     LossContext: 0.00001925
    Epoch:  1400     LossContext: 0.00001924
    Epoch:  1410     LossContext: 0.00001923
    Epoch:  1420     LossContext: 0.00001922
    Epoch:  1430     LossContext: 0.00001921
    Epoch:  1440     LossContext: 0.00001920
    Epoch:  1450     LossContext: 0.00001919
    Epoch:  1460     LossContext: 0.00001918
    Epoch:  1470     LossContext: 0.00001917
    Epoch:  1480     LossContext: 0.00001916
    Epoch:  1490     LossContext: 0.00001915
    Epoch:  1499     LossContext: 0.00001914

Gradient descent adaptation time: 0 hours 1 mins 32 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.01895260
    Epoch:     1     LossContext: 0.00907878
    Epoch:     2     LossContext: 0.00481212
    Epoch:     3     LossContext: 0.00276448
    Epoch:    10     LossContext: 0.00112500
    Epoch:    20     LossContext: 0.00139907
    Epoch:    30     LossContext: 0.00008839
    Epoch:    40     LossContext: 0.00019114
    Epoch:    50     LossContext: 0.00002443
    Epoch:    60     LossContext: 0.00004733
    Epoch:    70     LossContext: 0.00002272
    Epoch:    80     LossContext: 0.00002472
    Epoch:    90     LossContext: 0.00002277
    Epoch:   100     LossContext: 0.00002223
    Epoch:   110     LossContext: 0.00002224
    Epoch:   120     LossContext: 0.00002206
    Epoch:   130     LossContext: 0.00002199
    Epoch:   140     LossContext: 0.00002195
    Epoch:   150     LossContext: 0.00002190
    Epoch:   160     LossContext: 0.00002185
    Epoch:   170     LossContext: 0.00002181
    Epoch:   180     LossContext: 0.00002177
    Epoch:   190     LossContext: 0.00002173
    Epoch:   200     LossContext: 0.00002169
    Epoch:   210     LossContext: 0.00002165
    Epoch:   220     LossContext: 0.00002162
    Epoch:   230     LossContext: 0.00002158
    Epoch:   240     LossContext: 0.00002155
    Epoch:   250     LossContext: 0.00002152
    Epoch:   260     LossContext: 0.00002148
    Epoch:   270     LossContext: 0.00002145
    Epoch:   280     LossContext: 0.00002142
    Epoch:   290     LossContext: 0.00002139
    Epoch:   300     LossContext: 0.00002136
    Epoch:   310     LossContext: 0.00002133
    Epoch:   320     LossContext: 0.00002130
    Epoch:   330     LossContext: 0.00002128
    Epoch:   340     LossContext: 0.00002125
    Epoch:   350     LossContext: 0.00002123
    Epoch:   360     LossContext: 0.00002121
    Epoch:   370     LossContext: 0.00002119
    Epoch:   380     LossContext: 0.00002116
    Epoch:   390     LossContext: 0.00002114
    Epoch:   400     LossContext: 0.00002112
    Epoch:   410     LossContext: 0.00002110
    Epoch:   420     LossContext: 0.00002108
    Epoch:   430     LossContext: 0.00002106
    Epoch:   440     LossContext: 0.00002104
    Epoch:   450     LossContext: 0.00002102
    Epoch:   460     LossContext: 0.00002100
    Epoch:   470     LossContext: 0.00002098
    Epoch:   480     LossContext: 0.00002096
    Epoch:   490     LossContext: 0.00002094
    Epoch:   500     LossContext: 0.00002092
    Epoch:   510     LossContext: 0.00002090
    Epoch:   520     LossContext: 0.00002088
    Epoch:   530     LossContext: 0.00002086
    Epoch:   540     LossContext: 0.00002084
    Epoch:   550     LossContext: 0.00002083
    Epoch:   560     LossContext: 0.00002081
    Epoch:   570     LossContext: 0.00002079
    Epoch:   580     LossContext: 0.00002078
    Epoch:   590     LossContext: 0.00002076
    Epoch:   600     LossContext: 0.00002074
    Epoch:   610     LossContext: 0.00002072
    Epoch:   620     LossContext: 0.00002071
    Epoch:   630     LossContext: 0.00002069
    Epoch:   640     LossContext: 0.00002067
    Epoch:   650     LossContext: 0.00002066
    Epoch:   660     LossContext: 0.00002064
    Epoch:   670     LossContext: 0.00002063
    Epoch:   680     LossContext: 0.00002062
    Epoch:   690     LossContext: 0.00002061
    Epoch:   700     LossContext: 0.00002060
    Epoch:   710     LossContext: 0.00002059
    Epoch:   720     LossContext: 0.00002058
    Epoch:   730     LossContext: 0.00002058
    Epoch:   740     LossContext: 0.00002057
    Epoch:   750     LossContext: 0.00002056
    Epoch:   760     LossContext: 0.00002055
    Epoch:   770     LossContext: 0.00002054
    Epoch:   780     LossContext: 0.00002053
    Epoch:   790     LossContext: 0.00002052
    Epoch:   800     LossContext: 0.00002051
    Epoch:   810     LossContext: 0.00002051
    Epoch:   820     LossContext: 0.00002050
    Epoch:   830     LossContext: 0.00002049
    Epoch:   840     LossContext: 0.00002048
    Epoch:   850     LossContext: 0.00002047
    Epoch:   860     LossContext: 0.00002046
    Epoch:   870     LossContext: 0.00002045
    Epoch:   880     LossContext: 0.00002044
    Epoch:   890     LossContext: 0.00002043
    Epoch:   900     LossContext: 0.00002042
    Epoch:   910     LossContext: 0.00002042
    Epoch:   920     LossContext: 0.00002041
    Epoch:   930     LossContext: 0.00002040
    Epoch:   940     LossContext: 0.00002039
    Epoch:   950     LossContext: 0.00002038
    Epoch:   960     LossContext: 0.00002037
    Epoch:   970     LossContext: 0.00002036
    Epoch:   980     LossContext: 0.00002035
    Epoch:   990     LossContext: 0.00002034
    Epoch:  1000     LossContext: 0.00002033
    Epoch:  1010     LossContext: 0.00002032
    Epoch:  1020     LossContext: 0.00002031
    Epoch:  1030     LossContext: 0.00002030
    Epoch:  1040     LossContext: 0.00002029
    Epoch:  1050     LossContext: 0.00002028
    Epoch:  1060     LossContext: 0.00002027
    Epoch:  1070     LossContext: 0.00002027
    Epoch:  1080     LossContext: 0.00002026
    Epoch:  1090     LossContext: 0.00002025
    Epoch:  1100     LossContext: 0.00002024
    Epoch:  1110     LossContext: 0.00002023
    Epoch:  1120     LossContext: 0.00002022
    Epoch:  1130     LossContext: 0.00002021
    Epoch:  1140     LossContext: 0.00002020
    Epoch:  1150     LossContext: 0.00002019
    Epoch:  1160     LossContext: 0.00002018
    Epoch:  1170     LossContext: 0.00002017
    Epoch:  1180     LossContext: 0.00002016
    Epoch:  1190     LossContext: 0.00002015
    Epoch:  1200     LossContext: 0.00002014
    Epoch:  1210     LossContext: 0.00002013
    Epoch:  1220     LossContext: 0.00002012
    Epoch:  1230     LossContext: 0.00002011
    Epoch:  1240     LossContext: 0.00002010
    Epoch:  1250     LossContext: 0.00002009
    Epoch:  1260     LossContext: 0.00002008
    Epoch:  1270     LossContext: 0.00002007
    Epoch:  1280     LossContext: 0.00002006
    Epoch:  1290     LossContext: 0.00002005
    Epoch:  1300     LossContext: 0.00002004
    Epoch:  1310     LossContext: 0.00002003
    Epoch:  1320     LossContext: 0.00002002
    Epoch:  1330     LossContext: 0.00002001
    Epoch:  1340     LossContext: 0.00002000
    Epoch:  1350     LossContext: 0.00001999
    Epoch:  1360     LossContext: 0.00001999
    Epoch:  1370     LossContext: 0.00001998
    Epoch:  1380     LossContext: 0.00001998
    Epoch:  1390     LossContext: 0.00001997
    Epoch:  1400     LossContext: 0.00001997
    Epoch:  1410     LossContext: 0.00001996
    Epoch:  1420     LossContext: 0.00001996
    Epoch:  1430     LossContext: 0.00001995
    Epoch:  1440     LossContext: 0.00001995
    Epoch:  1450     LossContext: 0.00001994
    Epoch:  1460     LossContext: 0.00001993
    Epoch:  1470     LossContext: 0.00001993
    Epoch:  1480     LossContext: 0.00001992
    Epoch:  1490     LossContext: 0.00001992
    Epoch:  1499     LossContext: 0.00001991

Gradient descent adaptation time: 0 hours 1 mins 32 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./runs/25032024-162853/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 1.462843e-06

==  Begining out-of-distribution visualisation ... ==
    Environment id: 0
    Trajectory id: 0
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/25032024-162853/adapt/results_ood.png

Full evaluation of the model on many random seeds


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Mean and std of the scores across various datasets

