
############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
No training. Loading data and results from: ./runs/25032024-162853/
WARNING: You did not provide a dataloader id. A new one has been generated: 015411
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 015411
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 308240 


WARNING: No key provided for the context initialization. Initializing at 0.

No training, loading model and results from ./runs/25032024-162853/ folder ...

WARNING: You did not provide a dataloader id. A new one has been generated: 015412
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 2.277585e-05


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./runs/25032024-162853/adapt/
 Seed: 6078

==  Begining in-domain visualisation ... ==
    Environment id: 2
    Trajectory id: 18
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/25032024-162853/results_in_domain.png
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.03610767
    Epoch:     1     LossContext: 0.01900632
    Epoch:     2     LossContext: 0.00648179
    Epoch:     3     LossContext: 0.00135275
    Epoch:    10     LossContext: 0.00079885
    Epoch:    20     LossContext: 0.00090975
    Epoch:    30     LossContext: 0.00080537
    Epoch:    40     LossContext: 0.00015170
    Epoch:    50     LossContext: 0.00002647
    Epoch:    60     LossContext: 0.00005403
    Epoch:    70     LossContext: 0.00001515
    Epoch:    80     LossContext: 0.00002010
    Epoch:    90     LossContext: 0.00001538
    Epoch:   100     LossContext: 0.00001527
    Epoch:   110     LossContext: 0.00001520
    Epoch:   120     LossContext: 0.00001496
    Epoch:   130     LossContext: 0.00001490
    Epoch:   140     LossContext: 0.00001486
    Epoch:   150     LossContext: 0.00001482
    Epoch:   160     LossContext: 0.00001478
    Epoch:   170     LossContext: 0.00001474
    Epoch:   180     LossContext: 0.00001471
    Epoch:   190     LossContext: 0.00001467
    Epoch:   200     LossContext: 0.00001463
    Epoch:   210     LossContext: 0.00001459
    Epoch:   220     LossContext: 0.00001455
    Epoch:   230     LossContext: 0.00001452
    Epoch:   240     LossContext: 0.00001448
    Epoch:   250     LossContext: 0.00001444
    Epoch:   260     LossContext: 0.00001440
    Epoch:   270     LossContext: 0.00001437
    Epoch:   280     LossContext: 0.00001433
    Epoch:   290     LossContext: 0.00001429
    Epoch:   300     LossContext: 0.00001425
    Epoch:   310     LossContext: 0.00001422
    Epoch:   320     LossContext: 0.00001418
    Epoch:   330     LossContext: 0.00001415
    Epoch:   340     LossContext: 0.00001411
    Epoch:   350     LossContext: 0.00001408
    Epoch:   360     LossContext: 0.00001405
    Epoch:   370     LossContext: 0.00001401
    Epoch:   380     LossContext: 0.00001398
    Epoch:   390     LossContext: 0.00001395
    Epoch:   400     LossContext: 0.00001392
    Epoch:   410     LossContext: 0.00001389
    Epoch:   420     LossContext: 0.00001387
    Epoch:   430     LossContext: 0.00001384
    Epoch:   440     LossContext: 0.00001381
    Epoch:   450     LossContext: 0.00001378
    Epoch:   460     LossContext: 0.00001376
    Epoch:   470     LossContext: 0.00001373
    Epoch:   480     LossContext: 0.00001371
    Epoch:   490     LossContext: 0.00001369
    Epoch:   500     LossContext: 0.00001366
    Epoch:   510     LossContext: 0.00001364
    Epoch:   520     LossContext: 0.00001362
    Epoch:   530     LossContext: 0.00001360
    Epoch:   540     LossContext: 0.00001358
    Epoch:   550     LossContext: 0.00001356
    Epoch:   560     LossContext: 0.00001354
    Epoch:   570     LossContext: 0.00001352
    Epoch:   580     LossContext: 0.00001350
    Epoch:   590     LossContext: 0.00001348
    Epoch:   600     LossContext: 0.00001346
    Epoch:   610     LossContext: 0.00001344
    Epoch:   620     LossContext: 0.00001342
    Epoch:   630     LossContext: 0.00001340
    Epoch:   640     LossContext: 0.00001338
    Epoch:   650     LossContext: 0.00001336
    Epoch:   660     LossContext: 0.00001334
    Epoch:   670     LossContext: 0.00001333
    Epoch:   680     LossContext: 0.00001332
    Epoch:   690     LossContext: 0.00001331
    Epoch:   700     LossContext: 0.00001330
    Epoch:   710     LossContext: 0.00001329
    Epoch:   720     LossContext: 0.00001328
    Epoch:   730     LossContext: 0.00001327
    Epoch:   740     LossContext: 0.00001326
    Epoch:   750     LossContext: 0.00001325
    Epoch:   760     LossContext: 0.00001324
    Epoch:   770     LossContext: 0.00001323
    Epoch:   780     LossContext: 0.00001322
    Epoch:   790     LossContext: 0.00001321
    Epoch:   800     LossContext: 0.00001320
    Epoch:   810     LossContext: 0.00001319
    Epoch:   820     LossContext: 0.00001318
    Epoch:   830     LossContext: 0.00001317
    Epoch:   840     LossContext: 0.00001316
    Epoch:   850     LossContext: 0.00001315
    Epoch:   860     LossContext: 0.00001314
    Epoch:   870     LossContext: 0.00001313
    Epoch:   880     LossContext: 0.00001312
    Epoch:   890     LossContext: 0.00001311
    Epoch:   900     LossContext: 0.00001310
    Epoch:   910     LossContext: 0.00001309
    Epoch:   920     LossContext: 0.00001308
    Epoch:   930     LossContext: 0.00001307
    Epoch:   940     LossContext: 0.00001306
    Epoch:   950     LossContext: 0.00001305
    Epoch:   960     LossContext: 0.00001304
    Epoch:   970     LossContext: 0.00001303
    Epoch:   980     LossContext: 0.00001302
    Epoch:   990     LossContext: 0.00001301
    Epoch:  1000     LossContext: 0.00001300
    Epoch:  1010     LossContext: 0.00001299
    Epoch:  1020     LossContext: 0.00001298
    Epoch:  1030     LossContext: 0.00001297
    Epoch:  1040     LossContext: 0.00001296
    Epoch:  1050     LossContext: 0.00001295
    Epoch:  1060     LossContext: 0.00001294
    Epoch:  1070     LossContext: 0.00001293
    Epoch:  1080     LossContext: 0.00001292
    Epoch:  1090     LossContext: 0.00001291
    Epoch:  1100     LossContext: 0.00001290
    Epoch:  1110     LossContext: 0.00001289
    Epoch:  1120     LossContext: 0.00001288
    Epoch:  1130     LossContext: 0.00001287
    Epoch:  1140     LossContext: 0.00001286
    Epoch:  1150     LossContext: 0.00001285
    Epoch:  1160     LossContext: 0.00001285
    Epoch:  1170     LossContext: 0.00001284
    Epoch:  1180     LossContext: 0.00001283
    Epoch:  1190     LossContext: 0.00001282
    Epoch:  1200     LossContext: 0.00001281
    Epoch:  1210     LossContext: 0.00001280
    Epoch:  1220     LossContext: 0.00001279
    Epoch:  1230     LossContext: 0.00001279
    Epoch:  1240     LossContext: 0.00001278
    Epoch:  1250     LossContext: 0.00001277
    Epoch:  1260     LossContext: 0.00001276
    Epoch:  1270     LossContext: 0.00001275
    Epoch:  1280     LossContext: 0.00001274
    Epoch:  1290     LossContext: 0.00001273
    Epoch:  1300     LossContext: 0.00001273
    Epoch:  1310     LossContext: 0.00001272
    Epoch:  1320     LossContext: 0.00001271
    Epoch:  1330     LossContext: 0.00001270
    Epoch:  1340     LossContext: 0.00001270
    Epoch:  1350     LossContext: 0.00001269
    Epoch:  1360     LossContext: 0.00001269
    Epoch:  1370     LossContext: 0.00001268
    Epoch:  1380     LossContext: 0.00001268
    Epoch:  1390     LossContext: 0.00001268
    Epoch:  1400     LossContext: 0.00001267
    Epoch:  1410     LossContext: 0.00001267
    Epoch:  1420     LossContext: 0.00001266
    Epoch:  1430     LossContext: 0.00001266
    Epoch:  1440     LossContext: 0.00001266
    Epoch:  1450     LossContext: 0.00001265
    Epoch:  1460     LossContext: 0.00001265
    Epoch:  1470     LossContext: 0.00001264
    Epoch:  1480     LossContext: 0.00001264
    Epoch:  1490     LossContext: 0.00001264
    Epoch:  1499     LossContext: 0.00001263

Gradient descent adaptation time: 0 hours 1 mins 40 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.02850140
    Epoch:     1     LossContext: 0.01410572
    Epoch:     2     LossContext: 0.00629640
    Epoch:     3     LossContext: 0.00262400
    Epoch:    10     LossContext: 0.00132877
    Epoch:    20     LossContext: 0.00216247
    Epoch:    30     LossContext: 0.00046270
    Epoch:    40     LossContext: 0.00014523
    Epoch:    50     LossContext: 0.00008262
    Epoch:    60     LossContext: 0.00005252
    Epoch:    70     LossContext: 0.00003327
    Epoch:    80     LossContext: 0.00003246
    Epoch:    90     LossContext: 0.00002766
    Epoch:   100     LossContext: 0.00002799
    Epoch:   110     LossContext: 0.00002716
    Epoch:   120     LossContext: 0.00002687
    Epoch:   130     LossContext: 0.00002666
    Epoch:   140     LossContext: 0.00002643
    Epoch:   150     LossContext: 0.00002624
    Epoch:   160     LossContext: 0.00002606
    Epoch:   170     LossContext: 0.00002591
    Epoch:   180     LossContext: 0.00002576
    Epoch:   190     LossContext: 0.00002563
    Epoch:   200     LossContext: 0.00002550
    Epoch:   210     LossContext: 0.00002538
    Epoch:   220     LossContext: 0.00002527
    Epoch:   230     LossContext: 0.00002517
    Epoch:   240     LossContext: 0.00002508
    Epoch:   250     LossContext: 0.00002498
    Epoch:   260     LossContext: 0.00002489
    Epoch:   270     LossContext: 0.00002481
    Epoch:   280     LossContext: 0.00002473
    Epoch:   290     LossContext: 0.00002465
    Epoch:   300     LossContext: 0.00002458
    Epoch:   310     LossContext: 0.00002451
    Epoch:   320     LossContext: 0.00002444
    Epoch:   330     LossContext: 0.00002437
    Epoch:   340     LossContext: 0.00002430
    Epoch:   350     LossContext: 0.00002423
    Epoch:   360     LossContext: 0.00002416
    Epoch:   370     LossContext: 0.00002410
    Epoch:   380     LossContext: 0.00002403
    Epoch:   390     LossContext: 0.00002396
    Epoch:   400     LossContext: 0.00002390
    Epoch:   410     LossContext: 0.00002384
    Epoch:   420     LossContext: 0.00002377
    Epoch:   430     LossContext: 0.00002371
    Epoch:   440     LossContext: 0.00002365
    Epoch:   450     LossContext: 0.00002360
    Epoch:   460     LossContext: 0.00002354
    Epoch:   470     LossContext: 0.00002349
    Epoch:   480     LossContext: 0.00002343
    Epoch:   490     LossContext: 0.00002338
    Epoch:   500     LossContext: 0.00002333
    Epoch:   510     LossContext: 0.00002327
    Epoch:   520     LossContext: 0.00002322
    Epoch:   530     LossContext: 0.00002318
    Epoch:   540     LossContext: 0.00002313
    Epoch:   550     LossContext: 0.00002308
    Epoch:   560     LossContext: 0.00002304
    Epoch:   570     LossContext: 0.00002300
    Epoch:   580     LossContext: 0.00002295
    Epoch:   590     LossContext: 0.00002291
    Epoch:   600     LossContext: 0.00002287
    Epoch:   610     LossContext: 0.00002283
    Epoch:   620     LossContext: 0.00002279
    Epoch:   630     LossContext: 0.00002275
    Epoch:   640     LossContext: 0.00002271
    Epoch:   650     LossContext: 0.00002267
    Epoch:   660     LossContext: 0.00002264
    Epoch:   670     LossContext: 0.00002261
    Epoch:   680     LossContext: 0.00002260
    Epoch:   690     LossContext: 0.00002258
    Epoch:   700     LossContext: 0.00002256
    Epoch:   710     LossContext: 0.00002255
    Epoch:   720     LossContext: 0.00002253
    Epoch:   730     LossContext: 0.00002251
    Epoch:   740     LossContext: 0.00002250
    Epoch:   750     LossContext: 0.00002248
    Epoch:   760     LossContext: 0.00002247
    Epoch:   770     LossContext: 0.00002245
    Epoch:   780     LossContext: 0.00002243
    Epoch:   790     LossContext: 0.00002241
    Epoch:   800     LossContext: 0.00002240
    Epoch:   810     LossContext: 0.00002238
    Epoch:   820     LossContext: 0.00002236
    Epoch:   830     LossContext: 0.00002235
    Epoch:   840     LossContext: 0.00002233
    Epoch:   850     LossContext: 0.00002232
    Epoch:   860     LossContext: 0.00002231
    Epoch:   870     LossContext: 0.00002230
    Epoch:   880     LossContext: 0.00002228
    Epoch:   890     LossContext: 0.00002227
    Epoch:   900     LossContext: 0.00002226
    Epoch:   910     LossContext: 0.00002224
    Epoch:   920     LossContext: 0.00002223
    Epoch:   930     LossContext: 0.00002222
    Epoch:   940     LossContext: 0.00002221
    Epoch:   950     LossContext: 0.00002219
    Epoch:   960     LossContext: 0.00002218
    Epoch:   970     LossContext: 0.00002217
    Epoch:   980     LossContext: 0.00002216
    Epoch:   990     LossContext: 0.00002215
    Epoch:  1000     LossContext: 0.00002214
    Epoch:  1010     LossContext: 0.00002213
    Epoch:  1020     LossContext: 0.00002212
    Epoch:  1030     LossContext: 0.00002211
    Epoch:  1040     LossContext: 0.00002210
    Epoch:  1050     LossContext: 0.00002209
    Epoch:  1060     LossContext: 0.00002208
    Epoch:  1070     LossContext: 0.00002207
    Epoch:  1080     LossContext: 0.00002206
    Epoch:  1090     LossContext: 0.00002205
    Epoch:  1100     LossContext: 0.00002204
    Epoch:  1110     LossContext: 0.00002203
    Epoch:  1120     LossContext: 0.00002202
    Epoch:  1130     LossContext: 0.00002201
    Epoch:  1140     LossContext: 0.00002201
    Epoch:  1150     LossContext: 0.00002200
    Epoch:  1160     LossContext: 0.00002199
    Epoch:  1170     LossContext: 0.00002198
    Epoch:  1180     LossContext: 0.00002197
    Epoch:  1190     LossContext: 0.00002196
    Epoch:  1200     LossContext: 0.00002195
    Epoch:  1210     LossContext: 0.00002195
    Epoch:  1220     LossContext: 0.00002194
    Epoch:  1230     LossContext: 0.00002193
    Epoch:  1240     LossContext: 0.00002192
    Epoch:  1250     LossContext: 0.00002191
    Epoch:  1260     LossContext: 0.00002190
    Epoch:  1270     LossContext: 0.00002189
    Epoch:  1280     LossContext: 0.00002188
    Epoch:  1290     LossContext: 0.00002187
    Epoch:  1300     LossContext: 0.00002187
    Epoch:  1310     LossContext: 0.00002186
    Epoch:  1320     LossContext: 0.00002185
    Epoch:  1330     LossContext: 0.00002184
    Epoch:  1340     LossContext: 0.00002183
    Epoch:  1350     LossContext: 0.00002183
    Epoch:  1360     LossContext: 0.00002182
    Epoch:  1370     LossContext: 0.00002182
    Epoch:  1380     LossContext: 0.00002181
    Epoch:  1390     LossContext: 0.00002181
    Epoch:  1400     LossContext: 0.00002180
    Epoch:  1410     LossContext: 0.00002180
    Epoch:  1420     LossContext: 0.00002180
    Epoch:  1430     LossContext: 0.00002179
    Epoch:  1440     LossContext: 0.00002179
    Epoch:  1450     LossContext: 0.00002178
    Epoch:  1460     LossContext: 0.00002178
    Epoch:  1470     LossContext: 0.00002177
    Epoch:  1480     LossContext: 0.00002177
    Epoch:  1490     LossContext: 0.00002176
    Epoch:  1499     LossContext: 0.00002176

Gradient descent adaptation time: 0 hours 1 mins 32 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.03380170
    Epoch:     1     LossContext: 0.01840451
    Epoch:     2     LossContext: 0.00636190
    Epoch:     3     LossContext: 0.00017656
    Epoch:    10     LossContext: 0.00034347
    Epoch:    20     LossContext: 0.00054890
    Epoch:    30     LossContext: 0.00067697
    Epoch:    40     LossContext: 0.00022774
    Epoch:    50     LossContext: 0.00001559
    Epoch:    60     LossContext: 0.00004578
    Epoch:    70     LossContext: 0.00001841
    Epoch:    80     LossContext: 0.00001796
    Epoch:    90     LossContext: 0.00001497
    Epoch:   100     LossContext: 0.00001521
    Epoch:   110     LossContext: 0.00001461
    Epoch:   120     LossContext: 0.00001459
    Epoch:   130     LossContext: 0.00001456
    Epoch:   140     LossContext: 0.00001452
    Epoch:   150     LossContext: 0.00001450
    Epoch:   160     LossContext: 0.00001447
    Epoch:   170     LossContext: 0.00001445
    Epoch:   180     LossContext: 0.00001443
    Epoch:   190     LossContext: 0.00001441
    Epoch:   200     LossContext: 0.00001439
    Epoch:   210     LossContext: 0.00001437
    Epoch:   220     LossContext: 0.00001435
    Epoch:   230     LossContext: 0.00001433
    Epoch:   240     LossContext: 0.00001431
    Epoch:   250     LossContext: 0.00001429
    Epoch:   260     LossContext: 0.00001426
    Epoch:   270     LossContext: 0.00001424
    Epoch:   280     LossContext: 0.00001422
    Epoch:   290     LossContext: 0.00001420
    Epoch:   300     LossContext: 0.00001418
    Epoch:   310     LossContext: 0.00001416
    Epoch:   320     LossContext: 0.00001414
    Epoch:   330     LossContext: 0.00001412
    Epoch:   340     LossContext: 0.00001410
    Epoch:   350     LossContext: 0.00001408
    Epoch:   360     LossContext: 0.00001407
    Epoch:   370     LossContext: 0.00001405
    Epoch:   380     LossContext: 0.00001403
    Epoch:   390     LossContext: 0.00001401
    Epoch:   400     LossContext: 0.00001399
    Epoch:   410     LossContext: 0.00001398
    Epoch:   420     LossContext: 0.00001396
    Epoch:   430     LossContext: 0.00001394
    Epoch:   440     LossContext: 0.00001393
    Epoch:   450     LossContext: 0.00001392
    Epoch:   460     LossContext: 0.00001390
    Epoch:   470     LossContext: 0.00001389
    Epoch:   480     LossContext: 0.00001387
    Epoch:   490     LossContext: 0.00001386
    Epoch:   500     LossContext: 0.00001385
    Epoch:   510     LossContext: 0.00001384
    Epoch:   520     LossContext: 0.00001382
    Epoch:   530     LossContext: 0.00001381
    Epoch:   540     LossContext: 0.00001380
    Epoch:   550     LossContext: 0.00001379
    Epoch:   560     LossContext: 0.00001378
    Epoch:   570     LossContext: 0.00001376
    Epoch:   580     LossContext: 0.00001375
    Epoch:   590     LossContext: 0.00001374
    Epoch:   600     LossContext: 0.00001373
    Epoch:   610     LossContext: 0.00001371
    Epoch:   620     LossContext: 0.00001370
    Epoch:   630     LossContext: 0.00001369
    Epoch:   640     LossContext: 0.00001367
    Epoch:   650     LossContext: 0.00001366
    Epoch:   660     LossContext: 0.00001365
    Epoch:   670     LossContext: 0.00001364
    Epoch:   680     LossContext: 0.00001363
    Epoch:   690     LossContext: 0.00001362
    Epoch:   700     LossContext: 0.00001362
    Epoch:   710     LossContext: 0.00001361
    Epoch:   720     LossContext: 0.00001360
    Epoch:   730     LossContext: 0.00001359
    Epoch:   740     LossContext: 0.00001359
    Epoch:   750     LossContext: 0.00001358
    Epoch:   760     LossContext: 0.00001357
    Epoch:   770     LossContext: 0.00001356
    Epoch:   780     LossContext: 0.00001356
    Epoch:   790     LossContext: 0.00001355
    Epoch:   800     LossContext: 0.00001354
    Epoch:   810     LossContext: 0.00001353
    Epoch:   820     LossContext: 0.00001353
    Epoch:   830     LossContext: 0.00001352
    Epoch:   840     LossContext: 0.00001351
    Epoch:   850     LossContext: 0.00001350
    Epoch:   860     LossContext: 0.00001350
    Epoch:   870     LossContext: 0.00001349
    Epoch:   880     LossContext: 0.00001348
    Epoch:   890     LossContext: 0.00001347
    Epoch:   900     LossContext: 0.00001346
    Epoch:   910     LossContext: 0.00001346
    Epoch:   920     LossContext: 0.00001345
    Epoch:   930     LossContext: 0.00001344
    Epoch:   940     LossContext: 0.00001343
    Epoch:   950     LossContext: 0.00001342
    Epoch:   960     LossContext: 0.00001342
    Epoch:   970     LossContext: 0.00001341
    Epoch:   980     LossContext: 0.00001340
    Epoch:   990     LossContext: 0.00001339
    Epoch:  1000     LossContext: 0.00001338
    Epoch:  1010     LossContext: 0.00001337
    Epoch:  1020     LossContext: 0.00001337
    Epoch:  1030     LossContext: 0.00001336
    Epoch:  1040     LossContext: 0.00001335
    Epoch:  1050     LossContext: 0.00001334
    Epoch:  1060     LossContext: 0.00001334
    Epoch:  1070     LossContext: 0.00001333
    Epoch:  1080     LossContext: 0.00001332
    Epoch:  1090     LossContext: 0.00001331
    Epoch:  1100     LossContext: 0.00001330
    Epoch:  1110     LossContext: 0.00001330
    Epoch:  1120     LossContext: 0.00001329
    Epoch:  1130     LossContext: 0.00001328
    Epoch:  1140     LossContext: 0.00001327
    Epoch:  1150     LossContext: 0.00001326
    Epoch:  1160     LossContext: 0.00001326
    Epoch:  1170     LossContext: 0.00001325
    Epoch:  1180     LossContext: 0.00001324
    Epoch:  1190     LossContext: 0.00001323
    Epoch:  1200     LossContext: 0.00001323
    Epoch:  1210     LossContext: 0.00001322
    Epoch:  1220     LossContext: 0.00001321
    Epoch:  1230     LossContext: 0.00001320
    Epoch:  1240     LossContext: 0.00001319
    Epoch:  1250     LossContext: 0.00001319
    Epoch:  1260     LossContext: 0.00001318
    Epoch:  1270     LossContext: 0.00001317
    Epoch:  1280     LossContext: 0.00001316
    Epoch:  1290     LossContext: 0.00001316
    Epoch:  1300     LossContext: 0.00001315
    Epoch:  1310     LossContext: 0.00001314
    Epoch:  1320     LossContext: 0.00001314
    Epoch:  1330     LossContext: 0.00001313
    Epoch:  1340     LossContext: 0.00001312
    Epoch:  1350     LossContext: 0.00001312
    Epoch:  1360     LossContext: 0.00001312
    Epoch:  1370     LossContext: 0.00001311
    Epoch:  1380     LossContext: 0.00001311
    Epoch:  1390     LossContext: 0.00001311
    Epoch:  1400     LossContext: 0.00001310
    Epoch:  1410     LossContext: 0.00001310
    Epoch:  1420     LossContext: 0.00001310
    Epoch:  1430     LossContext: 0.00001309
    Epoch:  1440     LossContext: 0.00001309
    Epoch:  1450     LossContext: 0.00001309
    Epoch:  1460     LossContext: 0.00001308
    Epoch:  1470     LossContext: 0.00001308
    Epoch:  1480     LossContext: 0.00001308
    Epoch:  1490     LossContext: 0.00001308
    Epoch:  1499     LossContext: 0.00001307

Gradient descent adaptation time: 0 hours 1 mins 32 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.02674557
    Epoch:     1     LossContext: 0.01319233
    Epoch:     2     LossContext: 0.00604570
    Epoch:     3     LossContext: 0.00267959
    Epoch:    10     LossContext: 0.00126846
    Epoch:    20     LossContext: 0.00202272
    Epoch:    30     LossContext: 0.00037063
    Epoch:    40     LossContext: 0.00016020
    Epoch:    50     LossContext: 0.00006604
    Epoch:    60     LossContext: 0.00005135
    Epoch:    70     LossContext: 0.00002899
    Epoch:    80     LossContext: 0.00003007
    Epoch:    90     LossContext: 0.00002537
    Epoch:   100     LossContext: 0.00002572
    Epoch:   110     LossContext: 0.00002510
    Epoch:   120     LossContext: 0.00002487
    Epoch:   130     LossContext: 0.00002474
    Epoch:   140     LossContext: 0.00002459
    Epoch:   150     LossContext: 0.00002446
    Epoch:   160     LossContext: 0.00002435
    Epoch:   170     LossContext: 0.00002424
    Epoch:   180     LossContext: 0.00002414
    Epoch:   190     LossContext: 0.00002405
    Epoch:   200     LossContext: 0.00002396
    Epoch:   210     LossContext: 0.00002387
    Epoch:   220     LossContext: 0.00002380
    Epoch:   230     LossContext: 0.00002372
    Epoch:   240     LossContext: 0.00002365
    Epoch:   250     LossContext: 0.00002358
    Epoch:   260     LossContext: 0.00002351
    Epoch:   270     LossContext: 0.00002344
    Epoch:   280     LossContext: 0.00002338
    Epoch:   290     LossContext: 0.00002332
    Epoch:   300     LossContext: 0.00002326
    Epoch:   310     LossContext: 0.00002321
    Epoch:   320     LossContext: 0.00002315
    Epoch:   330     LossContext: 0.00002309
    Epoch:   340     LossContext: 0.00002303
    Epoch:   350     LossContext: 0.00002298
    Epoch:   360     LossContext: 0.00002292
    Epoch:   370     LossContext: 0.00002287
    Epoch:   380     LossContext: 0.00002282
    Epoch:   390     LossContext: 0.00002277
    Epoch:   400     LossContext: 0.00002272
    Epoch:   410     LossContext: 0.00002268
    Epoch:   420     LossContext: 0.00002263
    Epoch:   430     LossContext: 0.00002259
    Epoch:   440     LossContext: 0.00002254
    Epoch:   450     LossContext: 0.00002250
    Epoch:   460     LossContext: 0.00002246
    Epoch:   470     LossContext: 0.00002241
    Epoch:   480     LossContext: 0.00002238
    Epoch:   490     LossContext: 0.00002234
    Epoch:   500     LossContext: 0.00002230
    Epoch:   510     LossContext: 0.00002227
    Epoch:   520     LossContext: 0.00002223
    Epoch:   530     LossContext: 0.00002220
    Epoch:   540     LossContext: 0.00002217
    Epoch:   550     LossContext: 0.00002214
    Epoch:   560     LossContext: 0.00002211
    Epoch:   570     LossContext: 0.00002208
    Epoch:   580     LossContext: 0.00002205
    Epoch:   590     LossContext: 0.00002202
    Epoch:   600     LossContext: 0.00002198
    Epoch:   610     LossContext: 0.00002195
    Epoch:   620     LossContext: 0.00002192
    Epoch:   630     LossContext: 0.00002189
    Epoch:   640     LossContext: 0.00002186
    Epoch:   650     LossContext: 0.00002183
    Epoch:   660     LossContext: 0.00002180
    Epoch:   670     LossContext: 0.00002178
    Epoch:   680     LossContext: 0.00002176
    Epoch:   690     LossContext: 0.00002175
    Epoch:   700     LossContext: 0.00002173
    Epoch:   710     LossContext: 0.00002172
    Epoch:   720     LossContext: 0.00002170
    Epoch:   730     LossContext: 0.00002169
    Epoch:   740     LossContext: 0.00002167
    Epoch:   750     LossContext: 0.00002166
    Epoch:   760     LossContext: 0.00002164
    Epoch:   770     LossContext: 0.00002163
    Epoch:   780     LossContext: 0.00002161
    Epoch:   790     LossContext: 0.00002160
    Epoch:   800     LossContext: 0.00002158
    Epoch:   810     LossContext: 0.00002157
    Epoch:   820     LossContext: 0.00002156
    Epoch:   830     LossContext: 0.00002154
    Epoch:   840     LossContext: 0.00002153
    Epoch:   850     LossContext: 0.00002152
    Epoch:   860     LossContext: 0.00002151
    Epoch:   870     LossContext: 0.00002150
    Epoch:   880     LossContext: 0.00002148
    Epoch:   890     LossContext: 0.00002147
    Epoch:   900     LossContext: 0.00002146
    Epoch:   910     LossContext: 0.00002145
    Epoch:   920     LossContext: 0.00002144
    Epoch:   930     LossContext: 0.00002143
    Epoch:   940     LossContext: 0.00002142
    Epoch:   950     LossContext: 0.00002141
    Epoch:   960     LossContext: 0.00002140
    Epoch:   970     LossContext: 0.00002139
    Epoch:   980     LossContext: 0.00002138
    Epoch:   990     LossContext: 0.00002137
    Epoch:  1000     LossContext: 0.00002136
    Epoch:  1010     LossContext: 0.00002136
    Epoch:  1020     LossContext: 0.00002135
    Epoch:  1030     LossContext: 0.00002134
    Epoch:  1040     LossContext: 0.00002133
    Epoch:  1050     LossContext: 0.00002132
    Epoch:  1060     LossContext: 0.00002131
    Epoch:  1070     LossContext: 0.00002131
    Epoch:  1080     LossContext: 0.00002130
    Epoch:  1090     LossContext: 0.00002129
    Epoch:  1100     LossContext: 0.00002128
    Epoch:  1110     LossContext: 0.00002127
    Epoch:  1120     LossContext: 0.00002126
    Epoch:  1130     LossContext: 0.00002126
    Epoch:  1140     LossContext: 0.00002125
    Epoch:  1150     LossContext: 0.00002124
    Epoch:  1160     LossContext: 0.00002123
    Epoch:  1170     LossContext: 0.00002123
    Epoch:  1180     LossContext: 0.00002122
    Epoch:  1190     LossContext: 0.00002121
    Epoch:  1200     LossContext: 0.00002120
    Epoch:  1210     LossContext: 0.00002119
    Epoch:  1220     LossContext: 0.00002118
    Epoch:  1230     LossContext: 0.00002118
    Epoch:  1240     LossContext: 0.00002117
    Epoch:  1250     LossContext: 0.00002116
    Epoch:  1260     LossContext: 0.00002115
    Epoch:  1270     LossContext: 0.00002114
    Epoch:  1280     LossContext: 0.00002113
    Epoch:  1290     LossContext: 0.00002113
    Epoch:  1300     LossContext: 0.00002112
    Epoch:  1310     LossContext: 0.00002111
    Epoch:  1320     LossContext: 0.00002110
    Epoch:  1330     LossContext: 0.00002109
    Epoch:  1340     LossContext: 0.00002109
    Epoch:  1350     LossContext: 0.00002108
    Epoch:  1360     LossContext: 0.00002108
    Epoch:  1370     LossContext: 0.00002107
    Epoch:  1380     LossContext: 0.00002107
    Epoch:  1390     LossContext: 0.00002106
    Epoch:  1400     LossContext: 0.00002106
    Epoch:  1410     LossContext: 0.00002106
    Epoch:  1420     LossContext: 0.00002105
    Epoch:  1430     LossContext: 0.00002105
    Epoch:  1440     LossContext: 0.00002104
    Epoch:  1450     LossContext: 0.00002104
    Epoch:  1460     LossContext: 0.00002103
    Epoch:  1470     LossContext: 0.00002103
    Epoch:  1480     LossContext: 0.00002102
    Epoch:  1490     LossContext: 0.00002102
    Epoch:  1499     LossContext: 0.00002101

Gradient descent adaptation time: 0 hours 1 mins 32 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./runs/25032024-162853/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 1.736083e-06

==  Begining out-of-distribution visualisation ... ==
    Environment id: 1
    Trajectory id: 0
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/25032024-162853/adapt/results_ood.png

Full evaluation of the model on many random seeds

2024-03-28 02:00:46.163269: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error:
RESOURCE_EXHAUSTED: Failed to get module function:CUDA_ERROR_OUT_OF_MEMORY: out of memory
2024-03-28 02:00:46.163304: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2732] Execution of replica 0 failed: INTERNAL: Failed to instantiate GPU graphs: CaptureGpuGraph failed (Failed to get module function:CUDA_ERROR_OUT_OF_MEMORY: out of memory; current tracing scope: fusion.7): INTERNAL: failed to capture gpu graph: run time error: custom call 'xla.gpu.func.launch' failed

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Traceback (most recent call last):
  File "/home/gb21553/Projects/Nodax/examples/lotka-voltera/dataset.py", line 178, in <module>
    ys = RK4(lotka_volterra, 
         ^^^^^^^^^^^^^^^^^^^
  File "/home/gb21553/Projects/Nodax/nodax/learner.py", line 174, in RK4
    _, ys = jax.lax.scan(step, (t_solve[0], y0), t_solve[:])
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to instantiate GPU graphs: CaptureGpuGraph failed (Failed to get module function:CUDA_ERROR_OUT_OF_MEMORY: out of memory; current tracing scope: fusion.7): INTERNAL: failed to capture gpu graph: run time error: custom call 'xla.gpu.func.launch' failed
--------------------
For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
2024-03-28 02:00:55.958396: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error:
RESOURCE_EXHAUSTED: Failed to get module function:CUDA_ERROR_OUT_OF_MEMORY: out of memory
2024-03-28 02:00:55.958439: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2732] Execution of replica 0 failed: INTERNAL: Failed to instantiate GPU graphs: CaptureGpuGraph failed (Failed to get module function:CUDA_ERROR_OUT_OF_MEMORY: out of memory; current tracing scope: fusion.7): INTERNAL: failed to capture gpu graph: run time error: custom call 'xla.gpu.func.launch' failed

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Traceback (most recent call last):
  File "/home/gb21553/Projects/Nodax/examples/lotka-voltera/dataset.py", line 178, in <module>
    ys = RK4(lotka_volterra, 
         ^^^^^^^^^^^^^^^^^^^
  File "/home/gb21553/Projects/Nodax/nodax/learner.py", line 174, in RK4
    _, ys = jax.lax.scan(step, (t_solve[0], y0), t_solve[:])
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to instantiate GPU graphs: CaptureGpuGraph failed (Failed to get module function:CUDA_ERROR_OUT_OF_MEMORY: out of memory; current tracing scope: fusion.7): INTERNAL: failed to capture gpu graph: run time error: custom call 'xla.gpu.func.launch' failed
--------------------
For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
2024-03-28 02:01:05.573912: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error:
RESOURCE_EXHAUSTED: Failed to get module function:CUDA_ERROR_OUT_OF_MEMORY: out of memory
2024-03-28 02:01:05.573940: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2732] Execution of replica 0 failed: INTERNAL: Failed to instantiate GPU graphs: CaptureGpuGraph failed (Failed to get module function:CUDA_ERROR_OUT_OF_MEMORY: out of memory; current tracing scope: fusion.7): INTERNAL: failed to capture gpu graph: run time error: custom call 'xla.gpu.func.launch' failed

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Traceback (most recent call last):
  File "/home/gb21553/Projects/Nodax/examples/lotka-voltera/dataset.py", line 178, in <module>
    ys = RK4(lotka_volterra, 
         ^^^^^^^^^^^^^^^^^^^
  File "/home/gb21553/Projects/Nodax/nodax/learner.py", line 174, in RK4
    _, ys = jax.lax.scan(step, (t_solve[0], y0), t_solve[:])
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to instantiate GPU graphs: CaptureGpuGraph failed (Failed to get module function:CUDA_ERROR_OUT_OF_MEMORY: out of memory; current tracing scope: fusion.7): INTERNAL: failed to capture gpu graph: run time error: custom call 'xla.gpu.func.launch' failed
--------------------
For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
2024-03-28 02:01:15.337411: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error:
RESOURCE_EXHAUSTED: Failed to get module function:CUDA_ERROR_OUT_OF_MEMORY: out of memory
2024-03-28 02:01:15.337441: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2732] Execution of replica 0 failed: INTERNAL: Failed to instantiate GPU graphs: CaptureGpuGraph failed (Failed to get module function:CUDA_ERROR_OUT_OF_MEMORY: out of memory; current tracing scope: fusion.7): INTERNAL: failed to capture gpu graph: run time error: custom call 'xla.gpu.func.launch' failed

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Traceback (most recent call last):
  File "/home/gb21553/Projects/Nodax/examples/lotka-voltera/dataset.py", line 178, in <module>
    ys = RK4(lotka_volterra, 
         ^^^^^^^^^^^^^^^^^^^
  File "/home/gb21553/Projects/Nodax/nodax/learner.py", line 174, in RK4
    _, ys = jax.lax.scan(step, (t_solve[0], y0), t_solve[:])
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to instantiate GPU graphs: CaptureGpuGraph failed (Failed to get module function:CUDA_ERROR_OUT_OF_MEMORY: out of memory; current tracing scope: fusion.7): INTERNAL: failed to capture gpu graph: run time error: custom call 'xla.gpu.func.launch' failed
--------------------
For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
2024-03-28 02:01:25.240944: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error:
RESOURCE_EXHAUSTED: Failed to get module function:CUDA_ERROR_OUT_OF_MEMORY: out of memory
2024-03-28 02:01:25.240977: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2732] Execution of replica 0 failed: INTERNAL: Failed to instantiate GPU graphs: CaptureGpuGraph failed (Failed to get module function:CUDA_ERROR_OUT_OF_MEMORY: out of memory; current tracing scope: fusion.7): INTERNAL: failed to capture gpu graph: run time error: custom call 'xla.gpu.func.launch' failed

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Traceback (most recent call last):
  File "/home/gb21553/Projects/Nodax/examples/lotka-voltera/dataset.py", line 178, in <module>
    ys = RK4(lotka_volterra, 
         ^^^^^^^^^^^^^^^^^^^
  File "/home/gb21553/Projects/Nodax/nodax/learner.py", line 174, in RK4
    _, ys = jax.lax.scan(step, (t_solve[0], y0), t_solve[:])
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to instantiate GPU graphs: CaptureGpuGraph failed (Failed to get module function:CUDA_ERROR_OUT_OF_MEMORY: out of memory; current tracing scope: fusion.7): INTERNAL: failed to capture gpu graph: run time error: custom call 'xla.gpu.func.launch' failed
--------------------
For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Mean and std of the scores across various datasets

