
############# Neural Context Flow #############

Jax version: 0.4.28
Available devices: [cuda(id=0)]
No training. Loading data and results from: ./
WARNING: You did not provide a dataloader id. A new one has been generated: 185820
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 185820
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 308240 


WARNING: No key provided for the context initialization. Initializing at 0.

No training, loading model and results from ./ folder ...

WARNING: You did not provide a dataloader id. A new one has been generated: 185821
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 3.655587e-05


############# Neural Context Flow #############

Jax version: 0.4.28
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./adapt/
 Seed: 6078


############# Neural Context Flow #############

Jax version: 0.4.28
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt_test
 Savepath: ./adapt/
 Seed: 6078

==  Begining in-domain visualisation ... ==
    Environment id: 5
    Trajectory id: 18
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./results_in_domain.png
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.07344241
    Epoch:     1     LossContext: 0.05697871
    Epoch:     2     LossContext: 0.03931348
    Epoch:     3     LossContext: 0.02211238
    Epoch:    10     LossContext: 0.01375907
    Epoch:    20     LossContext: 0.00429381
    Epoch:    30     LossContext: 0.00088682
    Epoch:    40     LossContext: 0.00023399
    Epoch:    50     LossContext: 0.00007977
    Epoch:    60     LossContext: 0.00005319
    Epoch:    70     LossContext: 0.00004331
    Epoch:    80     LossContext: 0.00003853
    Epoch:    90     LossContext: 0.00003430
    Epoch:   100     LossContext: 0.00003147
    Epoch:   110     LossContext: 0.00003037
    Epoch:   120     LossContext: 0.00003019
    Epoch:   130     LossContext: 0.00003019
    Epoch:   140     LossContext: 0.00003012
    Epoch:   150     LossContext: 0.00003005
    Epoch:   160     LossContext: 0.00003001
    Epoch:   170     LossContext: 0.00002997
    Epoch:   180     LossContext: 0.00002992
    Epoch:   190     LossContext: 0.00002988
    Epoch:   200     LossContext: 0.00002984
    Epoch:   210     LossContext: 0.00002979
    Epoch:   220     LossContext: 0.00002975
    Epoch:   230     LossContext: 0.00002971
    Epoch:   240     LossContext: 0.00002966
    Epoch:   250     LossContext: 0.00002962
    Epoch:   260     LossContext: 0.00002957
    Epoch:   270     LossContext: 0.00002952
    Epoch:   280     LossContext: 0.00002948
    Epoch:   290     LossContext: 0.00002943
    Epoch:   300     LossContext: 0.00002938
    Epoch:   310     LossContext: 0.00002933
    Epoch:   320     LossContext: 0.00002928
    Epoch:   330     LossContext: 0.00002923
    Epoch:   340     LossContext: 0.00002918
    Epoch:   350     LossContext: 0.00002913
    Epoch:   360     LossContext: 0.00002908
    Epoch:   370     LossContext: 0.00002903
    Epoch:   380     LossContext: 0.00002898
    Epoch:   390     LossContext: 0.00002893
    Epoch:   400     LossContext: 0.00002888
    Epoch:   410     LossContext: 0.00002883
    Epoch:   420     LossContext: 0.00002878
    Epoch:   430     LossContext: 0.00002873
    Epoch:   440     LossContext: 0.00002868
    Epoch:   450     LossContext: 0.00002864
    Epoch:   460     LossContext: 0.00002859
    Epoch:   470     LossContext: 0.00002854
    Epoch:   480     LossContext: 0.00002850
    Epoch:   490     LossContext: 0.00002845
    Epoch:   500     LossContext: 0.00002840
    Epoch:   510     LossContext: 0.00002836
    Epoch:   520     LossContext: 0.00002831
    Epoch:   530     LossContext: 0.00002826
    Epoch:   540     LossContext: 0.00002822
    Epoch:   550     LossContext: 0.00002817
    Epoch:   560     LossContext: 0.00002812
    Epoch:   570     LossContext: 0.00002807
    Epoch:   580     LossContext: 0.00002803
    Epoch:   590     LossContext: 0.00002798
    Epoch:   600     LossContext: 0.00002793
    Epoch:   610     LossContext: 0.00002789
    Epoch:   620     LossContext: 0.00002784
    Epoch:   630     LossContext: 0.00002780
    Epoch:   640     LossContext: 0.00002775
    Epoch:   650     LossContext: 0.00002771
    Epoch:   660     LossContext: 0.00002767
    Epoch:   670     LossContext: 0.00002762
    Epoch:   680     LossContext: 0.00002758
    Epoch:   690     LossContext: 0.00002754
    Epoch:   700     LossContext: 0.00002749
    Epoch:   710     LossContext: 0.00002745
    Epoch:   720     LossContext: 0.00002741
    Epoch:   730     LossContext: 0.00002737
    Epoch:   740     LossContext: 0.00002732
    Epoch:   750     LossContext: 0.00002728
    Epoch:   760     LossContext: 0.00002724
    Epoch:   770     LossContext: 0.00002720
    Epoch:   780     LossContext: 0.00002716
    Epoch:   790     LossContext: 0.00002712
    Epoch:   800     LossContext: 0.00002708
    Epoch:   810     LossContext: 0.00002704
    Epoch:   820     LossContext: 0.00002700
    Epoch:   830     LossContext: 0.00002696
    Epoch:   840     LossContext: 0.00002692
    Epoch:   850     LossContext: 0.00002688
    Epoch:   860     LossContext: 0.00002684
    Epoch:   870     LossContext: 0.00002680
    Epoch:   880     LossContext: 0.00002677
    Epoch:   890     LossContext: 0.00002673
    Epoch:   900     LossContext: 0.00002669
    Epoch:   910     LossContext: 0.00002666
    Epoch:   920     LossContext: 0.00002662
    Epoch:   930     LossContext: 0.00002658
    Epoch:   940     LossContext: 0.00002655
    Epoch:   950     LossContext: 0.00002651
    Epoch:   960     LossContext: 0.00002648
    Epoch:   970     LossContext: 0.00002644
    Epoch:   980     LossContext: 0.00002641
    Epoch:   990     LossContext: 0.00002650
    Epoch:  1000     LossContext: 0.00003222
    Epoch:  1010     LossContext: 0.00006597
    Epoch:  1020     LossContext: 0.00002631
    Epoch:  1030     LossContext: 0.00003220
    Epoch:  1040     LossContext: 0.00002742
    Epoch:  1050     LossContext: 0.00002619
    Epoch:  1060     LossContext: 0.00002624
    Epoch:  1070     LossContext: 0.00002615
    Epoch:  1080     LossContext: 0.00002608
    Epoch:  1090     LossContext: 0.00002610
    Epoch:  1100     LossContext: 0.00002604
    Epoch:  1110     LossContext: 0.00002609
    Epoch:  1120     LossContext: 0.00002820
    Epoch:  1130     LossContext: 0.00007942
    Epoch:  1140     LossContext: 0.00004669
    Epoch:  1150     LossContext: 0.00003214
    Epoch:  1160     LossContext: 0.00002630
    Epoch:  1170     LossContext: 0.00002584
    Epoch:  1180     LossContext: 0.00002581
    Epoch:  1190     LossContext: 0.00002588
    Epoch:  1200     LossContext: 0.00002588
    Epoch:  1210     LossContext: 0.00002582
    Epoch:  1220     LossContext: 0.00002594
    Epoch:  1230     LossContext: 0.00002948
    Epoch:  1240     LossContext: 0.00007558
    Epoch:  1250     LossContext: 0.00004286
    Epoch:  1260     LossContext: 0.00003236
    Epoch:  1270     LossContext: 0.00002809
    Epoch:  1280     LossContext: 0.00002669
    Epoch:  1290     LossContext: 0.00002589
    Epoch:  1300     LossContext: 0.00002558
    Epoch:  1310     LossContext: 0.00002558
    Epoch:  1320     LossContext: 0.00002624
    Epoch:  1330     LossContext: 0.00003631
    Epoch:  1340     LossContext: 0.00005105
    Epoch:  1350     LossContext: 0.00003570
    Epoch:  1360     LossContext: 0.00002972
    Epoch:  1370     LossContext: 0.00002731
    Epoch:  1380     LossContext: 0.00002556
    Epoch:  1390     LossContext: 0.00002581
    Epoch:  1400     LossContext: 0.00002540
    Epoch:  1410     LossContext: 0.00002567
    Epoch:  1420     LossContext: 0.00003448
    Epoch:  1430     LossContext: 0.00006820
    Epoch:  1440     LossContext: 0.00003535
    Epoch:  1450     LossContext: 0.00002621
    Epoch:  1460     LossContext: 0.00002527
    Epoch:  1470     LossContext: 0.00002526
    Epoch:  1480     LossContext: 0.00002543
    Epoch:  1490     LossContext: 0.00002536
    Epoch:  1499     LossContext: 0.00002527

Gradient descent adaptation time: 0 hours 1 mins 41 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.17901951
    Epoch:     1     LossContext: 0.12534735
    Epoch:     2     LossContext: 0.07607574
    Epoch:     3     LossContext: 0.03661244
    Epoch:    10     LossContext: 0.03226256
    Epoch:    20     LossContext: 0.01171148
    Epoch:    30     LossContext: 0.00389009
    Epoch:    40     LossContext: 0.00132580
    Epoch:    50     LossContext: 0.00027920
    Epoch:    60     LossContext: 0.00005294
    Epoch:    70     LossContext: 0.00007842
    Epoch:    80     LossContext: 0.00008036
    Epoch:    90     LossContext: 0.00005487
    Epoch:   100     LossContext: 0.00005063
    Epoch:   110     LossContext: 0.00005120
    Epoch:   120     LossContext: 0.00004974
    Epoch:   130     LossContext: 0.00004979
    Epoch:   140     LossContext: 0.00004956
    Epoch:   150     LossContext: 0.00004948
    Epoch:   160     LossContext: 0.00004938
    Epoch:   170     LossContext: 0.00004929
    Epoch:   180     LossContext: 0.00004919
    Epoch:   190     LossContext: 0.00004910
    Epoch:   200     LossContext: 0.00004900
    Epoch:   210     LossContext: 0.00004890
    Epoch:   220     LossContext: 0.00004880
    Epoch:   230     LossContext: 0.00004869
    Epoch:   240     LossContext: 0.00004859
    Epoch:   250     LossContext: 0.00004848
    Epoch:   260     LossContext: 0.00004837
    Epoch:   270     LossContext: 0.00004827
    Epoch:   280     LossContext: 0.00004815
    Epoch:   290     LossContext: 0.00004804
    Epoch:   300     LossContext: 0.00004793
    Epoch:   310     LossContext: 0.00004782
    Epoch:   320     LossContext: 0.00004770
    Epoch:   330     LossContext: 0.00004759
    Epoch:   340     LossContext: 0.00004747
    Epoch:   350     LossContext: 0.00004736
    Epoch:   360     LossContext: 0.00004724
    Epoch:   370     LossContext: 0.00004713
    Epoch:   380     LossContext: 0.00004701
    Epoch:   390     LossContext: 0.00004690
    Epoch:   400     LossContext: 0.00004678
    Epoch:   410     LossContext: 0.00004666
    Epoch:   420     LossContext: 0.00004654
    Epoch:   430     LossContext: 0.00004642
    Epoch:   440     LossContext: 0.00004630
    Epoch:   450     LossContext: 0.00004618
    Epoch:   460     LossContext: 0.00004606
    Epoch:   470     LossContext: 0.00004594
    Epoch:   480     LossContext: 0.00004582
    Epoch:   490     LossContext: 0.00004569
    Epoch:   500     LossContext: 0.00004557
    Epoch:   510     LossContext: 0.00004545
    Epoch:   520     LossContext: 0.00004533
    Epoch:   530     LossContext: 0.00004521
    Epoch:   540     LossContext: 0.00004508
    Epoch:   550     LossContext: 0.00004496
    Epoch:   560     LossContext: 0.00004484
    Epoch:   570     LossContext: 0.00004472
    Epoch:   580     LossContext: 0.00004460
    Epoch:   590     LossContext: 0.00004447
    Epoch:   600     LossContext: 0.00004436
    Epoch:   610     LossContext: 0.00004424
    Epoch:   620     LossContext: 0.00004412
    Epoch:   630     LossContext: 0.00004401
    Epoch:   640     LossContext: 0.00004389
    Epoch:   650     LossContext: 0.00004377
    Epoch:   660     LossContext: 0.00004366
    Epoch:   670     LossContext: 0.00004354
    Epoch:   680     LossContext: 0.00004342
    Epoch:   690     LossContext: 0.00004331
    Epoch:   700     LossContext: 0.00004319
    Epoch:   710     LossContext: 0.00004307
    Epoch:   720     LossContext: 0.00004296
    Epoch:   730     LossContext: 0.00004284
    Epoch:   740     LossContext: 0.00004273
    Epoch:   750     LossContext: 0.00004262
    Epoch:   760     LossContext: 0.00004250
    Epoch:   770     LossContext: 0.00004239
    Epoch:   780     LossContext: 0.00004228
    Epoch:   790     LossContext: 0.00004216
    Epoch:   800     LossContext: 0.00004205
    Epoch:   810     LossContext: 0.00004194
    Epoch:   820     LossContext: 0.00004182
    Epoch:   830     LossContext: 0.00004171
    Epoch:   840     LossContext: 0.00004160
    Epoch:   850     LossContext: 0.00004150
    Epoch:   860     LossContext: 0.00004139
    Epoch:   870     LossContext: 0.00004128
    Epoch:   880     LossContext: 0.00004117
    Epoch:   890     LossContext: 0.00004107
    Epoch:   900     LossContext: 0.00004096
    Epoch:   910     LossContext: 0.00004085
    Epoch:   920     LossContext: 0.00004074
    Epoch:   930     LossContext: 0.00004064
    Epoch:   940     LossContext: 0.00004053
    Epoch:   950     LossContext: 0.00004043
    Epoch:   960     LossContext: 0.00004033
    Epoch:   970     LossContext: 0.00004022
    Epoch:   980     LossContext: 0.00004012
    Epoch:   990     LossContext: 0.00004001
    Epoch:  1000     LossContext: 0.00003991
    Epoch:  1010     LossContext: 0.00003981
    Epoch:  1020     LossContext: 0.00003971
    Epoch:  1030     LossContext: 0.00003961
    Epoch:  1040     LossContext: 0.00003951
    Epoch:  1050     LossContext: 0.00003941
    Epoch:  1060     LossContext: 0.00003931
    Epoch:  1070     LossContext: 0.00003921
    Epoch:  1080     LossContext: 0.00003911
    Epoch:  1090     LossContext: 0.00003901
    Epoch:  1100     LossContext: 0.00003891
    Epoch:  1110     LossContext: 0.00003882
    Epoch:  1120     LossContext: 0.00003872
    Epoch:  1130     LossContext: 0.00003862
    Epoch:  1140     LossContext: 0.00003853
    Epoch:  1150     LossContext: 0.00003844
    Epoch:  1160     LossContext: 0.00003834
    Epoch:  1170     LossContext: 0.00003825
    Epoch:  1180     LossContext: 0.00003816
    Epoch:  1190     LossContext: 0.00003807
    Epoch:  1200     LossContext: 0.00003798
    Epoch:  1210     LossContext: 0.00003788
    Epoch:  1220     LossContext: 0.00003779
    Epoch:  1230     LossContext: 0.00003770
    Epoch:  1240     LossContext: 0.00003761
    Epoch:  1250     LossContext: 0.00003753
    Epoch:  1260     LossContext: 0.00003744
    Epoch:  1270     LossContext: 0.00003735
    Epoch:  1280     LossContext: 0.00003726
    Epoch:  1290     LossContext: 0.00003717
    Epoch:  1300     LossContext: 0.00003708
    Epoch:  1310     LossContext: 0.00003700
    Epoch:  1320     LossContext: 0.00003691
    Epoch:  1330     LossContext: 0.00003682
    Epoch:  1340     LossContext: 0.00003674
    Epoch:  1350     LossContext: 0.00003665
    Epoch:  1360     LossContext: 0.00003657
    Epoch:  1370     LossContext: 0.00003649
    Epoch:  1380     LossContext: 0.00003640
    Epoch:  1390     LossContext: 0.00003632
    Epoch:  1400     LossContext: 0.00003624
    Epoch:  1410     LossContext: 0.00003615
    Epoch:  1420     LossContext: 0.00003607
    Epoch:  1430     LossContext: 0.00003599
    Epoch:  1440     LossContext: 0.00003591
    Epoch:  1450     LossContext: 0.00003583
    Epoch:  1460     LossContext: 0.00003575
    Epoch:  1470     LossContext: 0.00003567
    Epoch:  1480     LossContext: 0.00003559
    Epoch:  1490     LossContext: 0.00003551
    Epoch:  1499     LossContext: 0.00003543

Gradient descent adaptation time: 0 hours 1 mins 26 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.01670915
    Epoch:     1     LossContext: 0.00414213
    Epoch:     2     LossContext: 0.00009611
    Epoch:     3     LossContext: 0.00123243
    Epoch:    10     LossContext: 0.00088978
    Epoch:    20     LossContext: 0.00039605
    Epoch:    30     LossContext: 0.00033367
    Epoch:    40     LossContext: 0.00013976
    Epoch:    50     LossContext: 0.00002856
    Epoch:    60     LossContext: 0.00002519
    Epoch:    70     LossContext: 0.00001584
    Epoch:    80     LossContext: 0.00001557
    Epoch:    90     LossContext: 0.00001429
    Epoch:   100     LossContext: 0.00001398
    Epoch:   110     LossContext: 0.00001361
    Epoch:   120     LossContext: 0.00001349
    Epoch:   130     LossContext: 0.00001336
    Epoch:   140     LossContext: 0.00001324
    Epoch:   150     LossContext: 0.00001312
    Epoch:   160     LossContext: 0.00001300
    Epoch:   170     LossContext: 0.00001288
    Epoch:   180     LossContext: 0.00001275
    Epoch:   190     LossContext: 0.00001264
    Epoch:   200     LossContext: 0.00001253
    Epoch:   210     LossContext: 0.00001241
    Epoch:   220     LossContext: 0.00001231
    Epoch:   230     LossContext: 0.00001220
    Epoch:   240     LossContext: 0.00001210
    Epoch:   250     LossContext: 0.00001200
    Epoch:   260     LossContext: 0.00001191
    Epoch:   270     LossContext: 0.00001182
    Epoch:   280     LossContext: 0.00001173
    Epoch:   290     LossContext: 0.00001165
    Epoch:   300     LossContext: 0.00001157
    Epoch:   310     LossContext: 0.00001149
    Epoch:   320     LossContext: 0.00001141
    Epoch:   330     LossContext: 0.00001134
    Epoch:   340     LossContext: 0.00001126
    Epoch:   350     LossContext: 0.00001119
    Epoch:   360     LossContext: 0.00001113
    Epoch:   370     LossContext: 0.00001107
    Epoch:   380     LossContext: 0.00001100
    Epoch:   390     LossContext: 0.00001095
    Epoch:   400     LossContext: 0.00001089
    Epoch:   410     LossContext: 0.00001083
    Epoch:   420     LossContext: 0.00001077
    Epoch:   430     LossContext: 0.00001072
    Epoch:   440     LossContext: 0.00001067
    Epoch:   450     LossContext: 0.00001062
    Epoch:   460     LossContext: 0.00001057
    Epoch:   470     LossContext: 0.00001052
    Epoch:   480     LossContext: 0.00001047
    Epoch:   490     LossContext: 0.00001043
    Epoch:   500     LossContext: 0.00001038
    Epoch:   510     LossContext: 0.00001034
    Epoch:   520     LossContext: 0.00001029
    Epoch:   530     LossContext: 0.00001024
    Epoch:   540     LossContext: 0.00001020
    Epoch:   550     LossContext: 0.00001016
    Epoch:   560     LossContext: 0.00001012
    Epoch:   570     LossContext: 0.00001008
    Epoch:   580     LossContext: 0.00001004
    Epoch:   590     LossContext: 0.00001000
    Epoch:   600     LossContext: 0.00000996
    Epoch:   610     LossContext: 0.00000993
    Epoch:   620     LossContext: 0.00000989
    Epoch:   630     LossContext: 0.00000986
    Epoch:   640     LossContext: 0.00000983
    Epoch:   650     LossContext: 0.00000979
    Epoch:   660     LossContext: 0.00000976
    Epoch:   670     LossContext: 0.00000972
    Epoch:   680     LossContext: 0.00000969
    Epoch:   690     LossContext: 0.00000966
    Epoch:   700     LossContext: 0.00000963
    Epoch:   710     LossContext: 0.00000960
    Epoch:   720     LossContext: 0.00000957
    Epoch:   730     LossContext: 0.00000954
    Epoch:   740     LossContext: 0.00000951
    Epoch:   750     LossContext: 0.00000949
    Epoch:   760     LossContext: 0.00000946
    Epoch:   770     LossContext: 0.00000943
    Epoch:   780     LossContext: 0.00000941
    Epoch:   790     LossContext: 0.00000938
    Epoch:   800     LossContext: 0.00000936
    Epoch:   810     LossContext: 0.00000934
    Epoch:   820     LossContext: 0.00000931
    Epoch:   830     LossContext: 0.00000929
    Epoch:   840     LossContext: 0.00000927
    Epoch:   850     LossContext: 0.00000924
    Epoch:   860     LossContext: 0.00000922
    Epoch:   870     LossContext: 0.00000920
    Epoch:   880     LossContext: 0.00000918
    Epoch:   890     LossContext: 0.00000916
    Epoch:   900     LossContext: 0.00000914
    Epoch:   910     LossContext: 0.00000912
    Epoch:   920     LossContext: 0.00000910
    Epoch:   930     LossContext: 0.00000909
    Epoch:   940     LossContext: 0.00000907
    Epoch:   950     LossContext: 0.00000905
    Epoch:   960     LossContext: 0.00000903
    Epoch:   970     LossContext: 0.00000902
    Epoch:   980     LossContext: 0.00000900
    Epoch:   990     LossContext: 0.00000899
    Epoch:  1000     LossContext: 0.00000897
    Epoch:  1010     LossContext: 0.00000895
    Epoch:  1020     LossContext: 0.00000894
    Epoch:  1030     LossContext: 0.00000892
    Epoch:  1040     LossContext: 0.00000891
    Epoch:  1050     LossContext: 0.00000889
    Epoch:  1060     LossContext: 0.00000888
    Epoch:  1070     LossContext: 0.00000886
    Epoch:  1080     LossContext: 0.00000885
    Epoch:  1090     LossContext: 0.00000884
    Epoch:  1100     LossContext: 0.00000882
    Epoch:  1110     LossContext: 0.00000881
    Epoch:  1120     LossContext: 0.00000880
    Epoch:  1130     LossContext: 0.00000878
    Epoch:  1140     LossContext: 0.00000877
    Epoch:  1150     LossContext: 0.00000876
    Epoch:  1160     LossContext: 0.00000875
    Epoch:  1170     LossContext: 0.00000873
    Epoch:  1180     LossContext: 0.00000872
    Epoch:  1190     LossContext: 0.00000871
    Epoch:  1200     LossContext: 0.00000870
    Epoch:  1210     LossContext: 0.00000869
    Epoch:  1220     LossContext: 0.00000868
    Epoch:  1230     LossContext: 0.00000866
    Epoch:  1240     LossContext: 0.00000865
    Epoch:  1250     LossContext: 0.00000864
    Epoch:  1260     LossContext: 0.00000863
    Epoch:  1270     LossContext: 0.00000862
    Epoch:  1280     LossContext: 0.00000861
    Epoch:  1290     LossContext: 0.00000860
    Epoch:  1300     LossContext: 0.00000859
    Epoch:  1310     LossContext: 0.00000858
    Epoch:  1320     LossContext: 0.00000857
    Epoch:  1330     LossContext: 0.00000856
    Epoch:  1340     LossContext: 0.00000855
    Epoch:  1350     LossContext: 0.00000854
    Epoch:  1360     LossContext: 0.00000853
    Epoch:  1370     LossContext: 0.00000852
    Epoch:  1380     LossContext: 0.00000851
    Epoch:  1390     LossContext: 0.00000850
    Epoch:  1400     LossContext: 0.00000849
    Epoch:  1410     LossContext: 0.00000849
    Epoch:  1420     LossContext: 0.00000848
    Epoch:  1430     LossContext: 0.00000847
    Epoch:  1440     LossContext: 0.00000846
    Epoch:  1450     LossContext: 0.00000845
    Epoch:  1460     LossContext: 0.00000844
    Epoch:  1470     LossContext: 0.00000843
    Epoch:  1480     LossContext: 0.00000842
    Epoch:  1490     LossContext: 0.00000841
    Epoch:  1499     LossContext: 0.00000841

Gradient descent adaptation time: 0 hours 1 mins 26 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.00459428
    Epoch:     1     LossContext: 0.00174021
    Epoch:     2     LossContext: 0.00167169
    Epoch:     3     LossContext: 0.00054745
    Epoch:    10     LossContext: 0.00066537
    Epoch:    20     LossContext: 0.00005656
    Epoch:    30     LossContext: 0.00007149
    Epoch:    40     LossContext: 0.00005969
    Epoch:    50     LossContext: 0.00003174
    Epoch:    60     LossContext: 0.00002099
    Epoch:    70     LossContext: 0.00002139
    Epoch:    80     LossContext: 0.00002050
    Epoch:    90     LossContext: 0.00002047
    Epoch:   100     LossContext: 0.00002029
    Epoch:   110     LossContext: 0.00002015
    Epoch:   120     LossContext: 0.00002004
    Epoch:   130     LossContext: 0.00001995
    Epoch:   140     LossContext: 0.00001987
    Epoch:   150     LossContext: 0.00001979
    Epoch:   160     LossContext: 0.00001972
    Epoch:   170     LossContext: 0.00001965
    Epoch:   180     LossContext: 0.00001959
    Epoch:   190     LossContext: 0.00001953
    Epoch:   200     LossContext: 0.00001947
    Epoch:   210     LossContext: 0.00001941
    Epoch:   220     LossContext: 0.00001936
    Epoch:   230     LossContext: 0.00001931
    Epoch:   240     LossContext: 0.00001927
    Epoch:   250     LossContext: 0.00001922
    Epoch:   260     LossContext: 0.00001917
    Epoch:   270     LossContext: 0.00001913
    Epoch:   280     LossContext: 0.00001909
    Epoch:   290     LossContext: 0.00001905
    Epoch:   300     LossContext: 0.00001901
    Epoch:   310     LossContext: 0.00001897
    Epoch:   320     LossContext: 0.00001893
    Epoch:   330     LossContext: 0.00001889
    Epoch:   340     LossContext: 0.00001886
    Epoch:   350     LossContext: 0.00001882
    Epoch:   360     LossContext: 0.00001879
    Epoch:   370     LossContext: 0.00001875
    Epoch:   380     LossContext: 0.00001872
    Epoch:   390     LossContext: 0.00001869
    Epoch:   400     LossContext: 0.00001866
    Epoch:   410     LossContext: 0.00001862
    Epoch:   420     LossContext: 0.00001860
    Epoch:   430     LossContext: 0.00001857
    Epoch:   440     LossContext: 0.00001854
    Epoch:   450     LossContext: 0.00001851
    Epoch:   460     LossContext: 0.00001849
    Epoch:   470     LossContext: 0.00001846
    Epoch:   480     LossContext: 0.00001844
    Epoch:   490     LossContext: 0.00001841
    Epoch:   500     LossContext: 0.00001839
    Epoch:   510     LossContext: 0.00001837
    Epoch:   520     LossContext: 0.00001834
    Epoch:   530     LossContext: 0.00001832
    Epoch:   540     LossContext: 0.00001830
    Epoch:   550     LossContext: 0.00001828
    Epoch:   560     LossContext: 0.00001825
    Epoch:   570     LossContext: 0.00001823
    Epoch:   580     LossContext: 0.00001821
    Epoch:   590     LossContext: 0.00001819
    Epoch:   600     LossContext: 0.00001817
    Epoch:   610     LossContext: 0.00001815
    Epoch:   620     LossContext: 0.00001813
    Epoch:   630     LossContext: 0.00001811
    Epoch:   640     LossContext: 0.00001809
    Epoch:   650     LossContext: 0.00001807
    Epoch:   660     LossContext: 0.00001805
    Epoch:   670     LossContext: 0.00001804
    Epoch:   680     LossContext: 0.00001802
    Epoch:   690     LossContext: 0.00001800
    Epoch:   700     LossContext: 0.00001799
    Epoch:   710     LossContext: 0.00001797
    Epoch:   720     LossContext: 0.00001795
    Epoch:   730     LossContext: 0.00001794
    Epoch:   740     LossContext: 0.00001792
    Epoch:   750     LossContext: 0.00001791
    Epoch:   760     LossContext: 0.00001789
    Epoch:   770     LossContext: 0.00001787
    Epoch:   780     LossContext: 0.00001786
    Epoch:   790     LossContext: 0.00001785
    Epoch:   800     LossContext: 0.00001783
    Epoch:   810     LossContext: 0.00001782
    Epoch:   820     LossContext: 0.00001780
    Epoch:   830     LossContext: 0.00001779
    Epoch:   840     LossContext: 0.00001778
    Epoch:   850     LossContext: 0.00001776
    Epoch:   860     LossContext: 0.00001775
    Epoch:   870     LossContext: 0.00001774
    Epoch:   880     LossContext: 0.00001773
    Epoch:   890     LossContext: 0.00001771
    Epoch:   900     LossContext: 0.00001770
    Epoch:   910     LossContext: 0.00001769
    Epoch:   920     LossContext: 0.00001768
    Epoch:   930     LossContext: 0.00001767
    Epoch:   940     LossContext: 0.00001765
    Epoch:   950     LossContext: 0.00001764
    Epoch:   960     LossContext: 0.00001763
    Epoch:   970     LossContext: 0.00001762
    Epoch:   980     LossContext: 0.00001761
    Epoch:   990     LossContext: 0.00001760
    Epoch:  1000     LossContext: 0.00001759
    Epoch:  1010     LossContext: 0.00001758
    Epoch:  1020     LossContext: 0.00001757
    Epoch:  1030     LossContext: 0.00001756
    Epoch:  1040     LossContext: 0.00001755
    Epoch:  1050     LossContext: 0.00001754
    Epoch:  1060     LossContext: 0.00001753
    Epoch:  1070     LossContext: 0.00001752
    Epoch:  1080     LossContext: 0.00001751
    Epoch:  1090     LossContext: 0.00001750
    Epoch:  1100     LossContext: 0.00001749
    Epoch:  1110     LossContext: 0.00001748
    Epoch:  1120     LossContext: 0.00001748
    Epoch:  1130     LossContext: 0.00001747
    Epoch:  1140     LossContext: 0.00001746
    Epoch:  1150     LossContext: 0.00001745
    Epoch:  1160     LossContext: 0.00001744
    Epoch:  1170     LossContext: 0.00001743
    Epoch:  1180     LossContext: 0.00001742
    Epoch:  1190     LossContext: 0.00001742
    Epoch:  1200     LossContext: 0.00001741
    Epoch:  1210     LossContext: 0.00001740
    Epoch:  1220     LossContext: 0.00001739
    Epoch:  1230     LossContext: 0.00001739
    Epoch:  1240     LossContext: 0.00001738
    Epoch:  1250     LossContext: 0.00001737
    Epoch:  1260     LossContext: 0.00001736
    Epoch:  1270     LossContext: 0.00001736
    Epoch:  1280     LossContext: 0.00001735
    Epoch:  1290     LossContext: 0.00001734
    Epoch:  1300     LossContext: 0.00001733
    Epoch:  1310     LossContext: 0.00001733
    Epoch:  1320     LossContext: 0.00001732
    Epoch:  1330     LossContext: 0.00001731
    Epoch:  1340     LossContext: 0.00001731
    Epoch:  1350     LossContext: 0.00001730
    Epoch:  1360     LossContext: 0.00001730
    Epoch:  1370     LossContext: 0.00001728
    Epoch:  1380     LossContext: 0.00001728
    Epoch:  1390     LossContext: 0.00001728
    Epoch:  1400     LossContext: 0.00001737
    Epoch:  1410     LossContext: 0.00001726
    Epoch:  1420     LossContext: 0.00001731
    Epoch:  1430     LossContext: 0.00001798
    Epoch:  1440     LossContext: 0.00001734
    Epoch:  1450     LossContext: 0.00001725
    Epoch:  1460     LossContext: 0.00001925
    Epoch:  1470     LossContext: 0.00003487
    Epoch:  1480     LossContext: 0.00002350
    Epoch:  1490     LossContext: 0.00001968
    Epoch:  1499     LossContext: 0.00001742

Gradient descent adaptation time: 0 hours 1 mins 26 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 4.389332e-05

