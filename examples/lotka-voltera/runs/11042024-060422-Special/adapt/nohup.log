cp: 'main_proximal_coercive.py' and './main_proximal_coercive.py' are the same file
cp: 'dataset.py' and './dataset.py' are the same file
cp: cannot stat '../../nodax': No such file or directory

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: train
 Savepath: ./
 Seed: 2026


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: test
 Savepath: ./
 Seed: 4052


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 172231
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 172231
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 308240 


WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed


=== Beginning training with proximal alternating minimization ... ===
    Number of examples in a batch: 4
    Maximum number of steps per inner minimization: 25
    Maximum number of outer minimizations: 2000
    Maximum total number of training steps: 50000

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Outer Step:     0      LossTrajs: 1.81051588     ContextsNorm: 0.00000000     ValIndCrit: 1.69905555
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.71e-06
        -DiffCxt:  inf
    Outer Step:     1      LossTrajs: 1.52824664     ContextsNorm: 0.00132960     ValIndCrit: 1.43287253
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.67e-06
        -DiffCxt:  5.61e-04
    Outer Step:     2      LossTrajs: 1.22562921     ContextsNorm: 0.00391919     ValIndCrit: 1.14542675
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.11e-05
        -DiffCxt:  7.07e-04
    Outer Step:     3      LossTrajs: 0.81252706     ContextsNorm: 0.00774632     ValIndCrit: 0.74780351
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.19e-05
        -DiffCxt:  3.82e-04
    Outer Step:    10      LossTrajs: 0.23048156     ContextsNorm: 0.00969911     ValIndCrit: 0.27365136
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.35e-07
        -DiffCxt:  6.36e-06
    Outer Step:    20      LossTrajs: 0.16440676     ContextsNorm: 0.01046115     ValIndCrit: 0.18809357
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.43e-07
        -DiffCxt:  3.04e-07
    Outer Step:    30      LossTrajs: 0.07196356     ContextsNorm: 0.01074603     ValIndCrit: 0.07421020
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.86e-07
        -DiffCxt:  2.92e-06
    Outer Step:    40      LossTrajs: 0.02183240     ContextsNorm: 0.01407583     ValIndCrit: 0.02349509
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.34e-07
        -DiffCxt:  1.90e-06
    Outer Step:    50      LossTrajs: 0.01032743     ContextsNorm: 0.01486388     ValIndCrit: 0.01272014
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.74e-08
        -DiffCxt:  3.14e-07
    Outer Step:    60      LossTrajs: 0.00713300     ContextsNorm: 0.01490357     ValIndCrit: 0.00951189
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.86e-08
        -DiffCxt:  1.44e-07
    Outer Step:    70      LossTrajs: 0.00540683     ContextsNorm: 0.01498082     ValIndCrit: 0.00770469
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.98e-07
        -DiffCxt:  9.21e-08
    Outer Step:    80      LossTrajs: 0.00428836     ContextsNorm: 0.01522372     ValIndCrit: 0.00662565
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.81e-08
        -DiffCxt:  1.99e-07
    Outer Step:    90      LossTrajs: 0.00348260     ContextsNorm: 0.01523701     ValIndCrit: 0.00561304
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.12e-08
        -DiffCxt:  2.36e-07
    Outer Step:   100      LossTrajs: 0.00253174     ContextsNorm: 0.01529666     ValIndCrit: 0.00439688
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.13e-08
        -DiffCxt:  1.26e-07
    Outer Step:   110      LossTrajs: 0.00182972     ContextsNorm: 0.01524959     ValIndCrit: 0.00340092
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.95e-08
        -DiffCxt:  1.60e-07
    Outer Step:   120      LossTrajs: 0.00136411     ContextsNorm: 0.01509845     ValIndCrit: 0.00266124
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.73e-08
        -DiffCxt:  5.39e-08
    Outer Step:   130      LossTrajs: 0.00100299     ContextsNorm: 0.01522627     ValIndCrit: 0.00210530
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.75e-08
        -DiffCxt:  1.89e-07
    Outer Step:   140      LossTrajs: 0.00078518     ContextsNorm: 0.01528008     ValIndCrit: 0.00163184
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.83e-08
        -DiffCxt:  5.66e-08
    Outer Step:   150      LossTrajs: 0.00059248     ContextsNorm: 0.01520839     ValIndCrit: 0.00127787
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.40e-08
        -DiffCxt:  4.16e-08
    Outer Step:   160      LossTrajs: 0.00042749     ContextsNorm: 0.01511440     ValIndCrit: 0.00100450
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.48e-08
        -DiffCxt:  2.75e-08
    Outer Step:   170      LossTrajs: 0.00034203     ContextsNorm: 0.01506748     ValIndCrit: 0.00078484
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.20e-08
        -DiffCxt:  1.60e-08
    Outer Step:   180      LossTrajs: 0.00029250     ContextsNorm: 0.01509090     ValIndCrit: 0.00061867
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.72e-08
        -DiffCxt:  7.02e-08
    Outer Step:   190      LossTrajs: 0.00021680     ContextsNorm: 0.01508822     ValIndCrit: 0.00048611
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.09e-08
        -DiffCxt:  1.44e-08
    Outer Step:   200      LossTrajs: 0.00018242     ContextsNorm: 0.01513583     ValIndCrit: 0.00039368
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.89e-08
        -DiffCxt:  2.24e-08
    Outer Step:   210      LossTrajs: 0.00014641     ContextsNorm: 0.01511612     ValIndCrit: 0.00032472
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.09e-08
        -DiffCxt:  5.09e-08
    Outer Step:   220      LossTrajs: 0.00012470     ContextsNorm: 0.01515972     ValIndCrit: 0.00028358
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.90e-08
        -DiffCxt:  3.94e-08
    Outer Step:   230      LossTrajs: 0.00010586     ContextsNorm: 0.01519040     ValIndCrit: 0.00024276
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.69e-08
        -DiffCxt:  1.52e-08
    Outer Step:   240      LossTrajs: 0.00008903     ContextsNorm: 0.01517921     ValIndCrit: 0.00022050
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   23
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.10e-08
        -DiffCxt:  9.54e-09
    Outer Step:   250      LossTrajs: 0.00008574     ContextsNorm: 0.01514672     ValIndCrit: 0.00018737
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.45e-08
        -DiffCxt:  1.18e-07
    Outer Step:   260      LossTrajs: 0.00007060     ContextsNorm: 0.01511618     ValIndCrit: 0.00017485
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.34e-08
        -DiffCxt:  3.30e-08
    Outer Step:   270      LossTrajs: 0.00006792     ContextsNorm: 0.01504640     ValIndCrit: 0.00015739
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.54e-08
        -DiffCxt:  1.86e-08
    Outer Step:   280      LossTrajs: 0.00005998     ContextsNorm: 0.01510632     ValIndCrit: 0.00014409
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.41e-08
        -DiffCxt:  1.83e-08
    Outer Step:   290      LossTrajs: 0.00005878     ContextsNorm: 0.01503446     ValIndCrit: 0.00013471
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.37e-08
        -DiffCxt:  4.16e-08
    Outer Step:   300      LossTrajs: 0.00005069     ContextsNorm: 0.01502902     ValIndCrit: 0.00012658
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.64e-08
        -DiffCxt:  3.29e-08
    Outer Step:   310      LossTrajs: 0.00004726     ContextsNorm: 0.01502410     ValIndCrit: 0.00011750
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.34e-08
        -DiffCxt:  3.81e-08
    Outer Step:   320      LossTrajs: 0.00004536     ContextsNorm: 0.01496229     ValIndCrit: 0.00011270
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.50e-08
        -DiffCxt:  7.25e-08
    Outer Step:   330      LossTrajs: 0.00004514     ContextsNorm: 0.01491708     ValIndCrit: 0.00010380
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.93e-08
        -DiffCxt:  9.30e-08
    Outer Step:   340      LossTrajs: 0.00004404     ContextsNorm: 0.01486446     ValIndCrit: 0.00009902
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.95e-08
        -DiffCxt:  2.58e-08
    Outer Step:   350      LossTrajs: 0.00003919     ContextsNorm: 0.01479539     ValIndCrit: 0.00009430
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.18e-08
        -DiffCxt:  8.98e-08
    Outer Step:   360      LossTrajs: 0.00003662     ContextsNorm: 0.01476830     ValIndCrit: 0.00009019
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.02e-09
        -DiffCxt:  3.58e-08
    Outer Step:   370      LossTrajs: 0.00003500     ContextsNorm: 0.01473682     ValIndCrit: 0.00008464
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.87e-08
        -DiffCxt:  1.32e-07
    Outer Step:   380      LossTrajs: 0.00003301     ContextsNorm: 0.01478979     ValIndCrit: 0.00008232
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.49e-08
        -DiffCxt:  7.78e-08
    Outer Step:   390      LossTrajs: 0.00003451     ContextsNorm: 0.01478720     ValIndCrit: 0.00007737
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.54e-08
        -DiffCxt:  1.02e-07
    Outer Step:   400      LossTrajs: 0.00003334     ContextsNorm: 0.01472580     ValIndCrit: 0.00007632
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.88e-08
        -DiffCxt:  3.39e-08
    Outer Step:   410      LossTrajs: 0.00003672     ContextsNorm: 0.01480957     ValIndCrit: 0.00007130
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.19e-07
        -DiffCxt:  1.08e-07
    Outer Step:   420      LossTrajs: 0.00002981     ContextsNorm: 0.01467599     ValIndCrit: 0.00006926
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.29e-08
        -DiffCxt:  9.79e-08
    Outer Step:   430      LossTrajs: 0.00005127     ContextsNorm: 0.01461674     ValIndCrit: 0.00007293
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.39e-07
        -DiffCxt:  4.16e-07
    Outer Step:   440      LossTrajs: 0.00002820     ContextsNorm: 0.01456251     ValIndCrit: 0.00006484
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.12e-09
        -DiffCxt:  4.72e-08
    Outer Step:   450      LossTrajs: 0.00002770     ContextsNorm: 0.01453192     ValIndCrit: 0.00006355
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.29e-08
        -DiffCxt:  5.33e-08
    Outer Step:   460      LossTrajs: 0.00002912     ContextsNorm: 0.01450894     ValIndCrit: 0.00006191
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.26e-08
        -DiffCxt:  1.69e-07
    Outer Step:   470      LossTrajs: 0.00002636     ContextsNorm: 0.01448584     ValIndCrit: 0.00006281
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.96e-08
        -DiffCxt:  5.22e-07
    Outer Step:   480      LossTrajs: 0.00003019     ContextsNorm: 0.01445082     ValIndCrit: 0.00006102
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.27e-07
        -DiffCxt:  6.61e-07
    Outer Step:   490      LossTrajs: 0.00002951     ContextsNorm: 0.01444527     ValIndCrit: 0.00005652
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.93e-08
        -DiffCxt:  4.27e-07
    Outer Step:   500      LossTrajs: 0.00002578     ContextsNorm: 0.01439646     ValIndCrit: 0.00005370
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.83e-08
        -DiffCxt:  5.60e-07
    Outer Step:   510      LossTrajs: 0.00002423     ContextsNorm: 0.01436796     ValIndCrit: 0.00005298
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.93e-08
        -DiffCxt:  1.54e-07
    Outer Step:   520      LossTrajs: 0.00002494     ContextsNorm: 0.01433623     ValIndCrit: 0.00005188
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.57e-08
        -DiffCxt:  5.83e-08
    Outer Step:   530      LossTrajs: 0.00002378     ContextsNorm: 0.01433861     ValIndCrit: 0.00004975
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.40e-08
        -DiffCxt:  1.02e-07
    Outer Step:   540      LossTrajs: 0.00002378     ContextsNorm: 0.01426891     ValIndCrit: 0.00004954
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.76e-09
        -DiffCxt:  2.74e-08
    Outer Step:   550      LossTrajs: 0.00002419     ContextsNorm: 0.01423808     ValIndCrit: 0.00004899
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.70e-09
        -DiffCxt:  5.01e-08
    Outer Step:   560      LossTrajs: 0.00002426     ContextsNorm: 0.01426319     ValIndCrit: 0.00004697
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.11e-08
        -DiffCxt:  2.89e-08
    Outer Step:   570      LossTrajs: 0.00002222     ContextsNorm: 0.01424093     ValIndCrit: 0.00004780
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.32e-08
        -DiffCxt:  6.25e-08
    Outer Step:   580      LossTrajs: 0.00002262     ContextsNorm: 0.01416233     ValIndCrit: 0.00004768
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.05e-08
        -DiffCxt:  5.52e-08
    Outer Step:   590      LossTrajs: 0.00002313     ContextsNorm: 0.01411329     ValIndCrit: 0.00004710
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.14e-08
        -DiffCxt:  1.61e-07
    Outer Step:   600      LossTrajs: 0.00002216     ContextsNorm: 0.01407805     ValIndCrit: 0.00004404
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.87e-08
        -DiffCxt:  1.48e-07
    Outer Step:   610      LossTrajs: 0.00002326     ContextsNorm: 0.01417146     ValIndCrit: 0.00004487
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.26e-08
        -DiffCxt:  7.00e-08
    Outer Step:   620      LossTrajs: 0.00002195     ContextsNorm: 0.01413363     ValIndCrit: 0.00004374
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.40e-08
        -DiffCxt:  1.60e-07
    Outer Step:   630      LossTrajs: 0.00002152     ContextsNorm: 0.01409182     ValIndCrit: 0.00004293
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.82e-08
        -DiffCxt:  5.95e-08
    Outer Step:   640      LossTrajs: 0.00002238     ContextsNorm: 0.01416808     ValIndCrit: 0.00004338
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.16e-08
        -DiffCxt:  5.13e-07
    Outer Step:   650      LossTrajs: 0.00002079     ContextsNorm: 0.01415410     ValIndCrit: 0.00004110
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.39e-08
        -DiffCxt:  5.32e-08
    Outer Step:   660      LossTrajs: 0.00002187     ContextsNorm: 0.01412809     ValIndCrit: 0.00004103
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.44e-09
        -DiffCxt:  2.70e-08
    Outer Step:   670      LossTrajs: 0.00002243     ContextsNorm: 0.01412405     ValIndCrit: 0.00004062
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.83e-08
        -DiffCxt:  9.63e-08
    Outer Step:   680      LossTrajs: 0.00002810     ContextsNorm: 0.01404825     ValIndCrit: 0.00004088
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.03e-07
        -DiffCxt:  3.70e-07
    Outer Step:   690      LossTrajs: 0.00002117     ContextsNorm: 0.01401726     ValIndCrit: 0.00004060
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.72e-08
        -DiffCxt:  4.19e-07
    Outer Step:   700      LossTrajs: 0.00002137     ContextsNorm: 0.01401232     ValIndCrit: 0.00003966
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.11e-08
        -DiffCxt:  8.77e-08
    Outer Step:   710      LossTrajs: 0.00002057     ContextsNorm: 0.01401129     ValIndCrit: 0.00003931
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.39e-08
        -DiffCxt:  5.41e-08
    Outer Step:   720      LossTrajs: 0.00002068     ContextsNorm: 0.01393330     ValIndCrit: 0.00003933
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.45e-09
        -DiffCxt:  8.11e-08
    Outer Step:   730      LossTrajs: 0.00002290     ContextsNorm: 0.01389402     ValIndCrit: 0.00003852
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.26e-08
        -DiffCxt:  1.68e-07
    Outer Step:   740      LossTrajs: 0.00002114     ContextsNorm: 0.01384571     ValIndCrit: 0.00003733
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.26e-08
        -DiffCxt:  1.34e-07
    Outer Step:   750      LossTrajs: 0.00002062     ContextsNorm: 0.01377951     ValIndCrit: 0.00003697
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.84e-08
        -DiffCxt:  4.94e-08
    Outer Step:   760      LossTrajs: 0.00002159     ContextsNorm: 0.01381353     ValIndCrit: 0.00003662
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.52e-08
        -DiffCxt:  2.57e-07
    Outer Step:   770      LossTrajs: 0.00002009     ContextsNorm: 0.01374655     ValIndCrit: 0.00003562
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.59e-08
        -DiffCxt:  1.22e-07
    Outer Step:   780      LossTrajs: 0.00002073     ContextsNorm: 0.01373384     ValIndCrit: 0.00003615
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.80e-08
        -DiffCxt:  2.36e-07
    Outer Step:   790      LossTrajs: 0.00002089     ContextsNorm: 0.01363118     ValIndCrit: 0.00003457
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.74e-08
        -DiffCxt:  1.75e-07
    Outer Step:   800      LossTrajs: 0.00002067     ContextsNorm: 0.01362472     ValIndCrit: 0.00003720
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.97e-08
        -DiffCxt:  1.13e-07
    Outer Step:   810      LossTrajs: 0.00001884     ContextsNorm: 0.01364749     ValIndCrit: 0.00003503
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.82e-09
        -DiffCxt:  5.36e-08
    Outer Step:   820      LossTrajs: 0.00001988     ContextsNorm: 0.01367983     ValIndCrit: 0.00003606
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.58e-08
        -DiffCxt:  1.29e-07
    Outer Step:   830      LossTrajs: 0.00001891     ContextsNorm: 0.01364884     ValIndCrit: 0.00003535
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.62e-09
        -DiffCxt:  9.90e-08
    Outer Step:   840      LossTrajs: 0.00001969     ContextsNorm: 0.01358802     ValIndCrit: 0.00003547
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.26e-08
        -DiffCxt:  1.40e-07
    Outer Step:   850      LossTrajs: 0.00002099     ContextsNorm: 0.01354830     ValIndCrit: 0.00003412
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.48e-08
        -DiffCxt:  2.47e-07
    Outer Step:   860      LossTrajs: 0.00001983     ContextsNorm: 0.01349354     ValIndCrit: 0.00003488
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.88e-08
        -DiffCxt:  1.05e-07
    Outer Step:   870      LossTrajs: 0.00002365     ContextsNorm: 0.01343486     ValIndCrit: 0.00003540
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.30e-07
        -DiffCxt:  8.11e-07
    Outer Step:   880      LossTrajs: 0.00001866     ContextsNorm: 0.01343047     ValIndCrit: 0.00003331
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.16e-08
        -DiffCxt:  8.01e-08
    Outer Step:   890      LossTrajs: 0.00001896     ContextsNorm: 0.01340491     ValIndCrit: 0.00003241
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.25e-08
        -DiffCxt:  1.98e-07
    Outer Step:   900      LossTrajs: 0.00001891     ContextsNorm: 0.01332624     ValIndCrit: 0.00003240
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.09e-08
        -DiffCxt:  6.99e-08
    Outer Step:   910      LossTrajs: 0.00001808     ContextsNorm: 0.01330279     ValIndCrit: 0.00003238
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.04e-08
        -DiffCxt:  3.88e-08
    Outer Step:   920      LossTrajs: 0.00002146     ContextsNorm: 0.01326429     ValIndCrit: 0.00003078
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.25e-07
        -DiffCxt:  3.00e-07
    Outer Step:   930      LossTrajs: 0.00002593     ContextsNorm: 0.01318377     ValIndCrit: 0.00003405
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.13e-07
        -DiffCxt:  8.89e-07
    Outer Step:   940      LossTrajs: 0.00002232     ContextsNorm: 0.01320405     ValIndCrit: 0.00003320
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.64e-08
        -DiffCxt:  1.55e-06
    Outer Step:   950      LossTrajs: 0.00001825     ContextsNorm: 0.01317185     ValIndCrit: 0.00003217
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.80e-09
        -DiffCxt:  4.82e-08
    Outer Step:   960      LossTrajs: 0.00002041     ContextsNorm: 0.01316295     ValIndCrit: 0.00003394
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.84e-08
        -DiffCxt:  3.33e-07
    Outer Step:   970      LossTrajs: 0.00002337     ContextsNorm: 0.01313976     ValIndCrit: 0.00003083
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.96e-08
        -DiffCxt:  4.34e-07
    Outer Step:   980      LossTrajs: 0.00001816     ContextsNorm: 0.01314733     ValIndCrit: 0.00003104
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.54e-09
        -DiffCxt:  5.43e-08
    Outer Step:   990      LossTrajs: 0.00001805     ContextsNorm: 0.01312850     ValIndCrit: 0.00003149
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.56e-08
        -DiffCxt:  1.25e-07
    Outer Step:  1000      LossTrajs: 0.00001837     ContextsNorm: 0.01309286     ValIndCrit: 0.00002993
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.31e-08
        -DiffCxt:  1.57e-07
    Outer Step:  1010      LossTrajs: 0.00001894     ContextsNorm: 0.01303399     ValIndCrit: 0.00003401
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.67e-08
        -DiffCxt:  5.77e-07
    Outer Step:  1020      LossTrajs: 0.00001737     ContextsNorm: 0.01302976     ValIndCrit: 0.00003081
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.51e-09
        -DiffCxt:  4.17e-08
    Outer Step:  1030      LossTrajs: 0.00001747     ContextsNorm: 0.01298786     ValIndCrit: 0.00003022
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.65e-09
        -DiffCxt:  3.45e-08
    Outer Step:  1040      LossTrajs: 0.00002127     ContextsNorm: 0.01300737     ValIndCrit: 0.00003222
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.62e-08
        -DiffCxt:  1.63e-07
    Outer Step:  1050      LossTrajs: 0.00001726     ContextsNorm: 0.01298943     ValIndCrit: 0.00003025
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.40e-09
        -DiffCxt:  3.58e-08
    Outer Step:  1060      LossTrajs: 0.00001890     ContextsNorm: 0.01301926     ValIndCrit: 0.00003229
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.18e-07
        -DiffCxt:  7.26e-07
    Outer Step:  1070      LossTrajs: 0.00002089     ContextsNorm: 0.01299501     ValIndCrit: 0.00003008
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.83e-08
        -DiffCxt:  7.81e-08
    Outer Step:  1080      LossTrajs: 0.00001720     ContextsNorm: 0.01303375     ValIndCrit: 0.00002882
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.60e-09
        -DiffCxt:  7.39e-08
    Outer Step:  1090      LossTrajs: 0.00001718     ContextsNorm: 0.01304645     ValIndCrit: 0.00002900
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.13e-09
        -DiffCxt:  2.81e-08
    Outer Step:  1100      LossTrajs: 0.00001737     ContextsNorm: 0.01303403     ValIndCrit: 0.00002959
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.75e-08
        -DiffCxt:  8.23e-08
    Outer Step:  1110      LossTrajs: 0.00001803     ContextsNorm: 0.01304210     ValIndCrit: 0.00002785
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.49e-08
        -DiffCxt:  8.44e-08
    Outer Step:  1120      LossTrajs: 0.00001755     ContextsNorm: 0.01298607     ValIndCrit: 0.00002823
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.03e-08
        -DiffCxt:  7.19e-08
    Outer Step:  1130      LossTrajs: 0.00002009     ContextsNorm: 0.01300913     ValIndCrit: 0.00003030
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.95e-07
        -DiffCxt:  2.71e-07
    Outer Step:  1140      LossTrajs: 0.00002199     ContextsNorm: 0.01308400     ValIndCrit: 0.00003346
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.11e-07
        -DiffCxt:  7.66e-07
    Outer Step:  1150      LossTrajs: 0.00001719     ContextsNorm: 0.01307216     ValIndCrit: 0.00002746
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.81e-09
        -DiffCxt:  3.45e-08
    Outer Step:  1160      LossTrajs: 0.00001690     ContextsNorm: 0.01300153     ValIndCrit: 0.00002852
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.68e-09
        -DiffCxt:  6.03e-08
    Outer Step:  1170      LossTrajs: 0.00001822     ContextsNorm: 0.01301539     ValIndCrit: 0.00002798
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.59e-08
        -DiffCxt:  1.15e-07
    Outer Step:  1180      LossTrajs: 0.00002037     ContextsNorm: 0.01295939     ValIndCrit: 0.00002839
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.23e-08
        -DiffCxt:  2.97e-07
    Outer Step:  1190      LossTrajs: 0.00001673     ContextsNorm: 0.01299143     ValIndCrit: 0.00002703
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.54e-09
        -DiffCxt:  5.05e-08
    Outer Step:  1200      LossTrajs: 0.00001917     ContextsNorm: 0.01295210     ValIndCrit: 0.00002764
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.28e-08
        -DiffCxt:  5.73e-08
    Outer Step:  1210      LossTrajs: 0.00002315     ContextsNorm: 0.01288640     ValIndCrit: 0.00002925
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.44e-07
        -DiffCxt:  7.28e-07
    Outer Step:  1220      LossTrajs: 0.00001816     ContextsNorm: 0.01288962     ValIndCrit: 0.00002689
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.85e-08
        -DiffCxt:  1.83e-07
    Outer Step:  1230      LossTrajs: 0.00001835     ContextsNorm: 0.01287463     ValIndCrit: 0.00002692
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.16e-08
        -DiffCxt:  2.40e-07
    Outer Step:  1240      LossTrajs: 0.00005458     ContextsNorm: 0.01287892     ValIndCrit: 0.00004282
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.25e-07
        -DiffCxt:  5.09e-06
    Outer Step:  1250      LossTrajs: 0.00001666     ContextsNorm: 0.01289015     ValIndCrit: 0.00002666
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.83e-09
        -DiffCxt:  2.40e-08
    Outer Step:  1260      LossTrajs: 0.00001634     ContextsNorm: 0.01286216     ValIndCrit: 0.00002627
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.81e-10
        -DiffCxt:  3.37e-08
    Outer Step:  1270      LossTrajs: 0.00001664     ContextsNorm: 0.01285473     ValIndCrit: 0.00002689
        -NbInnerStepsNode:   18
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.37e-10
        -DiffCxt:  4.06e-08
    Outer Step:  1280      LossTrajs: 0.00001684     ContextsNorm: 0.01281215     ValIndCrit: 0.00002684
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.33e-09
        -DiffCxt:  7.77e-08
    Outer Step:  1290      LossTrajs: 0.00001680     ContextsNorm: 0.01278346     ValIndCrit: 0.00002718
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.70e-09
        -DiffCxt:  9.52e-08
    Outer Step:  1300      LossTrajs: 0.00001658     ContextsNorm: 0.01274810     ValIndCrit: 0.00002648
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.29e-09
        -DiffCxt:  7.89e-08
    Outer Step:  1310      LossTrajs: 0.00001662     ContextsNorm: 0.01271862     ValIndCrit: 0.00002626
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.81e-09
        -DiffCxt:  3.55e-08
    Outer Step:  1320      LossTrajs: 0.00001626     ContextsNorm: 0.01269037     ValIndCrit: 0.00002742
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.07e-09
        -DiffCxt:  4.63e-08
    Outer Step:  1330      LossTrajs: 0.00001777     ContextsNorm: 0.01267990     ValIndCrit: 0.00002843
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.62e-08
        -DiffCxt:  4.04e-07
    Outer Step:  1340      LossTrajs: 0.00001693     ContextsNorm: 0.01265031     ValIndCrit: 0.00002649
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.96e-08
        -DiffCxt:  6.99e-08
    Outer Step:  1350      LossTrajs: 0.00001602     ContextsNorm: 0.01253088     ValIndCrit: 0.00002626
        Saving best model so far ...
        -NbInnerStepsNode:    5
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.49e-10
        -DiffCxt:  2.75e-08
    Outer Step:  1360      LossTrajs: 0.00001581     ContextsNorm: 0.01251739     ValIndCrit: 0.00002582
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   21
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.35e-10
        -DiffCxt:  9.49e-09
    Outer Step:  1370      LossTrajs: 0.00001604     ContextsNorm: 0.01249577     ValIndCrit: 0.00002642
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.99e-10
        -DiffCxt:  2.61e-08
    Outer Step:  1380      LossTrajs: 0.00001617     ContextsNorm: 0.01247681     ValIndCrit: 0.00002601
        -NbInnerStepsNode:    5
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.93e-10
        -DiffCxt:  5.38e-08
    Outer Step:  1390      LossTrajs: 0.00001599     ContextsNorm: 0.01249107     ValIndCrit: 0.00002600
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.07e-10
        -DiffCxt:  3.58e-08
    Outer Step:  1400      LossTrajs: 0.00001794     ContextsNorm: 0.01242837     ValIndCrit: 0.00002485
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.81e-08
        -DiffCxt:  3.65e-07
    Outer Step:  1410      LossTrajs: 0.00001612     ContextsNorm: 0.01237643     ValIndCrit: 0.00002593
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.04e-09
        -DiffCxt:  5.83e-08
    Outer Step:  1420      LossTrajs: 0.00001603     ContextsNorm: 0.01236261     ValIndCrit: 0.00002479
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.32e-08
        -DiffCxt:  3.16e-07
    Outer Step:  1430      LossTrajs: 0.00002404     ContextsNorm: 0.01241325     ValIndCrit: 0.00002648
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.62e-07
        -DiffCxt:  1.41e-06
    Outer Step:  1440      LossTrajs: 0.00001773     ContextsNorm: 0.01239811     ValIndCrit: 0.00002556
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.17e-08
        -DiffCxt:  8.53e-08
    Outer Step:  1450      LossTrajs: 0.00001575     ContextsNorm: 0.01238571     ValIndCrit: 0.00002467
        Saving best model so far ...
        -NbInnerStepsNode:    5
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.19e-10
        -DiffCxt:  2.94e-08
    Outer Step:  1460      LossTrajs: 0.00001558     ContextsNorm: 0.01237660     ValIndCrit: 0.00002474
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.92e-10
        -DiffCxt:  8.45e-09
    Outer Step:  1470      LossTrajs: 0.00001577     ContextsNorm: 0.01235482     ValIndCrit: 0.00002505
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.52e-10
        -DiffCxt:  4.12e-08
    Outer Step:  1480      LossTrajs: 0.00001572     ContextsNorm: 0.01233880     ValIndCrit: 0.00002489
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.24e-10
        -DiffCxt:  6.21e-08
    Outer Step:  1490      LossTrajs: 0.00001614     ContextsNorm: 0.01227991     ValIndCrit: 0.00002613
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.47e-09
        -DiffCxt:  4.97e-08
    Outer Step:  1500      LossTrajs: 0.00001828     ContextsNorm: 0.01220799     ValIndCrit: 0.00002463
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.05e-08
        -DiffCxt:  2.66e-07
    Outer Step:  1510      LossTrajs: 0.00001662     ContextsNorm: 0.01219507     ValIndCrit: 0.00002457
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.41e-08
        -DiffCxt:  9.17e-08
    Outer Step:  1520      LossTrajs: 0.00001652     ContextsNorm: 0.01228633     ValIndCrit: 0.00002469
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.28e-08
        -DiffCxt:  7.24e-08
    Outer Step:  1530      LossTrajs: 0.00001762     ContextsNorm: 0.01226315     ValIndCrit: 0.00002552
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.92e-08
        -DiffCxt:  2.07e-07
    Outer Step:  1540      LossTrajs: 0.00001569     ContextsNorm: 0.01223553     ValIndCrit: 0.00002529
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.68e-09
        -DiffCxt:  5.83e-08
    Outer Step:  1550      LossTrajs: 0.00001564     ContextsNorm: 0.01223068     ValIndCrit: 0.00002368
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.28e-09
        -DiffCxt:  3.23e-08
    Outer Step:  1560      LossTrajs: 0.00001769     ContextsNorm: 0.01216676     ValIndCrit: 0.00002483
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.07e-08
        -DiffCxt:  1.22e-07
    Outer Step:  1570      LossTrajs: 0.00001606     ContextsNorm: 0.01213015     ValIndCrit: 0.00002423
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.29e-09
        -DiffCxt:  8.40e-08
    Outer Step:  1580      LossTrajs: 0.00001607     ContextsNorm: 0.01211395     ValIndCrit: 0.00002405
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.24e-08
        -DiffCxt:  3.28e-08
    Outer Step:  1590      LossTrajs: 0.00001703     ContextsNorm: 0.01208568     ValIndCrit: 0.00002402
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.51e-08
        -DiffCxt:  1.95e-07
    Outer Step:  1600      LossTrajs: 0.00001958     ContextsNorm: 0.01205997     ValIndCrit: 0.00002521
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.15e-08
        -DiffCxt:  5.55e-07
    Outer Step:  1610      LossTrajs: 0.00001551     ContextsNorm: 0.01201799     ValIndCrit: 0.00002323
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.84e-09
        -DiffCxt:  7.07e-08
    Outer Step:  1620      LossTrajs: 0.00001708     ContextsNorm: 0.01203219     ValIndCrit: 0.00002418
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.61e-08
        -DiffCxt:  4.14e-07
    Outer Step:  1630      LossTrajs: 0.00001548     ContextsNorm: 0.01197244     ValIndCrit: 0.00002376
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.59e-09
        -DiffCxt:  3.05e-08
    Outer Step:  1640      LossTrajs: 0.00001864     ContextsNorm: 0.01195543     ValIndCrit: 0.00002179
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.01e-07
        -DiffCxt:  1.11e-07
    Outer Step:  1650      LossTrajs: 0.00001506     ContextsNorm: 0.01190468     ValIndCrit: 0.00002290
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.43e-09
        -DiffCxt:  3.25e-08
    Outer Step:  1660      LossTrajs: 0.00001506     ContextsNorm: 0.01189822     ValIndCrit: 0.00002298
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.27e-09
        -DiffCxt:  5.58e-08
    Outer Step:  1670      LossTrajs: 0.00001957     ContextsNorm: 0.01190132     ValIndCrit: 0.00002310
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.19e-08
        -DiffCxt:  1.75e-06
    Outer Step:  1680      LossTrajs: 0.00001722     ContextsNorm: 0.01187740     ValIndCrit: 0.00002284
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.07e-08
        -DiffCxt:  3.61e-07
    Outer Step:  1690      LossTrajs: 0.00001641     ContextsNorm: 0.01186730     ValIndCrit: 0.00002221
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.58e-08
        -DiffCxt:  1.71e-07
    Outer Step:  1700      LossTrajs: 0.00001628     ContextsNorm: 0.01187136     ValIndCrit: 0.00002241
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.83e-08
        -DiffCxt:  1.39e-07
    Outer Step:  1710      LossTrajs: 0.00001673     ContextsNorm: 0.01194003     ValIndCrit: 0.00002250
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.01e-08
        -DiffCxt:  2.96e-07
    Outer Step:  1720      LossTrajs: 0.00001574     ContextsNorm: 0.01195429     ValIndCrit: 0.00002237
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.72e-08
        -DiffCxt:  3.92e-08
    Outer Step:  1730      LossTrajs: 0.00001645     ContextsNorm: 0.01192066     ValIndCrit: 0.00002291
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.93e-08
        -DiffCxt:  1.46e-07
    Outer Step:  1740      LossTrajs: 0.00001507     ContextsNorm: 0.01186769     ValIndCrit: 0.00002238
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.37e-09
        -DiffCxt:  2.93e-08
    Outer Step:  1750      LossTrajs: 0.00001482     ContextsNorm: 0.01182312     ValIndCrit: 0.00002262
        -NbInnerStepsNode:    7
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.44e-10
        -DiffCxt:  3.81e-08
    Outer Step:  1760      LossTrajs: 0.00001998     ContextsNorm: 0.01176278     ValIndCrit: 0.00002460
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.73e-07
        -DiffCxt:  2.19e-06
    Outer Step:  1770      LossTrajs: 0.00001476     ContextsNorm: 0.01177692     ValIndCrit: 0.00002172
        Saving best model so far ...
        -NbInnerStepsNode:    8
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.51e-10
        -DiffCxt:  5.43e-08
    Outer Step:  1780      LossTrajs: 0.00001470     ContextsNorm: 0.01174528     ValIndCrit: 0.00002203
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.33e-10
        -DiffCxt:  3.52e-08
    Outer Step:  1790      LossTrajs: 0.00001552     ContextsNorm: 0.01176356     ValIndCrit: 0.00002310
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.61e-08
        -DiffCxt:  1.01e-07
    Outer Step:  1800      LossTrajs: 0.00001643     ContextsNorm: 0.01183594     ValIndCrit: 0.00002259
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.74e-08
        -DiffCxt:  1.98e-07
    Outer Step:  1810      LossTrajs: 0.00001587     ContextsNorm: 0.01188209     ValIndCrit: 0.00002232
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.90e-09
        -DiffCxt:  1.87e-07
    Outer Step:  1820      LossTrajs: 0.00001511     ContextsNorm: 0.01193197     ValIndCrit: 0.00002190
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.22e-09
        -DiffCxt:  4.73e-08
    Outer Step:  1830      LossTrajs: 0.00001497     ContextsNorm: 0.01189150     ValIndCrit: 0.00002152
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.31e-09
        -DiffCxt:  6.47e-08
    Outer Step:  1840      LossTrajs: 0.00001496     ContextsNorm: 0.01183811     ValIndCrit: 0.00002251
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.23e-08
        -DiffCxt:  1.58e-07
    Outer Step:  1850      LossTrajs: 0.00002036     ContextsNorm: 0.01174675     ValIndCrit: 0.00002164
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.43e-08
        -DiffCxt:  2.83e-07
    Outer Step:  1860      LossTrajs: 0.00001534     ContextsNorm: 0.01171505     ValIndCrit: 0.00002061
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.52e-08
        -DiffCxt:  9.63e-08
    Outer Step:  1870      LossTrajs: 0.00001546     ContextsNorm: 0.01171230     ValIndCrit: 0.00002137
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.20e-08
        -DiffCxt:  1.54e-07
    Outer Step:  1880      LossTrajs: 0.00001498     ContextsNorm: 0.01169723     ValIndCrit: 0.00002244
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.69e-09
        -DiffCxt:  2.94e-08
    Outer Step:  1890      LossTrajs: 0.00001707     ContextsNorm: 0.01166368     ValIndCrit: 0.00002104
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.25e-08
        -DiffCxt:  3.16e-08
    Outer Step:  1900      LossTrajs: 0.00001604     ContextsNorm: 0.01173175     ValIndCrit: 0.00002165
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.90e-08
        -DiffCxt:  5.87e-08
    Outer Step:  1910      LossTrajs: 0.00001472     ContextsNorm: 0.01173276     ValIndCrit: 0.00002127
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.68e-09
        -DiffCxt:  5.44e-08
    Outer Step:  1920      LossTrajs: 0.00002132     ContextsNorm: 0.01169796     ValIndCrit: 0.00002553
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.10e-07
        -DiffCxt:  1.32e-06
    Outer Step:  1930      LossTrajs: 0.00001469     ContextsNorm: 0.01168977     ValIndCrit: 0.00002091
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.04e-09
        -DiffCxt:  4.18e-08
    Outer Step:  1940      LossTrajs: 0.00001554     ContextsNorm: 0.01166567     ValIndCrit: 0.00002231
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.49e-08
        -DiffCxt:  1.21e-07
    Outer Step:  1950      LossTrajs: 0.00001714     ContextsNorm: 0.01162652     ValIndCrit: 0.00002188
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.68e-08
        -DiffCxt:  2.11e-07
    Outer Step:  1960      LossTrajs: 0.00001481     ContextsNorm: 0.01158286     ValIndCrit: 0.00002135
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.05e-08
        -DiffCxt:  1.36e-07
    Outer Step:  1970      LossTrajs: 0.00001648     ContextsNorm: 0.01160661     ValIndCrit: 0.00002125
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.11e-08
        -DiffCxt:  1.85e-07
    Outer Step:  1980      LossTrajs: 0.00002177     ContextsNorm: 0.01161198     ValIndCrit: 0.00002215
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.87e-08
        -DiffCxt:  8.58e-07
    Outer Step:  1990      LossTrajs: 0.00001521     ContextsNorm: 0.01156929     ValIndCrit: 0.00002029
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.48e-08
        -DiffCxt:  6.10e-08
    Outer Step:  1999      LossTrajs: 0.00001598     ContextsNorm: 0.01158738     ValIndCrit: 0.00002183
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.79e-08
        -DiffCxt:  4.00e-07

Total gradient descent training time: 4 hours 44 mins 18 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 220652
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 2.029211e-05


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./adapt/
 Seed: 6078


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt_test
 Savepath: ./adapt/
 Seed: 6078

==  Begining in-domain visualisation ... ==
    Environment id: 4
    Trajectory id: 15
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./results_in_domain.png
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.02796384
    Epoch:     1     LossContext: 0.01340953
    Epoch:     2     LossContext: 0.00279659
    Epoch:     3     LossContext: 0.00050142
    Epoch:    10     LossContext: 0.00104471
    Epoch:    20     LossContext: 0.00121940
    Epoch:    30     LossContext: 0.00058873
    Epoch:    40     LossContext: 0.00003821
    Epoch:    50     LossContext: 0.00005903
    Epoch:    60     LossContext: 0.00003337
    Epoch:    70     LossContext: 0.00001916
    Epoch:    80     LossContext: 0.00001599
    Epoch:    90     LossContext: 0.00001573
    Epoch:   100     LossContext: 0.00001446
    Epoch:   110     LossContext: 0.00001448
    Epoch:   120     LossContext: 0.00001444
    Epoch:   130     LossContext: 0.00001437
    Epoch:   140     LossContext: 0.00001433
    Epoch:   150     LossContext: 0.00001429
    Epoch:   160     LossContext: 0.00001426
    Epoch:   170     LossContext: 0.00001423
    Epoch:   180     LossContext: 0.00001420
    Epoch:   190     LossContext: 0.00001417
    Epoch:   200     LossContext: 0.00001414
    Epoch:   210     LossContext: 0.00001411
    Epoch:   220     LossContext: 0.00001408
    Epoch:   230     LossContext: 0.00001405
    Epoch:   240     LossContext: 0.00001402
    Epoch:   250     LossContext: 0.00001399
    Epoch:   260     LossContext: 0.00001396
    Epoch:   270     LossContext: 0.00001393
    Epoch:   280     LossContext: 0.00001390
    Epoch:   290     LossContext: 0.00001386
    Epoch:   300     LossContext: 0.00001383
    Epoch:   310     LossContext: 0.00001380
    Epoch:   320     LossContext: 0.00001376
    Epoch:   330     LossContext: 0.00001373
    Epoch:   340     LossContext: 0.00001369
    Epoch:   350     LossContext: 0.00001366
    Epoch:   360     LossContext: 0.00001362
    Epoch:   370     LossContext: 0.00001359
    Epoch:   380     LossContext: 0.00001355
    Epoch:   390     LossContext: 0.00001351
    Epoch:   400     LossContext: 0.00001347
    Epoch:   410     LossContext: 0.00001344
    Epoch:   420     LossContext: 0.00001340
    Epoch:   430     LossContext: 0.00001336
    Epoch:   440     LossContext: 0.00001332
    Epoch:   450     LossContext: 0.00001329
    Epoch:   460     LossContext: 0.00001325
    Epoch:   470     LossContext: 0.00001321
    Epoch:   480     LossContext: 0.00001318
    Epoch:   490     LossContext: 0.00001314
    Epoch:   500     LossContext: 0.00001311
    Epoch:   510     LossContext: 0.00001308
    Epoch:   520     LossContext: 0.00001305
    Epoch:   530     LossContext: 0.00001302
    Epoch:   540     LossContext: 0.00001300
    Epoch:   550     LossContext: 0.00001298
    Epoch:   560     LossContext: 0.00001295
    Epoch:   570     LossContext: 0.00001293
    Epoch:   580     LossContext: 0.00001291
    Epoch:   590     LossContext: 0.00001288
    Epoch:   600     LossContext: 0.00001286
    Epoch:   610     LossContext: 0.00001284
    Epoch:   620     LossContext: 0.00001282
    Epoch:   630     LossContext: 0.00001280
    Epoch:   640     LossContext: 0.00001279
    Epoch:   650     LossContext: 0.00001277
    Epoch:   660     LossContext: 0.00001275
    Epoch:   670     LossContext: 0.00001274
    Epoch:   680     LossContext: 0.00001273
    Epoch:   690     LossContext: 0.00001273
    Epoch:   700     LossContext: 0.00001272
    Epoch:   710     LossContext: 0.00001271
    Epoch:   720     LossContext: 0.00001271
    Epoch:   730     LossContext: 0.00001270
    Epoch:   740     LossContext: 0.00001269
    Epoch:   750     LossContext: 0.00001269
    Epoch:   760     LossContext: 0.00001268
    Epoch:   770     LossContext: 0.00001267
    Epoch:   780     LossContext: 0.00001266
    Epoch:   790     LossContext: 0.00001266
    Epoch:   800     LossContext: 0.00001265
    Epoch:   810     LossContext: 0.00001264
    Epoch:   820     LossContext: 0.00001263
    Epoch:   830     LossContext: 0.00001263
    Epoch:   840     LossContext: 0.00001262
    Epoch:   850     LossContext: 0.00001261
    Epoch:   860     LossContext: 0.00001261
    Epoch:   870     LossContext: 0.00001260
    Epoch:   880     LossContext: 0.00001259
    Epoch:   890     LossContext: 0.00001259
    Epoch:   900     LossContext: 0.00001258
    Epoch:   910     LossContext: 0.00001257
    Epoch:   920     LossContext: 0.00001257
    Epoch:   930     LossContext: 0.00001256
    Epoch:   940     LossContext: 0.00001256
    Epoch:   950     LossContext: 0.00001255
    Epoch:   960     LossContext: 0.00001254
    Epoch:   970     LossContext: 0.00001254
    Epoch:   980     LossContext: 0.00001253
    Epoch:   990     LossContext: 0.00001252
    Epoch:  1000     LossContext: 0.00001252
    Epoch:  1010     LossContext: 0.00001251
    Epoch:  1020     LossContext: 0.00001251
    Epoch:  1030     LossContext: 0.00001250
    Epoch:  1040     LossContext: 0.00001249
    Epoch:  1050     LossContext: 0.00001249
    Epoch:  1060     LossContext: 0.00001248
    Epoch:  1070     LossContext: 0.00001248
    Epoch:  1080     LossContext: 0.00001247
    Epoch:  1090     LossContext: 0.00001246
    Epoch:  1100     LossContext: 0.00001246
    Epoch:  1110     LossContext: 0.00001245
    Epoch:  1120     LossContext: 0.00001245
    Epoch:  1130     LossContext: 0.00001244
    Epoch:  1140     LossContext: 0.00001243
    Epoch:  1150     LossContext: 0.00001243
    Epoch:  1160     LossContext: 0.00001242
    Epoch:  1170     LossContext: 0.00001242
    Epoch:  1180     LossContext: 0.00001241
    Epoch:  1190     LossContext: 0.00001240
    Epoch:  1200     LossContext: 0.00001240
    Epoch:  1210     LossContext: 0.00001239
    Epoch:  1220     LossContext: 0.00001239
    Epoch:  1230     LossContext: 0.00001238
    Epoch:  1240     LossContext: 0.00001237
    Epoch:  1250     LossContext: 0.00001237
    Epoch:  1260     LossContext: 0.00001236
    Epoch:  1270     LossContext: 0.00001235
    Epoch:  1280     LossContext: 0.00001235
    Epoch:  1290     LossContext: 0.00001234
    Epoch:  1300     LossContext: 0.00001234
    Epoch:  1310     LossContext: 0.00001233
    Epoch:  1320     LossContext: 0.00001232
    Epoch:  1330     LossContext: 0.00001232
    Epoch:  1340     LossContext: 0.00001231
    Epoch:  1350     LossContext: 0.00001231
    Epoch:  1360     LossContext: 0.00001231
    Epoch:  1370     LossContext: 0.00001230
    Epoch:  1380     LossContext: 0.00001230
    Epoch:  1390     LossContext: 0.00001230
    Epoch:  1400     LossContext: 0.00001229
    Epoch:  1410     LossContext: 0.00001229
    Epoch:  1420     LossContext: 0.00001229
    Epoch:  1430     LossContext: 0.00001228
    Epoch:  1440     LossContext: 0.00001228
    Epoch:  1450     LossContext: 0.00001228
    Epoch:  1460     LossContext: 0.00001227
    Epoch:  1470     LossContext: 0.00001227
    Epoch:  1480     LossContext: 0.00001227
    Epoch:  1490     LossContext: 0.00001226
    Epoch:  1499     LossContext: 0.00001226

Gradient descent adaptation time: 0 hours 1 mins 36 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.13387109
    Epoch:     1     LossContext: 0.09610978
    Epoch:     2     LossContext: 0.06132535
    Epoch:     3     LossContext: 0.03113988
    Epoch:    10     LossContext: 0.01293758
    Epoch:    20     LossContext: 0.00675859
    Epoch:    30     LossContext: 0.00205889
    Epoch:    40     LossContext: 0.00027799
    Epoch:    50     LossContext: 0.00015645
    Epoch:    60     LossContext: 0.00010450
    Epoch:    70     LossContext: 0.00007034
    Epoch:    80     LossContext: 0.00003740
    Epoch:    90     LossContext: 0.00003332
    Epoch:   100     LossContext: 0.00003250
    Epoch:   110     LossContext: 0.00003001
    Epoch:   120     LossContext: 0.00003010
    Epoch:   130     LossContext: 0.00002985
    Epoch:   140     LossContext: 0.00002986
    Epoch:   150     LossContext: 0.00002981
    Epoch:   160     LossContext: 0.00002979
    Epoch:   170     LossContext: 0.00002977
    Epoch:   180     LossContext: 0.00002975
    Epoch:   190     LossContext: 0.00002973
    Epoch:   200     LossContext: 0.00002970
    Epoch:   210     LossContext: 0.00002968
    Epoch:   220     LossContext: 0.00002966
    Epoch:   230     LossContext: 0.00002963
    Epoch:   240     LossContext: 0.00002961
    Epoch:   250     LossContext: 0.00002958
    Epoch:   260     LossContext: 0.00002956
    Epoch:   270     LossContext: 0.00002953
    Epoch:   280     LossContext: 0.00002951
    Epoch:   290     LossContext: 0.00002948
    Epoch:   300     LossContext: 0.00002945
    Epoch:   310     LossContext: 0.00002943
    Epoch:   320     LossContext: 0.00002940
    Epoch:   330     LossContext: 0.00002937
    Epoch:   340     LossContext: 0.00002934
    Epoch:   350     LossContext: 0.00002931
    Epoch:   360     LossContext: 0.00002928
    Epoch:   370     LossContext: 0.00002925
    Epoch:   380     LossContext: 0.00002922
    Epoch:   390     LossContext: 0.00002919
    Epoch:   400     LossContext: 0.00002916
    Epoch:   410     LossContext: 0.00002913
    Epoch:   420     LossContext: 0.00002910
    Epoch:   430     LossContext: 0.00002907
    Epoch:   440     LossContext: 0.00002904
    Epoch:   450     LossContext: 0.00002900
    Epoch:   460     LossContext: 0.00002897
    Epoch:   470     LossContext: 0.00002894
    Epoch:   480     LossContext: 0.00002890
    Epoch:   490     LossContext: 0.00002887
    Epoch:   500     LossContext: 0.00002884
    Epoch:   510     LossContext: 0.00002880
    Epoch:   520     LossContext: 0.00002877
    Epoch:   530     LossContext: 0.00002873
    Epoch:   540     LossContext: 0.00002870
    Epoch:   550     LossContext: 0.00002866
    Epoch:   560     LossContext: 0.00002863
    Epoch:   570     LossContext: 0.00002859
    Epoch:   580     LossContext: 0.00002855
    Epoch:   590     LossContext: 0.00002852
    Epoch:   600     LossContext: 0.00002848
    Epoch:   610     LossContext: 0.00002845
    Epoch:   620     LossContext: 0.00002841
    Epoch:   630     LossContext: 0.00002837
    Epoch:   640     LossContext: 0.00002833
    Epoch:   650     LossContext: 0.00002830
    Epoch:   660     LossContext: 0.00002826
    Epoch:   670     LossContext: 0.00002823
    Epoch:   680     LossContext: 0.00002821
    Epoch:   690     LossContext: 0.00002819
    Epoch:   700     LossContext: 0.00002817
    Epoch:   710     LossContext: 0.00002815
    Epoch:   720     LossContext: 0.00002813
    Epoch:   730     LossContext: 0.00002811
    Epoch:   740     LossContext: 0.00002809
    Epoch:   750     LossContext: 0.00002807
    Epoch:   760     LossContext: 0.00002805
    Epoch:   770     LossContext: 0.00002803
    Epoch:   780     LossContext: 0.00002801
    Epoch:   790     LossContext: 0.00002799
    Epoch:   800     LossContext: 0.00002797
    Epoch:   810     LossContext: 0.00002795
    Epoch:   820     LossContext: 0.00002793
    Epoch:   830     LossContext: 0.00002791
    Epoch:   840     LossContext: 0.00002788
    Epoch:   850     LossContext: 0.00002786
    Epoch:   860     LossContext: 0.00002784
    Epoch:   870     LossContext: 0.00002782
    Epoch:   880     LossContext: 0.00002780
    Epoch:   890     LossContext: 0.00002777
    Epoch:   900     LossContext: 0.00002775
    Epoch:   910     LossContext: 0.00002773
    Epoch:   920     LossContext: 0.00002771
    Epoch:   930     LossContext: 0.00002768
    Epoch:   940     LossContext: 0.00002766
    Epoch:   950     LossContext: 0.00002764
    Epoch:   960     LossContext: 0.00002761
    Epoch:   970     LossContext: 0.00002759
    Epoch:   980     LossContext: 0.00002757
    Epoch:   990     LossContext: 0.00002754
    Epoch:  1000     LossContext: 0.00002752
    Epoch:  1010     LossContext: 0.00002750
    Epoch:  1020     LossContext: 0.00002747
    Epoch:  1030     LossContext: 0.00002745
    Epoch:  1040     LossContext: 0.00002742
    Epoch:  1050     LossContext: 0.00002740
    Epoch:  1060     LossContext: 0.00002737
    Epoch:  1070     LossContext: 0.00002735
    Epoch:  1080     LossContext: 0.00002732
    Epoch:  1090     LossContext: 0.00002730
    Epoch:  1100     LossContext: 0.00002727
    Epoch:  1110     LossContext: 0.00002725
    Epoch:  1120     LossContext: 0.00002722
    Epoch:  1130     LossContext: 0.00002719
    Epoch:  1140     LossContext: 0.00002717
    Epoch:  1150     LossContext: 0.00002714
    Epoch:  1160     LossContext: 0.00002712
    Epoch:  1170     LossContext: 0.00002709
    Epoch:  1180     LossContext: 0.00002706
    Epoch:  1190     LossContext: 0.00002704
    Epoch:  1200     LossContext: 0.00002701
    Epoch:  1210     LossContext: 0.00002698
    Epoch:  1220     LossContext: 0.00002696
    Epoch:  1230     LossContext: 0.00002693
    Epoch:  1240     LossContext: 0.00002690
    Epoch:  1250     LossContext: 0.00002687
    Epoch:  1260     LossContext: 0.00002684
    Epoch:  1270     LossContext: 0.00002682
    Epoch:  1280     LossContext: 0.00002679
    Epoch:  1290     LossContext: 0.00002676
    Epoch:  1300     LossContext: 0.00002673
    Epoch:  1310     LossContext: 0.00002670
    Epoch:  1320     LossContext: 0.00002667
    Epoch:  1330     LossContext: 0.00002665
    Epoch:  1340     LossContext: 0.00002663
    Epoch:  1350     LossContext: 0.00002661
    Epoch:  1360     LossContext: 0.00002660
    Epoch:  1370     LossContext: 0.00002658
    Epoch:  1380     LossContext: 0.00002657
    Epoch:  1390     LossContext: 0.00002655
    Epoch:  1400     LossContext: 0.00002654
    Epoch:  1410     LossContext: 0.00002652
    Epoch:  1420     LossContext: 0.00002651
    Epoch:  1430     LossContext: 0.00002649
    Epoch:  1440     LossContext: 0.00002648
    Epoch:  1450     LossContext: 0.00002646
    Epoch:  1460     LossContext: 0.00002645
    Epoch:  1470     LossContext: 0.00002643
    Epoch:  1480     LossContext: 0.00002641
    Epoch:  1490     LossContext: 0.00002640
    Epoch:  1499     LossContext: 0.00002638

Gradient descent adaptation time: 0 hours 1 mins 27 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.03713315
    Epoch:     1     LossContext: 0.02100294
    Epoch:     2     LossContext: 0.01042945
    Epoch:     3     LossContext: 0.00421531
    Epoch:    10     LossContext: 0.00349160
    Epoch:    20     LossContext: 0.00179846
    Epoch:    30     LossContext: 0.00059600
    Epoch:    40     LossContext: 0.00004317
    Epoch:    50     LossContext: 0.00015362
    Epoch:    60     LossContext: 0.00005206
    Epoch:    70     LossContext: 0.00002841
    Epoch:    80     LossContext: 0.00003201
    Epoch:    90     LossContext: 0.00003020
    Epoch:   100     LossContext: 0.00002826
    Epoch:   110     LossContext: 0.00002766
    Epoch:   120     LossContext: 0.00002748
    Epoch:   130     LossContext: 0.00002733
    Epoch:   140     LossContext: 0.00002717
    Epoch:   150     LossContext: 0.00002701
    Epoch:   160     LossContext: 0.00002686
    Epoch:   170     LossContext: 0.00002670
    Epoch:   180     LossContext: 0.00002655
    Epoch:   190     LossContext: 0.00002640
    Epoch:   200     LossContext: 0.00002625
    Epoch:   210     LossContext: 0.00002612
    Epoch:   220     LossContext: 0.00002599
    Epoch:   230     LossContext: 0.00002586
    Epoch:   240     LossContext: 0.00002573
    Epoch:   250     LossContext: 0.00002560
    Epoch:   260     LossContext: 0.00002547
    Epoch:   270     LossContext: 0.00002535
    Epoch:   280     LossContext: 0.00002522
    Epoch:   290     LossContext: 0.00002510
    Epoch:   300     LossContext: 0.00002498
    Epoch:   310     LossContext: 0.00002487
    Epoch:   320     LossContext: 0.00002477
    Epoch:   330     LossContext: 0.00002467
    Epoch:   340     LossContext: 0.00002457
    Epoch:   350     LossContext: 0.00002447
    Epoch:   360     LossContext: 0.00002437
    Epoch:   370     LossContext: 0.00002428
    Epoch:   380     LossContext: 0.00002419
    Epoch:   390     LossContext: 0.00002410
    Epoch:   400     LossContext: 0.00002402
    Epoch:   410     LossContext: 0.00002393
    Epoch:   420     LossContext: 0.00002385
    Epoch:   430     LossContext: 0.00002377
    Epoch:   440     LossContext: 0.00002370
    Epoch:   450     LossContext: 0.00002363
    Epoch:   460     LossContext: 0.00002356
    Epoch:   470     LossContext: 0.00002350
    Epoch:   480     LossContext: 0.00002343
    Epoch:   490     LossContext: 0.00002337
    Epoch:   500     LossContext: 0.00002331
    Epoch:   510     LossContext: 0.00002325
    Epoch:   520     LossContext: 0.00002319
    Epoch:   530     LossContext: 0.00002313
    Epoch:   540     LossContext: 0.00002307
    Epoch:   550     LossContext: 0.00002301
    Epoch:   560     LossContext: 0.00002296
    Epoch:   570     LossContext: 0.00002291
    Epoch:   580     LossContext: 0.00002286
    Epoch:   590     LossContext: 0.00002281
    Epoch:   600     LossContext: 0.00002277
    Epoch:   610     LossContext: 0.00002272
    Epoch:   620     LossContext: 0.00002268
    Epoch:   630     LossContext: 0.00002263
    Epoch:   640     LossContext: 0.00002259
    Epoch:   650     LossContext: 0.00002255
    Epoch:   660     LossContext: 0.00002251
    Epoch:   670     LossContext: 0.00002248
    Epoch:   680     LossContext: 0.00002246
    Epoch:   690     LossContext: 0.00002244
    Epoch:   700     LossContext: 0.00002243
    Epoch:   710     LossContext: 0.00002241
    Epoch:   720     LossContext: 0.00002239
    Epoch:   730     LossContext: 0.00002237
    Epoch:   740     LossContext: 0.00002235
    Epoch:   750     LossContext: 0.00002234
    Epoch:   760     LossContext: 0.00002232
    Epoch:   770     LossContext: 0.00002230
    Epoch:   780     LossContext: 0.00002229
    Epoch:   790     LossContext: 0.00002227
    Epoch:   800     LossContext: 0.00002225
    Epoch:   810     LossContext: 0.00002224
    Epoch:   820     LossContext: 0.00002222
    Epoch:   830     LossContext: 0.00002221
    Epoch:   840     LossContext: 0.00002219
    Epoch:   850     LossContext: 0.00002218
    Epoch:   860     LossContext: 0.00002217
    Epoch:   870     LossContext: 0.00002215
    Epoch:   880     LossContext: 0.00002214
    Epoch:   890     LossContext: 0.00002212
    Epoch:   900     LossContext: 0.00002211
    Epoch:   910     LossContext: 0.00002209
    Epoch:   920     LossContext: 0.00002208
    Epoch:   930     LossContext: 0.00002207
    Epoch:   940     LossContext: 0.00002205
    Epoch:   950     LossContext: 0.00002204
    Epoch:   960     LossContext: 0.00002203
    Epoch:   970     LossContext: 0.00002201
    Epoch:   980     LossContext: 0.00002200
    Epoch:   990     LossContext: 0.00002199
    Epoch:  1000     LossContext: 0.00002198
    Epoch:  1010     LossContext: 0.00002197
    Epoch:  1020     LossContext: 0.00002196
    Epoch:  1030     LossContext: 0.00002194
    Epoch:  1040     LossContext: 0.00002193
    Epoch:  1050     LossContext: 0.00002192
    Epoch:  1060     LossContext: 0.00002191
    Epoch:  1070     LossContext: 0.00002190
    Epoch:  1080     LossContext: 0.00002189
    Epoch:  1090     LossContext: 0.00002188
    Epoch:  1100     LossContext: 0.00002187
    Epoch:  1110     LossContext: 0.00002186
    Epoch:  1120     LossContext: 0.00002185
    Epoch:  1130     LossContext: 0.00002183
    Epoch:  1140     LossContext: 0.00002182
    Epoch:  1150     LossContext: 0.00002181
    Epoch:  1160     LossContext: 0.00002180
    Epoch:  1170     LossContext: 0.00002179
    Epoch:  1180     LossContext: 0.00002178
    Epoch:  1190     LossContext: 0.00002177
    Epoch:  1200     LossContext: 0.00002176
    Epoch:  1210     LossContext: 0.00002175
    Epoch:  1220     LossContext: 0.00002174
    Epoch:  1230     LossContext: 0.00002173
    Epoch:  1240     LossContext: 0.00002172
    Epoch:  1250     LossContext: 0.00002171
    Epoch:  1260     LossContext: 0.00002169
    Epoch:  1270     LossContext: 0.00002168
    Epoch:  1280     LossContext: 0.00002167
    Epoch:  1290     LossContext: 0.00002166
    Epoch:  1300     LossContext: 0.00002165
    Epoch:  1310     LossContext: 0.00002164
    Epoch:  1320     LossContext: 0.00002163
    Epoch:  1330     LossContext: 0.00002162
    Epoch:  1340     LossContext: 0.00002162
    Epoch:  1350     LossContext: 0.00002161
    Epoch:  1360     LossContext: 0.00002161
    Epoch:  1370     LossContext: 0.00002161
    Epoch:  1380     LossContext: 0.00002160
    Epoch:  1390     LossContext: 0.00002160
    Epoch:  1400     LossContext: 0.00002159
    Epoch:  1410     LossContext: 0.00002159
    Epoch:  1420     LossContext: 0.00002158
    Epoch:  1430     LossContext: 0.00002158
    Epoch:  1440     LossContext: 0.00002157
    Epoch:  1450     LossContext: 0.00002157
    Epoch:  1460     LossContext: 0.00002157
    Epoch:  1470     LossContext: 0.00002156
    Epoch:  1480     LossContext: 0.00002156
    Epoch:  1490     LossContext: 0.00002155
    Epoch:  1499     LossContext: 0.00002155

Gradient descent adaptation time: 0 hours 1 mins 27 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.02116286
    Epoch:     1     LossContext: 0.01082441
    Epoch:     2     LossContext: 0.00504938
    Epoch:     3     LossContext: 0.00207424
    Epoch:    10     LossContext: 0.00101281
    Epoch:    20     LossContext: 0.00158583
    Epoch:    30     LossContext: 0.00014919
    Epoch:    40     LossContext: 0.00019832
    Epoch:    50     LossContext: 0.00003462
    Epoch:    60     LossContext: 0.00005393
    Epoch:    70     LossContext: 0.00002699
    Epoch:    80     LossContext: 0.00003032
    Epoch:    90     LossContext: 0.00002694
    Epoch:   100     LossContext: 0.00002662
    Epoch:   110     LossContext: 0.00002643
    Epoch:   120     LossContext: 0.00002610
    Epoch:   130     LossContext: 0.00002594
    Epoch:   140     LossContext: 0.00002577
    Epoch:   150     LossContext: 0.00002559
    Epoch:   160     LossContext: 0.00002543
    Epoch:   170     LossContext: 0.00002527
    Epoch:   180     LossContext: 0.00002511
    Epoch:   190     LossContext: 0.00002496
    Epoch:   200     LossContext: 0.00002481
    Epoch:   210     LossContext: 0.00002466
    Epoch:   220     LossContext: 0.00002451
    Epoch:   230     LossContext: 0.00002437
    Epoch:   240     LossContext: 0.00002423
    Epoch:   250     LossContext: 0.00002408
    Epoch:   260     LossContext: 0.00002394
    Epoch:   270     LossContext: 0.00002379
    Epoch:   280     LossContext: 0.00002365
    Epoch:   290     LossContext: 0.00002351
    Epoch:   300     LossContext: 0.00002338
    Epoch:   310     LossContext: 0.00002325
    Epoch:   320     LossContext: 0.00002312
    Epoch:   330     LossContext: 0.00002299
    Epoch:   340     LossContext: 0.00002287
    Epoch:   350     LossContext: 0.00002275
    Epoch:   360     LossContext: 0.00002264
    Epoch:   370     LossContext: 0.00002253
    Epoch:   380     LossContext: 0.00002243
    Epoch:   390     LossContext: 0.00002232
    Epoch:   400     LossContext: 0.00002222
    Epoch:   410     LossContext: 0.00002212
    Epoch:   420     LossContext: 0.00002203
    Epoch:   430     LossContext: 0.00002194
    Epoch:   440     LossContext: 0.00002185
    Epoch:   450     LossContext: 0.00002176
    Epoch:   460     LossContext: 0.00002167
    Epoch:   470     LossContext: 0.00002160
    Epoch:   480     LossContext: 0.00002152
    Epoch:   490     LossContext: 0.00002145
    Epoch:   500     LossContext: 0.00002138
    Epoch:   510     LossContext: 0.00002130
    Epoch:   520     LossContext: 0.00002124
    Epoch:   530     LossContext: 0.00002117
    Epoch:   540     LossContext: 0.00002111
    Epoch:   550     LossContext: 0.00002105
    Epoch:   560     LossContext: 0.00002099
    Epoch:   570     LossContext: 0.00002093
    Epoch:   580     LossContext: 0.00002087
    Epoch:   590     LossContext: 0.00002082
    Epoch:   600     LossContext: 0.00002077
    Epoch:   610     LossContext: 0.00002072
    Epoch:   620     LossContext: 0.00002068
    Epoch:   630     LossContext: 0.00002064
    Epoch:   640     LossContext: 0.00002061
    Epoch:   650     LossContext: 0.00002058
    Epoch:   660     LossContext: 0.00002055
    Epoch:   670     LossContext: 0.00002052
    Epoch:   680     LossContext: 0.00002051
    Epoch:   690     LossContext: 0.00002049
    Epoch:   700     LossContext: 0.00002048
    Epoch:   710     LossContext: 0.00002046
    Epoch:   720     LossContext: 0.00002045
    Epoch:   730     LossContext: 0.00002043
    Epoch:   740     LossContext: 0.00002042
    Epoch:   750     LossContext: 0.00002040
    Epoch:   760     LossContext: 0.00002039
    Epoch:   770     LossContext: 0.00002037
    Epoch:   780     LossContext: 0.00002035
    Epoch:   790     LossContext: 0.00002034
    Epoch:   800     LossContext: 0.00002032
    Epoch:   810     LossContext: 0.00002031
    Epoch:   820     LossContext: 0.00002029
    Epoch:   830     LossContext: 0.00002028
    Epoch:   840     LossContext: 0.00002026
    Epoch:   850     LossContext: 0.00002025
    Epoch:   860     LossContext: 0.00002023
    Epoch:   870     LossContext: 0.00002021
    Epoch:   880     LossContext: 0.00002020
    Epoch:   890     LossContext: 0.00002018
    Epoch:   900     LossContext: 0.00002017
    Epoch:   910     LossContext: 0.00002015
    Epoch:   920     LossContext: 0.00002014
    Epoch:   930     LossContext: 0.00002012
    Epoch:   940     LossContext: 0.00002011
    Epoch:   950     LossContext: 0.00002010
    Epoch:   960     LossContext: 0.00002008
    Epoch:   970     LossContext: 0.00002007
    Epoch:   980     LossContext: 0.00002005
    Epoch:   990     LossContext: 0.00002004
    Epoch:  1000     LossContext: 0.00002003
    Epoch:  1010     LossContext: 0.00002001
    Epoch:  1020     LossContext: 0.00002000
    Epoch:  1030     LossContext: 0.00001998
    Epoch:  1040     LossContext: 0.00001997
    Epoch:  1050     LossContext: 0.00001996
    Epoch:  1060     LossContext: 0.00001994
    Epoch:  1070     LossContext: 0.00001993
    Epoch:  1080     LossContext: 0.00001992
    Epoch:  1090     LossContext: 0.00001990
    Epoch:  1100     LossContext: 0.00001989
    Epoch:  1110     LossContext: 0.00001987
    Epoch:  1120     LossContext: 0.00001986
    Epoch:  1130     LossContext: 0.00001985
    Epoch:  1140     LossContext: 0.00001983
    Epoch:  1150     LossContext: 0.00001982
    Epoch:  1160     LossContext: 0.00001981
    Epoch:  1170     LossContext: 0.00001979
    Epoch:  1180     LossContext: 0.00001978
    Epoch:  1190     LossContext: 0.00001976
    Epoch:  1200     LossContext: 0.00001975
    Epoch:  1210     LossContext: 0.00001974
    Epoch:  1220     LossContext: 0.00001972
    Epoch:  1230     LossContext: 0.00001971
    Epoch:  1240     LossContext: 0.00001970
    Epoch:  1250     LossContext: 0.00001968
    Epoch:  1260     LossContext: 0.00001967
    Epoch:  1270     LossContext: 0.00001965
    Epoch:  1280     LossContext: 0.00001964
    Epoch:  1290     LossContext: 0.00001963
    Epoch:  1300     LossContext: 0.00001962
    Epoch:  1310     LossContext: 0.00001960
    Epoch:  1320     LossContext: 0.00001959
    Epoch:  1330     LossContext: 0.00001958
    Epoch:  1340     LossContext: 0.00001957
    Epoch:  1350     LossContext: 0.00001956
    Epoch:  1360     LossContext: 0.00001955
    Epoch:  1370     LossContext: 0.00001955
    Epoch:  1380     LossContext: 0.00001954
    Epoch:  1390     LossContext: 0.00001953
    Epoch:  1400     LossContext: 0.00001953
    Epoch:  1410     LossContext: 0.00001952
    Epoch:  1420     LossContext: 0.00001951
    Epoch:  1430     LossContext: 0.00001951
    Epoch:  1440     LossContext: 0.00001950
    Epoch:  1450     LossContext: 0.00001949
    Epoch:  1460     LossContext: 0.00001948
    Epoch:  1470     LossContext: 0.00001948
    Epoch:  1480     LossContext: 0.00001947
    Epoch:  1490     LossContext: 0.00001946
    Epoch:  1499     LossContext: 0.00001946

Gradient descent adaptation time: 0 hours 1 mins 26 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 2.7683043e-05

