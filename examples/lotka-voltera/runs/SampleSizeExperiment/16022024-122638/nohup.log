Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: train
 Savepath: ./runs/16022024-115227/
 Seed: 2026

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: test
 Savepath: ./runs/16022024-115227/
 Seed: 4052

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./runs/16022024-115227/adapt/
 Seed: 6078


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Data folder created successfuly: ./runs/16022024-115227/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 115229
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning training ... ===
    Number of examples in a batch: 4
    Number of train steps per epoch: 1
    Number of training epochs: 24000
    Total number of training steps: 24000

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Epoch:     0      LossTrajs: 15.28007126     ContextsNorm: 0.00000000
    Epoch:     1      LossTrajs: 9.44129181     ContextsNorm: 0.00098262
    Epoch:     2      LossTrajs: 5.22011566     ContextsNorm: 0.00194848
    Epoch:     3      LossTrajs: 2.49048972     ContextsNorm: 0.00296067
    Epoch:  1000      LossTrajs: 0.00374382     ContextsNorm: 0.09556148
    Epoch:  2000      LossTrajs: 0.00125136     ContextsNorm: 0.11418320
    Epoch:  3000      LossTrajs: 0.00088010     ContextsNorm: 0.13322374
    Epoch:  4000      LossTrajs: 0.00252491     ContextsNorm: 0.16215442
    Epoch:  5000      LossTrajs: 0.00274228     ContextsNorm: 0.18882115
    Epoch:  6000      LossTrajs: 0.00199111     ContextsNorm: 0.21085554
    Epoch:  7000      LossTrajs: 0.00095087     ContextsNorm: 0.23435251
    Epoch:  8000      LossTrajs: 0.00057507     ContextsNorm: 0.26120734
    Epoch:  9000      LossTrajs: 0.00014109     ContextsNorm: 0.26138306
    Epoch: 10000      LossTrajs: 0.00010014     ContextsNorm: 0.26133251
    Epoch: 11000      LossTrajs: 0.00009597     ContextsNorm: 0.26159152
    Epoch: 12000      LossTrajs: 0.00020356     ContextsNorm: 0.26238519
    Epoch: 13000      LossTrajs: 0.00005256     ContextsNorm: 0.26274216
    Epoch: 14000      LossTrajs: 0.00004234     ContextsNorm: 0.26359245
    Epoch: 15000      LossTrajs: 0.00005140     ContextsNorm: 0.26396075
    Epoch: 16000      LossTrajs: 0.00005271     ContextsNorm: 0.26473677
    Epoch: 17000      LossTrajs: 0.00003998     ContextsNorm: 0.26471177
    Epoch: 18000      LossTrajs: 0.00002817     ContextsNorm: 0.26464725
    Epoch: 19000      LossTrajs: 0.00002892     ContextsNorm: 0.26461002
    Epoch: 20000      LossTrajs: 0.00004241     ContextsNorm: 0.26462477
    Epoch: 21000      LossTrajs: 0.00002856     ContextsNorm: 0.26451674
    Epoch: 22000      LossTrajs: 0.00003554     ContextsNorm: 0.26451889
    Epoch: 23000      LossTrajs: 0.00003108     ContextsNorm: 0.26449847
    Epoch: 23999      LossTrajs: 0.00002413     ContextsNorm: 0.26449737

Total gradient descent training time: 0 hours 24 mins 27 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 121701
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 0.00010516177

==  Begining in-domain visualisation ... ==
    Environment id: 8
    Trajectory id: 19
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/16022024-115227/results_in_domain.png
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 24000
    Total number of training steps: 24000
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (4, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.04120019
    Epoch:     1     LossContext: 0.04878373
    Epoch:     2     LossContext: 0.04887294
    Epoch:     3     LossContext: 0.04843488
    Epoch:  1000     LossContext: 0.00011551
    Epoch:  2000     LossContext: 0.00008326
    Epoch:  3000     LossContext: 0.00008981
    Epoch:  4000     LossContext: 0.00008722
    Epoch:  5000     LossContext: 0.00007822
    Epoch:  6000     LossContext: 0.00004977
    Epoch:  7000     LossContext: 0.00007936
    Epoch:  8000     LossContext: 0.00004465
    Epoch:  9000     LossContext: 0.00005278
    Epoch: 10000     LossContext: 0.00011731
    Epoch: 11000     LossContext: 0.00008255
    Epoch: 12000     LossContext: 0.00005060
    Epoch: 13000     LossContext: 0.00003622
    Epoch: 14000     LossContext: 0.00007084
    Epoch: 15000     LossContext: 0.00005982
    Epoch: 16000     LossContext: 0.00005088
    Epoch: 17000     LossContext: 0.00007645
    Epoch: 18000     LossContext: 0.00006920
    Epoch: 19000     LossContext: 0.00007143
    Epoch: 20000     LossContext: 0.00005199
    Epoch: 21000     LossContext: 0.00007166
    Epoch: 22000     LossContext: 0.00004447
    Epoch: 23000     LossContext: 0.00004414
    Epoch: 23999     LossContext: 0.00007643

Total gradient descent adaptation time: 0 hours 9 mins 15 secs
Environment weights at the end of the adaptation: [0.14774248 0.22479913 0.33272412 0.29473436]

Saving adaptation parameters into ./runs/16022024-115227/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 4.467225e-05

==  Begining out-of-distribution visualisation ... ==
    Environment id: 0
    Trajectory id: 0
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/16022024-115227/adapt/results_ood.png
          seed  ind_crit  ood_crit
count 1.00e+01  1.00e+01  1.00e+01
mean  5.25e+03  1.01e-04  2.51e-04
std   3.39e+03  7.95e-06  1.88e-04

-----------------------------------------------------------------------------------------------------------

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: train
 Savepath: ./runs/16022024-122638/
 Seed: 2026

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: test
 Savepath: ./runs/16022024-122638/
 Seed: 4052

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./runs/16022024-122638/adapt/
 Seed: 6078


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Data folder created successfuly: ./runs/16022024-122638/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 122640
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning training ... ===
    Number of examples in a batch: 4
    Number of train steps per epoch: 1
    Number of training epochs: 24000
    Total number of training steps: 24000

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Epoch:     0      LossTrajs: 15.28012562     ContextsNorm: 0.00000000
    Epoch:     1      LossTrajs: 9.44128990     ContextsNorm: 0.00098261
    Epoch:     2      LossTrajs: 5.22014618     ContextsNorm: 0.00194849
    Epoch:     3      LossTrajs: 2.49044490     ContextsNorm: 0.00296065
    Epoch:  1000      LossTrajs: 0.00191321     ContextsNorm: 0.09601472
    Epoch:  2000      LossTrajs: 0.00151187     ContextsNorm: 0.10133616
    Epoch:  3000      LossTrajs: 0.00091610     ContextsNorm: 0.10823384
    Epoch:  4000      LossTrajs: 0.00020317     ContextsNorm: 0.11848863
    Epoch:  5000      LossTrajs: 0.00119980     ContextsNorm: 0.12606487
    Epoch:  6000      LossTrajs: 0.00098973     ContextsNorm: 0.14023589
    Epoch:  7000      LossTrajs: 0.00044583     ContextsNorm: 0.14732802
    Epoch:  8000      LossTrajs: 0.00026447     ContextsNorm: 0.15924712
    Epoch:  9000      LossTrajs: 0.00006451     ContextsNorm: 0.15929300
    Epoch: 10000      LossTrajs: 0.00005586     ContextsNorm: 0.15935047
    Epoch: 11000      LossTrajs: 0.00004534     ContextsNorm: 0.15936865
    Epoch: 12000      LossTrajs: 0.00004033     ContextsNorm: 0.15938602
    Epoch: 13000      LossTrajs: 0.00002495     ContextsNorm: 0.15982996
    Epoch: 14000      LossTrajs: 0.00004692     ContextsNorm: 0.15971237
    Epoch: 15000      LossTrajs: 0.00004191     ContextsNorm: 0.15958206
    Epoch: 16000      LossTrajs: 0.00002728     ContextsNorm: 0.15980069
    Epoch: 17000      LossTrajs: 0.00001981     ContextsNorm: 0.15982480
    Epoch: 18000      LossTrajs: 0.00001825     ContextsNorm: 0.15982321
    Epoch: 19000      LossTrajs: 0.00001639     ContextsNorm: 0.15974827
    Epoch: 20000      LossTrajs: 0.00001683     ContextsNorm: 0.15971880
    Epoch: 21000      LossTrajs: 0.00001862     ContextsNorm: 0.15976809
    Epoch: 22000      LossTrajs: 0.00002900     ContextsNorm: 0.15974444
    Epoch: 23000      LossTrajs: 0.00001751     ContextsNorm: 0.15970680
    Epoch: 23999      LossTrajs: 0.00001792     ContextsNorm: 0.15968044

Total gradient descent training time: 0 hours 35 mins 46 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 130230
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 7.417796e-05

==  Begining in-domain visualisation ... ==
    Environment id: 0
    Trajectory id: 3
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/16022024-122638/results_in_domain.png
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 24000
    Total number of training steps: 24000
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (4, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.04656645
    Epoch:     1     LossContext: 0.06811714
    Epoch:     2     LossContext: 0.06791044
    Epoch:     3     LossContext: 0.06722498
    Epoch:  1000     LossContext: 0.00028544
    Epoch:  2000     LossContext: 0.00012001
    Epoch:  3000     LossContext: 0.00010967
    Epoch:  4000     LossContext: 0.00010424
    Epoch:  5000     LossContext: 0.00002374
    Epoch:  6000     LossContext: 0.00008676
    Epoch:  7000     LossContext: 0.00007820
    Epoch:  8000     LossContext: 0.00007133
    Epoch:  9000     LossContext: 0.00007087
    Epoch: 10000     LossContext: 0.00002113
    Epoch: 11000     LossContext: 0.00011825
    Epoch: 12000     LossContext: 0.00006630
    Epoch: 13000     LossContext: 0.00011487
    Epoch: 14000     LossContext: 0.00006615
    Epoch: 15000     LossContext: 0.00006718
    Epoch: 16000     LossContext: 0.00002063
    Epoch: 17000     LossContext: 0.00011322
    Epoch: 18000     LossContext: 0.00006657
    Epoch: 19000     LossContext: 0.00002086
    Epoch: 20000     LossContext: 0.00010931
    Epoch: 21000     LossContext: 0.00006428
    Epoch: 22000     LossContext: 0.00006483
    Epoch: 23000     LossContext: 0.00002618
    Epoch: 23999     LossContext: 0.00006498

Total gradient descent adaptation time: 0 hours 10 mins 57 secs
Environment weights at the end of the adaptation: [0.18978345 0.2321007  0.31362337 0.26449248]

Saving adaptation parameters into ./runs/16022024-122638/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 5.401857e-05

