Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: train
 Savepath: ./runs/16022024-115227/
 Seed: 2026

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: test
 Savepath: ./runs/16022024-115227/
 Seed: 4052

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./runs/16022024-115227/adapt/
 Seed: 6078


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Data folder created successfuly: ./runs/16022024-115227/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 115229
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning training ... ===
    Number of examples in a batch: 4
    Number of train steps per epoch: 1
    Number of training epochs: 24000
    Total number of training steps: 24000

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Epoch:     0      LossTrajs: 15.28007126     ContextsNorm: 0.00000000
    Epoch:     1      LossTrajs: 9.44129181     ContextsNorm: 0.00098262
    Epoch:     2      LossTrajs: 5.22011566     ContextsNorm: 0.00194848
    Epoch:     3      LossTrajs: 2.49048972     ContextsNorm: 0.00296067
    Epoch:  1000      LossTrajs: 0.00374382     ContextsNorm: 0.09556148
    Epoch:  2000      LossTrajs: 0.00125136     ContextsNorm: 0.11418320
    Epoch:  3000      LossTrajs: 0.00088010     ContextsNorm: 0.13322374
    Epoch:  4000      LossTrajs: 0.00252491     ContextsNorm: 0.16215442
    Epoch:  5000      LossTrajs: 0.00274228     ContextsNorm: 0.18882115
    Epoch:  6000      LossTrajs: 0.00199111     ContextsNorm: 0.21085554
    Epoch:  7000      LossTrajs: 0.00095087     ContextsNorm: 0.23435251
    Epoch:  8000      LossTrajs: 0.00057507     ContextsNorm: 0.26120734
    Epoch:  9000      LossTrajs: 0.00014109     ContextsNorm: 0.26138306
    Epoch: 10000      LossTrajs: 0.00010014     ContextsNorm: 0.26133251
    Epoch: 11000      LossTrajs: 0.00009597     ContextsNorm: 0.26159152
    Epoch: 12000      LossTrajs: 0.00020356     ContextsNorm: 0.26238519
    Epoch: 13000      LossTrajs: 0.00005256     ContextsNorm: 0.26274216
    Epoch: 14000      LossTrajs: 0.00004234     ContextsNorm: 0.26359245
    Epoch: 15000      LossTrajs: 0.00005140     ContextsNorm: 0.26396075
    Epoch: 16000      LossTrajs: 0.00005271     ContextsNorm: 0.26473677
    Epoch: 17000      LossTrajs: 0.00003998     ContextsNorm: 0.26471177
    Epoch: 18000      LossTrajs: 0.00002817     ContextsNorm: 0.26464725
    Epoch: 19000      LossTrajs: 0.00002892     ContextsNorm: 0.26461002
    Epoch: 20000      LossTrajs: 0.00004241     ContextsNorm: 0.26462477
    Epoch: 21000      LossTrajs: 0.00002856     ContextsNorm: 0.26451674
    Epoch: 22000      LossTrajs: 0.00003554     ContextsNorm: 0.26451889
    Epoch: 23000      LossTrajs: 0.00003108     ContextsNorm: 0.26449847
    Epoch: 23999      LossTrajs: 0.00002413     ContextsNorm: 0.26449737

Total gradient descent training time: 0 hours 24 mins 27 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 121701
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 0.00010516177

==  Begining in-domain visualisation ... ==
    Environment id: 8
    Trajectory id: 19
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/16022024-115227/results_in_domain.png
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 24000
    Total number of training steps: 24000
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (4, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.04120019
    Epoch:     1     LossContext: 0.04878373
    Epoch:     2     LossContext: 0.04887294
    Epoch:     3     LossContext: 0.04843488
    Epoch:  1000     LossContext: 0.00011551
    Epoch:  2000     LossContext: 0.00008326
    Epoch:  3000     LossContext: 0.00008981
    Epoch:  4000     LossContext: 0.00008722
    Epoch:  5000     LossContext: 0.00007822
    Epoch:  6000     LossContext: 0.00004977
    Epoch:  7000     LossContext: 0.00007936
    Epoch:  8000     LossContext: 0.00004465
    Epoch:  9000     LossContext: 0.00005278
    Epoch: 10000     LossContext: 0.00011731
    Epoch: 11000     LossContext: 0.00008255
    Epoch: 12000     LossContext: 0.00005060
    Epoch: 13000     LossContext: 0.00003622
    Epoch: 14000     LossContext: 0.00007084
    Epoch: 15000     LossContext: 0.00005982
    Epoch: 16000     LossContext: 0.00005088
    Epoch: 17000     LossContext: 0.00007645
    Epoch: 18000     LossContext: 0.00006920
    Epoch: 19000     LossContext: 0.00007143
    Epoch: 20000     LossContext: 0.00005199
    Epoch: 21000     LossContext: 0.00007166
    Epoch: 22000     LossContext: 0.00004447
    Epoch: 23000     LossContext: 0.00004414
    Epoch: 23999     LossContext: 0.00007643

Total gradient descent adaptation time: 0 hours 9 mins 15 secs
Environment weights at the end of the adaptation: [0.14774248 0.22479913 0.33272412 0.29473436]

Saving adaptation parameters into ./runs/16022024-115227/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 4.467225e-05

==  Begining out-of-distribution visualisation ... ==
    Environment id: 0
    Trajectory id: 0
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/16022024-115227/adapt/results_ood.png
          seed  ind_crit  ood_crit
count 1.00e+01  1.00e+01  1.00e+01
mean  5.25e+03  1.01e-04  2.51e-04
std   3.39e+03  7.95e-06  1.88e-04

-----------------------------------------------------------------------------------------------------------

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: train
 Savepath: ./runs/16022024-122638/
 Seed: 2026

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: test
 Savepath: ./runs/16022024-122638/
 Seed: 4052

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./runs/16022024-122638/adapt/
 Seed: 6078


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Data folder created successfuly: ./runs/16022024-122638/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 122640
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning training ... ===
    Number of examples in a batch: 4
    Number of train steps per epoch: 1
    Number of training epochs: 24000
    Total number of training steps: 24000

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Epoch:     0      LossTrajs: 15.28012562     ContextsNorm: 0.00000000
    Epoch:     1      LossTrajs: 9.44128990     ContextsNorm: 0.00098261
    Epoch:     2      LossTrajs: 5.22014618     ContextsNorm: 0.00194849
    Epoch:     3      LossTrajs: 2.49044490     ContextsNorm: 0.00296065
    Epoch:  1000      LossTrajs: 0.00191321     ContextsNorm: 0.09601472
    Epoch:  2000      LossTrajs: 0.00151187     ContextsNorm: 0.10133616
    Epoch:  3000      LossTrajs: 0.00091610     ContextsNorm: 0.10823384
    Epoch:  4000      LossTrajs: 0.00020317     ContextsNorm: 0.11848863
    Epoch:  5000      LossTrajs: 0.00119980     ContextsNorm: 0.12606487
    Epoch:  6000      LossTrajs: 0.00098973     ContextsNorm: 0.14023589
    Epoch:  7000      LossTrajs: 0.00044583     ContextsNorm: 0.14732802
    Epoch:  8000      LossTrajs: 0.00026447     ContextsNorm: 0.15924712
    Epoch:  9000      LossTrajs: 0.00006451     ContextsNorm: 0.15929300
    Epoch: 10000      LossTrajs: 0.00005586     ContextsNorm: 0.15935047
    Epoch: 11000      LossTrajs: 0.00004534     ContextsNorm: 0.15936865
    Epoch: 12000      LossTrajs: 0.00004033     ContextsNorm: 0.15938602
    Epoch: 13000      LossTrajs: 0.00002495     ContextsNorm: 0.15982996
    Epoch: 14000      LossTrajs: 0.00004692     ContextsNorm: 0.15971237
    Epoch: 15000      LossTrajs: 0.00004191     ContextsNorm: 0.15958206
    Epoch: 16000      LossTrajs: 0.00002728     ContextsNorm: 0.15980069
    Epoch: 17000      LossTrajs: 0.00001981     ContextsNorm: 0.15982480
    Epoch: 18000      LossTrajs: 0.00001825     ContextsNorm: 0.15982321
    Epoch: 19000      LossTrajs: 0.00001639     ContextsNorm: 0.15974827
    Epoch: 20000      LossTrajs: 0.00001683     ContextsNorm: 0.15971880
    Epoch: 21000      LossTrajs: 0.00001862     ContextsNorm: 0.15976809
    Epoch: 22000      LossTrajs: 0.00002900     ContextsNorm: 0.15974444
    Epoch: 23000      LossTrajs: 0.00001751     ContextsNorm: 0.15970680
    Epoch: 23999      LossTrajs: 0.00001792     ContextsNorm: 0.15968044

Total gradient descent training time: 0 hours 35 mins 46 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 130230
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 7.417796e-05

==  Begining in-domain visualisation ... ==
    Environment id: 0
    Trajectory id: 3
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/16022024-122638/results_in_domain.png
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 24000
    Total number of training steps: 24000
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (4, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.04656645
    Epoch:     1     LossContext: 0.06811714
    Epoch:     2     LossContext: 0.06791044
    Epoch:     3     LossContext: 0.06722498
    Epoch:  1000     LossContext: 0.00028544
    Epoch:  2000     LossContext: 0.00012001
    Epoch:  3000     LossContext: 0.00010967
    Epoch:  4000     LossContext: 0.00010424
    Epoch:  5000     LossContext: 0.00002374
    Epoch:  6000     LossContext: 0.00008676
    Epoch:  7000     LossContext: 0.00007820
    Epoch:  8000     LossContext: 0.00007133
    Epoch:  9000     LossContext: 0.00007087
    Epoch: 10000     LossContext: 0.00002113
    Epoch: 11000     LossContext: 0.00011825
    Epoch: 12000     LossContext: 0.00006630
    Epoch: 13000     LossContext: 0.00011487
    Epoch: 14000     LossContext: 0.00006615
    Epoch: 15000     LossContext: 0.00006718
    Epoch: 16000     LossContext: 0.00002063
    Epoch: 17000     LossContext: 0.00011322
    Epoch: 18000     LossContext: 0.00006657
    Epoch: 19000     LossContext: 0.00002086
    Epoch: 20000     LossContext: 0.00010931
    Epoch: 21000     LossContext: 0.00006428
    Epoch: 22000     LossContext: 0.00006483
    Epoch: 23000     LossContext: 0.00002618
    Epoch: 23999     LossContext: 0.00006498

Total gradient descent adaptation time: 0 hours 10 mins 57 secs
Environment weights at the end of the adaptation: [0.18978345 0.2321007  0.31362337 0.26449248]

Saving adaptation parameters into ./runs/16022024-122638/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 5.401857e-05

==  Begining out-of-distribution visualisation ... ==
    Environment id: 0
    Trajectory id: 0
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/16022024-122638/adapt/results_ood.png
          seed  ind_crit  ood_crit
count 1.00e+01  1.00e+01  1.00e+01
mean  5.25e+03  7.45e-05  1.74e-04
std   3.39e+03  7.89e-06  9.28e-05

-----------------------------------------------------------------------------------------------------------

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: train
 Savepath: ./runs/16022024-131350/
 Seed: 2026

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: test
 Savepath: ./runs/16022024-131350/
 Seed: 4052

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./runs/16022024-131350/adapt/
 Seed: 6078


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Data folder created successfuly: ./runs/16022024-131350/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 131352
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning training ... ===
    Number of examples in a batch: 4
    Number of train steps per epoch: 1
    Number of training epochs: 24000
    Total number of training steps: 24000

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Epoch:     0      LossTrajs: 15.28014183     ContextsNorm: 0.00000000
    Epoch:     1      LossTrajs: 9.44126129     ContextsNorm: 0.00098263
    Epoch:     2      LossTrajs: 5.22011089     ContextsNorm: 0.00194847
    Epoch:     3      LossTrajs: 2.49049568     ContextsNorm: 0.00296065
    Epoch:  1000      LossTrajs: 0.00369440     ContextsNorm: 0.09966553
    Epoch:  2000      LossTrajs: 0.00043642     ContextsNorm: 0.10170451
    Epoch:  3000      LossTrajs: 0.00051480     ContextsNorm: 0.10214472
    Epoch:  4000      LossTrajs: 0.00023225     ContextsNorm: 0.10322068
    Epoch:  5000      LossTrajs: 0.00038008     ContextsNorm: 0.10449348
    Epoch:  6000      LossTrajs: 0.00100586     ContextsNorm: 0.10520066
    Epoch:  7000      LossTrajs: 0.00122571     ContextsNorm: 0.10990802
    Epoch:  8000      LossTrajs: 0.00022954     ContextsNorm: 0.11044489
    Epoch:  9000      LossTrajs: 0.00006269     ContextsNorm: 0.11045418
    Epoch: 10000      LossTrajs: 0.00005276     ContextsNorm: 0.11000439
    Epoch: 11000      LossTrajs: 0.00004959     ContextsNorm: 0.10932592
    Epoch: 12000      LossTrajs: 0.00004649     ContextsNorm: 0.10905249
    Epoch: 13000      LossTrajs: 0.00003382     ContextsNorm: 0.10891653
    Epoch: 14000      LossTrajs: 0.00004940     ContextsNorm: 0.10890071
    Epoch: 15000      LossTrajs: 0.00003167     ContextsNorm: 0.10867821
    Epoch: 16000      LossTrajs: 0.00002882     ContextsNorm: 0.10766261
    Epoch: 17000      LossTrajs: 0.00002427     ContextsNorm: 0.10764077
    Epoch: 18000      LossTrajs: 0.00003223     ContextsNorm: 0.10762212
    Epoch: 19000      LossTrajs: 0.00001804     ContextsNorm: 0.10763402
    Epoch: 20000      LossTrajs: 0.00002198     ContextsNorm: 0.10766722
    Epoch: 21000      LossTrajs: 0.00001923     ContextsNorm: 0.10763245
    Epoch: 22000      LossTrajs: 0.00001880     ContextsNorm: 0.10759797
    Epoch: 23000      LossTrajs: 0.00001951     ContextsNorm: 0.10753027
    Epoch: 23999      LossTrajs: 0.00001865     ContextsNorm: 0.10747650

Total gradient descent training time: 0 hours 40 mins 41 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 135437
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 7.2886796e-05

==  Begining in-domain visualisation ... ==
    Environment id: 6
    Trajectory id: 27
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/16022024-131350/results_in_domain.png
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 24000
    Total number of training steps: 24000
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (4, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.06464451
    Epoch:     1     LossContext: 0.12177449
    Epoch:     2     LossContext: 0.12069739
    Epoch:     3     LossContext: 0.11944991
    Epoch:  1000     LossContext: 0.00013798
    Epoch:  2000     LossContext: 0.00010775
    Epoch:  3000     LossContext: 0.00010080
    Epoch:  4000     LossContext: 0.00007707
    Epoch:  5000     LossContext: 0.00009892
    Epoch:  6000     LossContext: 0.00006962
    Epoch:  7000     LossContext: 0.00008884
    Epoch:  8000     LossContext: 0.00006176
    Epoch:  9000     LossContext: 0.00006606
    Epoch: 10000     LossContext: 0.00008805
    Epoch: 11000     LossContext: 0.00008673
    Epoch: 12000     LossContext: 0.00008689
    Epoch: 13000     LossContext: 0.00006149
    Epoch: 14000     LossContext: 0.00005445
    Epoch: 15000     LossContext: 0.00008146
    Epoch: 16000     LossContext: 0.00008638
    Epoch: 17000     LossContext: 0.00008410
    Epoch: 18000     LossContext: 0.00008391
    Epoch: 19000     LossContext: 0.00006374
    Epoch: 20000     LossContext: 0.00006295
    Epoch: 21000     LossContext: 0.00008323
    Epoch: 22000     LossContext: 0.00006098
    Epoch: 23000     LossContext: 0.00008392
    Epoch: 23999     LossContext: 0.00006289

Total gradient descent adaptation time: 0 hours 12 mins 20 secs
Environment weights at the end of the adaptation: [0.13259543 0.2561976  0.32755625 0.2836507 ]

Saving adaptation parameters into ./runs/16022024-131350/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 5.8625097e-05

==  Begining out-of-distribution visualisation ... ==
    Environment id: 3
    Trajectory id: 0
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/16022024-131350/adapt/results_ood.png
          seed  ind_crit  ood_crit
count 1.00e+01  1.00e+01  1.00e+01
mean  5.25e+03  7.65e-05  1.36e-04
std   3.39e+03  7.14e-06  5.14e-05

-----------------------------------------------------------------------------------------------------------

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: train
 Savepath: ./runs/16022024-140720/
 Seed: 2026

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: test
 Savepath: ./runs/16022024-140720/
 Seed: 4052

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./runs/16022024-140720/adapt/
 Seed: 6078


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Data folder created successfuly: ./runs/16022024-140720/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 140722
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning training ... ===
    Number of examples in a batch: 4
    Number of train steps per epoch: 1
    Number of training epochs: 24000
    Total number of training steps: 24000

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Epoch:     0      LossTrajs: 15.28000164     ContextsNorm: 0.00000000
    Epoch:     1      LossTrajs: 9.44133759     ContextsNorm: 0.00098262
    Epoch:     2      LossTrajs: 5.22011614     ContextsNorm: 0.00194850
    Epoch:     3      LossTrajs: 2.49047089     ContextsNorm: 0.00296067
    Epoch:  1000      LossTrajs: 0.00277634     ContextsNorm: 0.09219702
    Epoch:  2000      LossTrajs: 0.00038679     ContextsNorm: 0.08848325
    Epoch:  3000      LossTrajs: 0.00024726     ContextsNorm: 0.08692347
    Epoch:  4000      LossTrajs: 0.00018586     ContextsNorm: 0.08491409
    Epoch:  5000      LossTrajs: 0.00018457     ContextsNorm: 0.08407042
    Epoch:  6000      LossTrajs: 0.00016065     ContextsNorm: 0.08291028
    Epoch:  7000      LossTrajs: 0.00020326     ContextsNorm: 0.08145955
    Epoch:  8000      LossTrajs: 0.00016430     ContextsNorm: 0.08411927
    Epoch:  9000      LossTrajs: 0.00006289     ContextsNorm: 0.08442758
    Epoch: 10000      LossTrajs: 0.00004673     ContextsNorm: 0.08438790
    Epoch: 11000      LossTrajs: 0.00003827     ContextsNorm: 0.08435284
    Epoch: 12000      LossTrajs: 0.00004624     ContextsNorm: 0.08468242
    Epoch: 13000      LossTrajs: 0.00004628     ContextsNorm: 0.08497584
    Epoch: 14000      LossTrajs: 0.00002994     ContextsNorm: 0.08581243
    Epoch: 15000      LossTrajs: 0.00003465     ContextsNorm: 0.08544850
    Epoch: 16000      LossTrajs: 0.00002920     ContextsNorm: 0.08550079
    Epoch: 17000      LossTrajs: 0.00002441     ContextsNorm: 0.08557846
    Epoch: 18000      LossTrajs: 0.00002719     ContextsNorm: 0.08561432
    Epoch: 19000      LossTrajs: 0.00002287     ContextsNorm: 0.08566970
    Epoch: 20000      LossTrajs: 0.00024857     ContextsNorm: 0.08575411
    Epoch: 21000      LossTrajs: 0.00002159     ContextsNorm: 0.08575482
    Epoch: 22000      LossTrajs: 0.00002256     ContextsNorm: 0.08580159
    Epoch: 23000      LossTrajs: 0.00002040     ContextsNorm: 0.08589390
    Epoch: 23999      LossTrajs: 0.00002049     ContextsNorm: 0.08597459

Total gradient descent training time: 0 hours 49 mins 43 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 145710
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 8.3876155e-05

==  Begining in-domain visualisation ... ==
    Environment id: 3
    Trajectory id: 18
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/16022024-140720/results_in_domain.png
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 24000
    Total number of training steps: 24000
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (4, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.06573407
    Epoch:     1     LossContext: 0.12590323
    Epoch:     2     LossContext: 0.12408198
    Epoch:     3     LossContext: 0.12251721
    Epoch:  1000     LossContext: 0.00017497
    Epoch:  2000     LossContext: 0.00010362
    Epoch:  3000     LossContext: 0.00010125
    Epoch:  4000     LossContext: 0.00009364
    Epoch:  5000     LossContext: 0.00009036
    Epoch:  6000     LossContext: 0.00008782
    Epoch:  7000     LossContext: 0.00008343
    Epoch:  8000     LossContext: 0.00007948
    Epoch:  9000     LossContext: 0.00007747
    Epoch: 10000     LossContext: 0.00007919
    Epoch: 11000     LossContext: 0.00007798
    Epoch: 12000     LossContext: 0.00007844
    Epoch: 13000     LossContext: 0.00007791
    Epoch: 14000     LossContext: 0.00009444
    Epoch: 15000     LossContext: 0.00007527
    Epoch: 16000     LossContext: 0.00007646
    Epoch: 17000     LossContext: 0.00007581
    Epoch: 18000     LossContext: 0.00007652
    Epoch: 19000     LossContext: 0.00007575
    Epoch: 20000     LossContext: 0.00007576
    Epoch: 21000     LossContext: 0.00007441
    Epoch: 22000     LossContext: 0.00007586
    Epoch: 23000     LossContext: 0.00007569
    Epoch: 23999     LossContext: 0.00007610

Total gradient descent adaptation time: 0 hours 11 mins 51 secs
Environment weights at the end of the adaptation: [0.14692213 0.2365107  0.35405692 0.2625103 ]

Saving adaptation parameters into ./runs/16022024-140720/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 5.0789375e-05

==  Begining out-of-distribution visualisation ... ==
    Environment id: 3
    Trajectory id: 0
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/16022024-140720/adapt/results_ood.png
          seed  ind_crit  ood_crit
count 1.00e+01  1.00e+01  1.00e+01
mean  5.25e+03  8.41e-05  1.78e-04
std   3.39e+03  7.27e-06  1.01e-04

-----------------------------------------------------------------------------------------------------------

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: train
 Savepath: ./runs/16022024-150923/
 Seed: 2026

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: test
 Savepath: ./runs/16022024-150923/
 Seed: 4052

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./runs/16022024-150923/adapt/
 Seed: 6078


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Data folder created successfuly: ./runs/16022024-150923/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 150925
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning training ... ===
    Number of examples in a batch: 4
    Number of train steps per epoch: 1
    Number of training epochs: 24000
    Total number of training steps: 24000

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Epoch:     0      LossTrajs: 15.28014183     ContextsNorm: 0.00000000
    Epoch:     1      LossTrajs: 9.44128990     ContextsNorm: 0.00098263
    Epoch:     2      LossTrajs: 5.22011900     ContextsNorm: 0.00194849
    Epoch:     3      LossTrajs: 2.49050808     ContextsNorm: 0.00296066
    Epoch:  1000      LossTrajs: 0.00294690     ContextsNorm: 0.08887760
    Epoch:  2000      LossTrajs: 0.00090048     ContextsNorm: 0.08653957
    Epoch:  3000      LossTrajs: 0.00034680     ContextsNorm: 0.08592117
    Epoch:  4000      LossTrajs: 0.00025294     ContextsNorm: 0.08351782
    Epoch:  5000      LossTrajs: 0.00040274     ContextsNorm: 0.08175487
    Epoch:  6000      LossTrajs: 0.00015543     ContextsNorm: 0.08006684
    Epoch:  7000      LossTrajs: 0.00013236     ContextsNorm: 0.07803082
    Epoch:  8000      LossTrajs: 0.00017954     ContextsNorm: 0.07879100
    Epoch:  9000      LossTrajs: 0.00008240     ContextsNorm: 0.07903263
    Epoch: 10000      LossTrajs: 0.00004516     ContextsNorm: 0.07905009
    Epoch: 11000      LossTrajs: 0.00003669     ContextsNorm: 0.07868383
    Epoch: 12000      LossTrajs: 0.00003486     ContextsNorm: 0.07874695
    Epoch: 13000      LossTrajs: 0.00002870     ContextsNorm: 0.07914795
    Epoch: 14000      LossTrajs: 0.00003026     ContextsNorm: 0.07900520
    Epoch: 15000      LossTrajs: 0.00002218     ContextsNorm: 0.07913633
    Epoch: 16000      LossTrajs: 0.00002498     ContextsNorm: 0.07972391
    Epoch: 17000      LossTrajs: 0.00001932     ContextsNorm: 0.07980593
    Epoch: 18000      LossTrajs: 0.00002252     ContextsNorm: 0.07985492
    Epoch: 19000      LossTrajs: 0.00002502     ContextsNorm: 0.07984635
    Epoch: 20000      LossTrajs: 0.00001876     ContextsNorm: 0.07969961
    Epoch: 21000      LossTrajs: 0.00002286     ContextsNorm: 0.07949699
    Epoch: 22000      LossTrajs: 0.00001835     ContextsNorm: 0.07962767
    Epoch: 23000      LossTrajs: 0.00002345     ContextsNorm: 0.07962418
    Epoch: 23999      LossTrajs: 0.00001963     ContextsNorm: 0.07966988

Total gradient descent training time: 0 hours 51 mins 47 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 160117
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 8.4287276e-05

==  Begining in-domain visualisation ... ==
    Environment id: 7
    Trajectory id: 21
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/16022024-150923/results_in_domain.png
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 24000
    Total number of training steps: 24000
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (4, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.08442659
    Epoch:     1     LossContext: 0.16060929
    Epoch:     2     LossContext: 0.15838204
    Epoch:     3     LossContext: 0.15544397
    Epoch:  1000     LossContext: 0.00014110
    Epoch:  2000     LossContext: 0.00009218
    Epoch:  3000     LossContext: 0.00007913
    Epoch:  4000     LossContext: 0.00007350
    Epoch:  5000     LossContext: 0.00006869
    Epoch:  6000     LossContext: 0.00006562
    Epoch:  7000     LossContext: 0.00006186
    Epoch:  8000     LossContext: 0.00006031
    Epoch:  9000     LossContext: 0.00006048
    Epoch: 10000     LossContext: 0.00006035
    Epoch: 11000     LossContext: 0.00005744
    Epoch: 12000     LossContext: 0.00005983
    Epoch: 13000     LossContext: 0.00005957
    Epoch: 14000     LossContext: 0.00005965
    Epoch: 15000     LossContext: 0.00005924
    Epoch: 16000     LossContext: 0.00006325
    Epoch: 17000     LossContext: 0.00005801
    Epoch: 18000     LossContext: 0.00005892
    Epoch: 19000     LossContext: 0.00005796
    Epoch: 20000     LossContext: 0.00005904
    Epoch: 21000     LossContext: 0.00005732
    Epoch: 22000     LossContext: 0.00005894
    Epoch: 23000     LossContext: 0.00005856
    Epoch: 23999     LossContext: 0.00005779

Total gradient descent adaptation time: 0 hours 11 mins 22 secs
Environment weights at the end of the adaptation: [0.11148523 0.26764712 0.34104374 0.27982384]

Saving adaptation parameters into ./runs/16022024-150923/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 5.2576364e-05

==  Begining out-of-distribution visualisation ... ==
    Environment id: 3
    Trajectory id: 0
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/16022024-150923/adapt/results_ood.png
          seed  ind_crit  ood_crit
count 1.00e+01  1.00e+01  1.00e+01
mean  5.25e+03  9.05e-05  1.74e-04
std   3.39e+03  2.65e-05  8.63e-05

-----------------------------------------------------------------------------------------------------------

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: train
 Savepath: ./runs/16022024-161301/
 Seed: 2026

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: test
 Savepath: ./runs/16022024-161301/
 Seed: 4052

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./runs/16022024-161301/adapt/
 Seed: 6078


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Data folder created successfuly: ./runs/16022024-161301/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 161303
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning training ... ===
    Number of examples in a batch: 4
    Number of train steps per epoch: 1
    Number of training epochs: 24000
    Total number of training steps: 24000

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Epoch:     0      LossTrajs: 15.27994728     ContextsNorm: 0.00000000
    Epoch:     1      LossTrajs: 9.44130898     ContextsNorm: 0.00098262
    Epoch:     2      LossTrajs: 5.22011900     ContextsNorm: 0.00194847
    Epoch:     3      LossTrajs: 2.49045634     ContextsNorm: 0.00296064
    Epoch:  1000      LossTrajs: 0.00353450     ContextsNorm: 0.09619335
    Epoch:  2000      LossTrajs: 0.00053514     ContextsNorm: 0.09187843
    Epoch:  3000      LossTrajs: 0.00070998     ContextsNorm: 0.09005751
    Epoch:  4000      LossTrajs: 0.00032432     ContextsNorm: 0.08885302
    Epoch:  5000      LossTrajs: 0.00021070     ContextsNorm: 0.08715699
    Epoch:  6000      LossTrajs: 0.00015432     ContextsNorm: 0.08459190
    Epoch:  7000      LossTrajs: 0.00008765     ContextsNorm: 0.08308978
    Epoch:  8000      LossTrajs: 0.00061289     ContextsNorm: 0.08053003
    Epoch:  9000      LossTrajs: 0.00005656     ContextsNorm: 0.08088054
    Epoch: 10000      LossTrajs: 0.00004330     ContextsNorm: 0.08121676
    Epoch: 11000      LossTrajs: 0.00003195     ContextsNorm: 0.08134604
    Epoch: 12000      LossTrajs: 0.00003078     ContextsNorm: 0.08111809
    Epoch: 13000      LossTrajs: 0.00003068     ContextsNorm: 0.08156078
    Epoch: 14000      LossTrajs: 0.00003130     ContextsNorm: 0.08137551
    Epoch: 15000      LossTrajs: 0.00003513     ContextsNorm: 0.08169217
    Epoch: 16000      LossTrajs: 0.00002117     ContextsNorm: 0.08200436
    Epoch: 17000      LossTrajs: 0.00003037     ContextsNorm: 0.08206413
    Epoch: 18000      LossTrajs: 0.00002131     ContextsNorm: 0.08206984
    Epoch: 19000      LossTrajs: 0.00002189     ContextsNorm: 0.08207414
    Epoch: 20000      LossTrajs: 0.00002400     ContextsNorm: 0.08191777
    Epoch: 21000      LossTrajs: 0.00001994     ContextsNorm: 0.08181912
    Epoch: 22000      LossTrajs: 0.00001993     ContextsNorm: 0.08174214
    Epoch: 23000      LossTrajs: 0.00001766     ContextsNorm: 0.08168891
    Epoch: 23999      LossTrajs: 0.00001671     ContextsNorm: 0.08164915

Total gradient descent training time: 0 hours 56 mins 3 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 170911
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 5.4946577e-05

==  Begining in-domain visualisation ... ==
    Environment id: 3
    Trajectory id: 16
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/16022024-161301/results_in_domain.png
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 24000
    Total number of training steps: 24000
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (4, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.07790217
    Epoch:     1     LossContext: 0.12881976
    Epoch:     2     LossContext: 0.12790342
    Epoch:     3     LossContext: 0.12649244
    Epoch:  1000     LossContext: 0.00018526
    Epoch:  2000     LossContext: 0.00011433
    Epoch:  3000     LossContext: 0.00009127
    Epoch:  4000     LossContext: 0.00007703
    Epoch:  5000     LossContext: 0.00006405
    Epoch:  6000     LossContext: 0.00005359
    Epoch:  7000     LossContext: 0.00004579
    Epoch:  8000     LossContext: 0.00003324
    Epoch:  9000     LossContext: 0.00003574
    Epoch: 10000     LossContext: 0.00004273
    Epoch: 11000     LossContext: 0.00003828
    Epoch: 12000     LossContext: 0.00003554
    Epoch: 13000     LossContext: 0.00003495
    Epoch: 14000     LossContext: 0.00003747
    Epoch: 15000     LossContext: 0.00003393
    Epoch: 16000     LossContext: 0.00003664
    Epoch: 17000     LossContext: 0.00003375
    Epoch: 18000     LossContext: 0.00003214
    Epoch: 19000     LossContext: 0.00003377
    Epoch: 20000     LossContext: 0.00003470
    Epoch: 21000     LossContext: 0.00003415
    Epoch: 22000     LossContext: 0.00003645
    Epoch: 23000     LossContext: 0.00003434
    Epoch: 23999     LossContext: 0.00003452

Total gradient descent adaptation time: 0 hours 11 mins 3 secs
Environment weights at the end of the adaptation: [0.1850527  0.2662211  0.29165196 0.25707415]

Saving adaptation parameters into ./runs/16022024-161301/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 2.6432572e-05

