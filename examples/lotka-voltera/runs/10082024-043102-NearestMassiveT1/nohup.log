
############# Neural Context Flow #############

Jax version: 0.4.28
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: train
 Savepath: ./runs/10082024-043102/
 Seed: 2026


############# Neural Context Flow #############

Jax version: 0.4.28
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: test
 Savepath: ./runs/10082024-043102/
 Seed: 4052


############# Neural Context Flow #############

Jax version: 0.4.28
Available devices: [cuda(id=0)]
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 043120
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 043120
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 35632 


WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed


=== Beginning training ... ===
    Number of examples in a batch: 4
    Number of train steps per epoch: 1
    Number of training epochs: 10000
    Total number of training steps: 10000

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Epoch:     0      LossTrajs: 1.81142581     ContextsNorm: 0.00000000     ValIndCrit: 1.67749763
        Saving best model so far ...
    Epoch:     1      LossTrajs: 1.77592278     ContextsNorm: 0.00029932     ValIndCrit: 1.64448416
        Saving best model so far ...
    Epoch:     2      LossTrajs: 1.74103713     ContextsNorm: 0.00058236     ValIndCrit: 1.61200368
        Saving best model so far ...
    Epoch:     3      LossTrajs: 1.70669496     ContextsNorm: 0.00086080     ValIndCrit: 1.58000064
        Saving best model so far ...
    Epoch:   100      LossTrajs: 0.24260327     ContextsNorm: 0.01331256     ValIndCrit: 0.28691807
        Saving best model so far ...
    Epoch:   200      LossTrajs: 0.14740019     ContextsNorm: 0.04107622     ValIndCrit: 0.15855432
        Saving best model so far ...
    Epoch:   300      LossTrajs: 0.06267649     ContextsNorm: 0.03841134     ValIndCrit: 0.06463621
        Saving best model so far ...
    Epoch:   400      LossTrajs: 0.02234200     ContextsNorm: 0.04579281     ValIndCrit: 0.02382136
        Saving best model so far ...
    Epoch:   500      LossTrajs: 0.01231917     ContextsNorm: 0.04828714     ValIndCrit: 0.01464449
        Saving best model so far ...
    Epoch:   600      LossTrajs: 0.00855944     ContextsNorm: 0.04890135     ValIndCrit: 0.01059901
        Saving best model so far ...
    Epoch:   700      LossTrajs: 0.00575788     ContextsNorm: 0.04915969     ValIndCrit: 0.00796375
        Saving best model so far ...
    Epoch:   800      LossTrajs: 0.00408922     ContextsNorm: 0.04979254     ValIndCrit: 0.00614356
        Saving best model so far ...
    Epoch:   900      LossTrajs: 0.00317003     ContextsNorm: 0.05053628     ValIndCrit: 0.00482817
        Saving best model so far ...
    Epoch:  1000      LossTrajs: 0.00249358     ContextsNorm: 0.05156665     ValIndCrit: 0.00387425
        Saving best model so far ...
    Epoch:  1100      LossTrajs: 0.00196054     ContextsNorm: 0.05295444     ValIndCrit: 0.00317329
        Saving best model so far ...
    Epoch:  1200      LossTrajs: 0.00159214     ContextsNorm: 0.05390979     ValIndCrit: 0.00268225
        Saving best model so far ...
    Epoch:  1300      LossTrajs: 0.00131492     ContextsNorm: 0.05420625     ValIndCrit: 0.00228784
        Saving best model so far ...
    Epoch:  1400      LossTrajs: 0.00109449     ContextsNorm: 0.05406440     ValIndCrit: 0.00193143
        Saving best model so far ...
    Epoch:  1500      LossTrajs: 0.00091164     ContextsNorm: 0.05366294     ValIndCrit: 0.00160990
        Saving best model so far ...
    Epoch:  1600      LossTrajs: 0.00076081     ContextsNorm: 0.05313442     ValIndCrit: 0.00134815
        Saving best model so far ...
    Epoch:  1700      LossTrajs: 0.00067588     ContextsNorm: 0.05252845     ValIndCrit: 0.00116697
        Saving best model so far ...
    Epoch:  1800      LossTrajs: 0.00055540     ContextsNorm: 0.05189666     ValIndCrit: 0.00101923
        Saving best model so far ...
    Epoch:  1900      LossTrajs: 0.00046389     ContextsNorm: 0.05120893     ValIndCrit: 0.00087495
        Saving best model so far ...
    Epoch:  2000      LossTrajs: 0.00038912     ContextsNorm: 0.05053820     ValIndCrit: 0.00072823
        Saving best model so far ...
    Epoch:  2100      LossTrajs: 0.00031874     ContextsNorm: 0.04982926     ValIndCrit: 0.00058408
        Saving best model so far ...
    Epoch:  2200      LossTrajs: 0.00025768     ContextsNorm: 0.04912106     ValIndCrit: 0.00045658
        Saving best model so far ...
    Epoch:  2300      LossTrajs: 0.00022678     ContextsNorm: 0.04834870     ValIndCrit: 0.00036998
        Saving best model so far ...
    Epoch:  2400      LossTrajs: 0.00019537     ContextsNorm: 0.04752030     ValIndCrit: 0.00030736
        Saving best model so far ...
    Epoch:  2500      LossTrajs: 0.00016205     ContextsNorm: 0.04662220     ValIndCrit: 0.00026650
        Saving best model so far ...
    Epoch:  2600      LossTrajs: 0.00015042     ContextsNorm: 0.04638498     ValIndCrit: 0.00023899
        Saving best model so far ...
    Epoch:  2700      LossTrajs: 0.00399463     ContextsNorm: 0.04848167     ValIndCrit: 0.00599576
    Epoch:  2800      LossTrajs: 0.00054401     ContextsNorm: 0.06014019     ValIndCrit: 0.00073106
    Epoch:  2900      LossTrajs: 0.00030293     ContextsNorm: 0.05977263     ValIndCrit: 0.00049887
    Epoch:  3000      LossTrajs: 0.00023726     ContextsNorm: 0.05937801     ValIndCrit: 0.00040366
    Epoch:  3100      LossTrajs: 0.00020116     ContextsNorm: 0.05908659     ValIndCrit: 0.00033974
    Epoch:  3200      LossTrajs: 0.00017701     ContextsNorm: 0.05883822     ValIndCrit: 0.00028838
    Epoch:  3300      LossTrajs: 0.00015876     ContextsNorm: 0.05859406     ValIndCrit: 0.00025344
    Epoch:  3400      LossTrajs: 0.00014516     ContextsNorm: 0.05835250     ValIndCrit: 0.00022463
        Saving best model so far ...
    Epoch:  3500      LossTrajs: 0.00013452     ContextsNorm: 0.05810141     ValIndCrit: 0.00020490
        Saving best model so far ...
    Epoch:  3600      LossTrajs: 0.00012576     ContextsNorm: 0.05785349     ValIndCrit: 0.00018870
        Saving best model so far ...
    Epoch:  3700      LossTrajs: 0.00011893     ContextsNorm: 0.05760055     ValIndCrit: 0.00017176
        Saving best model so far ...
    Epoch:  3800      LossTrajs: 0.00011469     ContextsNorm: 0.05732950     ValIndCrit: 0.00016678
        Saving best model so far ...
    Epoch:  3900      LossTrajs: 0.00010840     ContextsNorm: 0.05705950     ValIndCrit: 0.00015160
        Saving best model so far ...
    Epoch:  4000      LossTrajs: 0.00010753     ContextsNorm: 0.05677860     ValIndCrit: 0.00015327
    Epoch:  4100      LossTrajs: 0.00010209     ContextsNorm: 0.05648338     ValIndCrit: 0.00014165
        Saving best model so far ...
    Epoch:  4200      LossTrajs: 0.00009738     ContextsNorm: 0.05617015     ValIndCrit: 0.00013908
        Saving best model so far ...
    Epoch:  4300      LossTrajs: 0.00009681     ContextsNorm: 0.05577549     ValIndCrit: 0.00013282
        Saving best model so far ...
    Epoch:  4400      LossTrajs: 0.00009941     ContextsNorm: 0.05513043     ValIndCrit: 0.00013332
    Epoch:  4500      LossTrajs: 0.00242479     ContextsNorm: 0.05339357     ValIndCrit: 0.00409090
    Epoch:  4600      LossTrajs: 0.00039071     ContextsNorm: 0.06712891     ValIndCrit: 0.00123807
    Epoch:  4700      LossTrajs: 0.00028139     ContextsNorm: 0.06678736     ValIndCrit: 0.00097335
    Epoch:  4800      LossTrajs: 0.00022924     ContextsNorm: 0.06648479     ValIndCrit: 0.00076692
    Epoch:  4900      LossTrajs: 0.00019767     ContextsNorm: 0.06617099     ValIndCrit: 0.00062814
    Epoch:  5000      LossTrajs: 0.00017644     ContextsNorm: 0.06584135     ValIndCrit: 0.00053188
    Epoch:  5100      LossTrajs: 0.00022099     ContextsNorm: 0.06551377     ValIndCrit: 0.00048785
    Epoch:  5200      LossTrajs: 0.00014854     ContextsNorm: 0.06516855     ValIndCrit: 0.00039926
    Epoch:  5300      LossTrajs: 0.00014211     ContextsNorm: 0.06481313     ValIndCrit: 0.00036029
    Epoch:  5400      LossTrajs: 0.00013571     ContextsNorm: 0.06446400     ValIndCrit: 0.00033971
    Epoch:  5500      LossTrajs: 0.00013135     ContextsNorm: 0.06409744     ValIndCrit: 0.00029751
    Epoch:  5600      LossTrajs: 0.00012996     ContextsNorm: 0.06370682     ValIndCrit: 0.00026602
    Epoch:  5700      LossTrajs: 0.00011518     ContextsNorm: 0.06332002     ValIndCrit: 0.00023835
    Epoch:  5800      LossTrajs: 0.00011084     ContextsNorm: 0.06293298     ValIndCrit: 0.00021812
    Epoch:  5900      LossTrajs: 0.00010743     ContextsNorm: 0.06252636     ValIndCrit: 0.00020035
    Epoch:  6000      LossTrajs: 0.00010415     ContextsNorm: 0.06212674     ValIndCrit: 0.00018545
    Epoch:  6100      LossTrajs: 0.00010944     ContextsNorm: 0.06170781     ValIndCrit: 0.00019480
    Epoch:  6200      LossTrajs: 0.00009868     ContextsNorm: 0.06126024     ValIndCrit: 0.00016134
    Epoch:  6300      LossTrajs: 0.00009669     ContextsNorm: 0.06081012     ValIndCrit: 0.00015334
    Epoch:  6400      LossTrajs: 0.00009557     ContextsNorm: 0.06034027     ValIndCrit: 0.00014466
    Epoch:  6500      LossTrajs: 0.00009428     ContextsNorm: 0.05986734     ValIndCrit: 0.00013442
    Epoch:  6600      LossTrajs: 0.00009009     ContextsNorm: 0.05937270     ValIndCrit: 0.00012777
        Saving best model so far ...
    Epoch:  6700      LossTrajs: 0.00010794     ContextsNorm: 0.05884562     ValIndCrit: 0.00014217
    Epoch:  6800      LossTrajs: 0.00008704     ContextsNorm: 0.05822829     ValIndCrit: 0.00011371
        Saving best model so far ...
    Epoch:  6900      LossTrajs: 0.00008901     ContextsNorm: 0.05763451     ValIndCrit: 0.00011194
        Saving best model so far ...
    Epoch:  7000      LossTrajs: 0.00014666     ContextsNorm: 0.05702336     ValIndCrit: 0.00017665
    Epoch:  7100      LossTrajs: 0.00010704     ContextsNorm: 0.05631772     ValIndCrit: 0.00012843
    Epoch:  7200      LossTrajs: 0.00009316     ContextsNorm: 0.05543679     ValIndCrit: 0.00009905
        Saving best model so far ...
    Epoch:  7300      LossTrajs: 0.00007816     ContextsNorm: 0.05431799     ValIndCrit: 0.00008155
        Saving best model so far ...
    Epoch:  7400      LossTrajs: 0.00008640     ContextsNorm: 0.05289559     ValIndCrit: 0.00008344
    Epoch:  7500      LossTrajs: 0.00008986     ContextsNorm: 0.05133519     ValIndCrit: 0.00008346
    Epoch:  7600      LossTrajs: 0.00007377     ContextsNorm: 0.05012676     ValIndCrit: 0.00006877
        Saving best model so far ...
    Epoch:  7700      LossTrajs: 0.00007081     ContextsNorm: 0.04950430     ValIndCrit: 0.00006440
        Saving best model so far ...
    Epoch:  7800      LossTrajs: 0.00006849     ContextsNorm: 0.04900612     ValIndCrit: 0.00006542
    Epoch:  7900      LossTrajs: 0.00006594     ContextsNorm: 0.04852743     ValIndCrit: 0.00006723
    Epoch:  8000      LossTrajs: 0.00006444     ContextsNorm: 0.04807587     ValIndCrit: 0.00006802
    Epoch:  8100      LossTrajs: 0.00007828     ContextsNorm: 0.04775919     ValIndCrit: 0.00007923
    Epoch:  8200      LossTrajs: 0.00053520     ContextsNorm: 0.05185034     ValIndCrit: 0.00037034
    Epoch:  8300      LossTrajs: 0.00025444     ContextsNorm: 0.06444816     ValIndCrit: 0.00064803
    Epoch:  8400      LossTrajs: 0.00016978     ContextsNorm: 0.06568541     ValIndCrit: 0.00042814
    Epoch:  8500      LossTrajs: 0.00013008     ContextsNorm: 0.06522712     ValIndCrit: 0.00029595
    Epoch:  8600      LossTrajs: 0.00011264     ContextsNorm: 0.06463376     ValIndCrit: 0.00024355
    Epoch:  8700      LossTrajs: 0.00010696     ContextsNorm: 0.06406306     ValIndCrit: 0.00020505
    Epoch:  8800      LossTrajs: 0.00012241     ContextsNorm: 0.06347267     ValIndCrit: 0.00024058
    Epoch:  8900      LossTrajs: 0.00009326     ContextsNorm: 0.06289850     ValIndCrit: 0.00016276
    Epoch:  9000      LossTrajs: 0.00009678     ContextsNorm: 0.06231742     ValIndCrit: 0.00016231
    Epoch:  9100      LossTrajs: 0.00008682     ContextsNorm: 0.06173604     ValIndCrit: 0.00013735
    Epoch:  9200      LossTrajs: 0.00014242     ContextsNorm: 0.06114225     ValIndCrit: 0.00018741
    Epoch:  9300      LossTrajs: 0.00008620     ContextsNorm: 0.06052964     ValIndCrit: 0.00011881
    Epoch:  9400      LossTrajs: 0.00008613     ContextsNorm: 0.06001118     ValIndCrit: 0.00010969
    Epoch:  9500      LossTrajs: 0.00008277     ContextsNorm: 0.05964478     ValIndCrit: 0.00009731
    Epoch:  9600      LossTrajs: 0.00008030     ContextsNorm: 0.05944839     ValIndCrit: 0.00008560
    Epoch:  9700      LossTrajs: 0.00007936     ContextsNorm: 0.05918982     ValIndCrit: 0.00007761
    Epoch:  9800      LossTrajs: 0.00007745     ContextsNorm: 0.05880378     ValIndCrit: 0.00007284
    Epoch:  9900      LossTrajs: 0.00007870     ContextsNorm: 0.05824608     ValIndCrit: 0.00007189
    Epoch:  9999      LossTrajs: 0.00007754     ContextsNorm: 0.05757972     ValIndCrit: 0.00006938

Total gradient descent training time: 0 hours 46 mins 52 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 051815
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 6.9383976e-05


############# Neural Context Flow #############

Jax version: 0.4.28
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./runs/10082024-043102/adapt/
 Seed: 6078


############# Neural Context Flow #############

Jax version: 0.4.28
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt_test
 Savepath: ./runs/10082024-043102/adapt/
 Seed: 6078

==  Begining in-domain visualisation ... ==
    Environment id: 6
    Trajectory id: 7
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/10082024-043102/results_in_domain.png
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 4.49772215
    Epoch:     1     LossContext: 2.04708052
    Epoch:     2     LossContext: 0.97626120
    Epoch:     3     LossContext: 0.50781655
    Epoch:   100     LossContext: 0.00376245
    Epoch:   200     LossContext: 0.00006201
    Epoch:   300     LossContext: 0.00006197
    Epoch:   400     LossContext: 0.00006192
    Epoch:   500     LossContext: 0.00006188
    Epoch:   600     LossContext: 0.00006182
    Epoch:   700     LossContext: 0.00006176
    Epoch:   800     LossContext: 0.00006169
    Epoch:   900     LossContext: 0.00006161
    Epoch:  1000     LossContext: 0.00006153
    Epoch:  1100     LossContext: 0.00006144
    Epoch:  1200     LossContext: 0.00006135
    Epoch:  1300     LossContext: 0.00006125
    Epoch:  1400     LossContext: 0.00006114
    Epoch:  1499     LossContext: 0.00006102

Gradient descent adaptation time: 0 hours 0 mins 46 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 5.68970633
    Epoch:     1     LossContext: 2.94601345
    Epoch:     2     LossContext: 1.67074227
    Epoch:     3     LossContext: 1.06740344
    Epoch:   100     LossContext: 0.00802623
    Epoch:   200     LossContext: 0.00178908
    Epoch:   300     LossContext: 0.00049454
    Epoch:   400     LossContext: 0.00021939
    Epoch:   500     LossContext: 0.00016729
    Epoch:   600     LossContext: 0.00015901
    Epoch:   700     LossContext: 0.00015791
    Epoch:   800     LossContext: 0.00015774
    Epoch:   900     LossContext: 0.00015765
    Epoch:  1000     LossContext: 0.00015757
    Epoch:  1100     LossContext: 0.00015748
    Epoch:  1200     LossContext: 0.00015738
    Epoch:  1300     LossContext: 0.00015727
    Epoch:  1400     LossContext: 0.00015716
    Epoch:  1499     LossContext: 0.00015704

Gradient descent adaptation time: 0 hours 0 mins 42 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 5.11623335
    Epoch:     1     LossContext: 2.46966720
    Epoch:     2     LossContext: 1.26762664
    Epoch:     3     LossContext: 0.71596152
    Epoch:   100     LossContext: 0.00676030
    Epoch:   200     LossContext: 0.00204575
    Epoch:   300     LossContext: 0.00064917
    Epoch:   400     LossContext: 0.00024420
    Epoch:   500     LossContext: 0.00013756
    Epoch:   600     LossContext: 0.00011277
    Epoch:   700     LossContext: 0.00010776
    Epoch:   800     LossContext: 0.00010686
    Epoch:   900     LossContext: 0.00010666
    Epoch:  1000     LossContext: 0.00010656
    Epoch:  1100     LossContext: 0.00010647
    Epoch:  1200     LossContext: 0.00010636
    Epoch:  1300     LossContext: 0.00010625
    Epoch:  1400     LossContext: 0.00010613
    Epoch:  1499     LossContext: 0.00010601

Gradient descent adaptation time: 0 hours 0 mins 42 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 5.50385189
    Epoch:     1     LossContext: 2.75270605
    Epoch:     2     LossContext: 1.47762740
    Epoch:     3     LossContext: 0.87737179
    Epoch:   100     LossContext: 0.00623870
    Epoch:   200     LossContext: 0.00341415
    Epoch:   300     LossContext: 0.00189878
    Epoch:   400     LossContext: 0.00109704
    Epoch:   500     LossContext: 0.00066126
    Epoch:   600     LossContext: 0.00041939
    Epoch:   700     LossContext: 0.00028394
    Epoch:   800     LossContext: 0.00020821
    Epoch:   900     LossContext: 0.00016631
    Epoch:  1000     LossContext: 0.00014354
    Epoch:  1100     LossContext: 0.00013145
    Epoch:  1200     LossContext: 0.00012519
    Epoch:  1300     LossContext: 0.00012203
    Epoch:  1400     LossContext: 0.00012046
    Epoch:  1499     LossContext: 0.00011966

Gradient descent adaptation time: 0 hours 0 mins 42 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./runs/10082024-043102/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 0.00010234196

