
############# Neural Context Flow #############

Jax version: 0.4.28
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: train
 Savepath: ./runs/10082024-101628/
 Seed: 2026


############# Neural Context Flow #############

Jax version: 0.4.28
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: test
 Savepath: ./runs/10082024-101628/
 Seed: 4052


############# Neural Context Flow #############

Jax version: 0.4.28
Available devices: [cuda(id=0)]
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 101646
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 101646
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 102380 


WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed


=== Beginning training ... ===
    Number of examples in a batch: 4
    Number of train steps per epoch: 1
    Number of training epochs: 10000
    Total number of training steps: 10000

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 2) (20,)
    Epoch:     0      LossTrajs: 1.47047102     ContextsNorm: 0.00000000     ValIndCrit: 1.07546484
        Saving best model so far ...
    Epoch:     1      LossTrajs: 1.13432264     ContextsNorm: 0.00030000     ValIndCrit: 0.84414613
        Saving best model so far ...
    Epoch:     2      LossTrajs: 0.88184178     ContextsNorm: 0.00059584     ValIndCrit: 0.67118043
        Saving best model so far ...
    Epoch:     3      LossTrajs: 0.69197190     ContextsNorm: 0.00088504     ValIndCrit: 0.54260027
        Saving best model so far ...
    Epoch:   100      LossTrajs: 0.08560498     ContextsNorm: 0.01318731     ValIndCrit: 0.08962008
        Saving best model so far ...
    Epoch:   200      LossTrajs: 0.06937917     ContextsNorm: 0.05063277     ValIndCrit: 0.07261150
        Saving best model so far ...
    Epoch:   300      LossTrajs: 0.05331540     ContextsNorm: 0.08482144     ValIndCrit: 0.05576381
        Saving best model so far ...
    Epoch:   400      LossTrajs: 0.03118677     ContextsNorm: 0.10181432     ValIndCrit: 0.03311326
        Saving best model so far ...
    Epoch:   500      LossTrajs: 0.02625284     ContextsNorm: 0.09917887     ValIndCrit: 0.02866347
        Saving best model so far ...
    Epoch:   600      LossTrajs: 0.02125135     ContextsNorm: 0.09644625     ValIndCrit: 0.02426811
        Saving best model so far ...
    Epoch:   700      LossTrajs: 0.01637308     ContextsNorm: 0.09550042     ValIndCrit: 0.01941328
        Saving best model so far ...
    Epoch:   800      LossTrajs: 0.01076860     ContextsNorm: 0.09585438     ValIndCrit: 0.01352161
        Saving best model so far ...
    Epoch:   900      LossTrajs: 0.00727034     ContextsNorm: 0.09758679     ValIndCrit: 0.01004065
        Saving best model so far ...
    Epoch:  1000      LossTrajs: 0.00583649     ContextsNorm: 0.09948463     ValIndCrit: 0.00902391
        Saving best model so far ...
    Epoch:  1100      LossTrajs: 0.00470642     ContextsNorm: 0.10040752     ValIndCrit: 0.00777522
        Saving best model so far ...
    Epoch:  1200      LossTrajs: 0.00392691     ContextsNorm: 0.10089896     ValIndCrit: 0.00761576
        Saving best model so far ...
    Epoch:  1300      LossTrajs: 0.00348930     ContextsNorm: 0.10144017     ValIndCrit: 0.00730981
        Saving best model so far ...
    Epoch:  1400      LossTrajs: 0.00323670     ContextsNorm: 0.10207988     ValIndCrit: 0.00724682
        Saving best model so far ...
    Epoch:  1500      LossTrajs: 0.00307032     ContextsNorm: 0.10267576     ValIndCrit: 0.00718820
        Saving best model so far ...
    Epoch:  1600      LossTrajs: 0.00288933     ContextsNorm: 0.10325053     ValIndCrit: 0.00664526
        Saving best model so far ...
    Epoch:  1700      LossTrajs: 0.00273121     ContextsNorm: 0.10387430     ValIndCrit: 0.00620859
        Saving best model so far ...
    Epoch:  1800      LossTrajs: 0.00257417     ContextsNorm: 0.10461944     ValIndCrit: 0.00577705
        Saving best model so far ...
    Epoch:  1900      LossTrajs: 0.00238566     ContextsNorm: 0.10552469     ValIndCrit: 0.00563210
        Saving best model so far ...
    Epoch:  2000      LossTrajs: 0.00222430     ContextsNorm: 0.10651781     ValIndCrit: 0.00467736
        Saving best model so far ...
    Epoch:  2100      LossTrajs: 0.00194718     ContextsNorm: 0.10742304     ValIndCrit: 0.00429575
        Saving best model so far ...
    Epoch:  2200      LossTrajs: 0.00174677     ContextsNorm: 0.10823456     ValIndCrit: 0.00375949
        Saving best model so far ...
    Epoch:  2300      LossTrajs: 0.00156380     ContextsNorm: 0.10897498     ValIndCrit: 0.00317261
        Saving best model so far ...
    Epoch:  2400      LossTrajs: 0.00137313     ContextsNorm: 0.10931335     ValIndCrit: 0.00264753
        Saving best model so far ...
    Epoch:  2500      LossTrajs: 0.00120824     ContextsNorm: 0.10971720     ValIndCrit: 0.00221613
        Saving best model so far ...
    Epoch:  2600      LossTrajs: 0.00106193     ContextsNorm: 0.11007816     ValIndCrit: 0.00196686
        Saving best model so far ...
    Epoch:  2700      LossTrajs: 0.00096008     ContextsNorm: 0.11045842     ValIndCrit: 0.00159904
        Saving best model so far ...
    Epoch:  2800      LossTrajs: 0.00100177     ContextsNorm: 0.11097149     ValIndCrit: 0.00179128
    Epoch:  2900      LossTrajs: 0.00068041     ContextsNorm: 0.11152633     ValIndCrit: 0.00120618
        Saving best model so far ...
    Epoch:  3000      LossTrajs: 0.00065399     ContextsNorm: 0.11203673     ValIndCrit: 0.00090662
        Saving best model so far ...
    Epoch:  3100      LossTrajs: 0.00074086     ContextsNorm: 0.11244591     ValIndCrit: 0.00105052
    Epoch:  3200      LossTrajs: 0.00108148     ContextsNorm: 0.11296327     ValIndCrit: 0.00125943
    Epoch:  3300      LossTrajs: 0.00054309     ContextsNorm: 0.11350719     ValIndCrit: 0.00090628
        Saving best model so far ...
    Epoch:  3400      LossTrajs: 0.00049011     ContextsNorm: 0.11373511     ValIndCrit: 0.00065826
        Saving best model so far ...
    Epoch:  3500      LossTrajs: 0.00043130     ContextsNorm: 0.11375280     ValIndCrit: 0.00060120
        Saving best model so far ...
    Epoch:  3600      LossTrajs: 0.00035563     ContextsNorm: 0.11361740     ValIndCrit: 0.00051895
        Saving best model so far ...
    Epoch:  3700      LossTrajs: 0.00035298     ContextsNorm: 0.11346061     ValIndCrit: 0.00051127
        Saving best model so far ...
    Epoch:  3800      LossTrajs: 0.00035110     ContextsNorm: 0.11317803     ValIndCrit: 0.00049730
        Saving best model so far ...
    Epoch:  3900      LossTrajs: 0.00034093     ContextsNorm: 0.11289401     ValIndCrit: 0.00048571
        Saving best model so far ...
    Epoch:  4000      LossTrajs: 0.00039514     ContextsNorm: 0.11253687     ValIndCrit: 0.00049515
    Epoch:  4100      LossTrajs: 0.00037222     ContextsNorm: 0.11223902     ValIndCrit: 0.00047785
        Saving best model so far ...
    Epoch:  4200      LossTrajs: 0.00040788     ContextsNorm: 0.11188771     ValIndCrit: 0.00068064
    Epoch:  4300      LossTrajs: 0.00042638     ContextsNorm: 0.11148504     ValIndCrit: 0.00077021
    Epoch:  4400      LossTrajs: 0.00033354     ContextsNorm: 0.11124115     ValIndCrit: 0.00040363
        Saving best model so far ...
    Epoch:  4500      LossTrajs: 0.00030246     ContextsNorm: 0.11085816     ValIndCrit: 0.00044517
    Epoch:  4600      LossTrajs: 0.00028709     ContextsNorm: 0.11052141     ValIndCrit: 0.00040266
        Saving best model so far ...
    Epoch:  4700      LossTrajs: 0.00028261     ContextsNorm: 0.11014009     ValIndCrit: 0.00038949
        Saving best model so far ...
    Epoch:  4800      LossTrajs: 0.00047783     ContextsNorm: 0.10977551     ValIndCrit: 0.00062962
    Epoch:  4900      LossTrajs: 0.00027527     ContextsNorm: 0.10948300     ValIndCrit: 0.00040623
    Epoch:  5000      LossTrajs: 0.00026728     ContextsNorm: 0.10914794     ValIndCrit: 0.00037952
        Saving best model so far ...
    Epoch:  5100      LossTrajs: 0.00039830     ContextsNorm: 0.10873675     ValIndCrit: 0.00059752
    Epoch:  5200      LossTrajs: 0.00034239     ContextsNorm: 0.10843182     ValIndCrit: 0.00046836
    Epoch:  5300      LossTrajs: 0.00026311     ContextsNorm: 0.10814584     ValIndCrit: 0.00035092
        Saving best model so far ...
    Epoch:  5400      LossTrajs: 0.00026454     ContextsNorm: 0.10779177     ValIndCrit: 0.00034556
        Saving best model so far ...
    Epoch:  5500      LossTrajs: 0.00025390     ContextsNorm: 0.10746238     ValIndCrit: 0.00037194
    Epoch:  5600      LossTrajs: 0.00030662     ContextsNorm: 0.10718985     ValIndCrit: 0.00035268
    Epoch:  5700      LossTrajs: 0.00024845     ContextsNorm: 0.10686973     ValIndCrit: 0.00037603
    Epoch:  5800      LossTrajs: 0.00045244     ContextsNorm: 0.10646468     ValIndCrit: 0.00064812
    Epoch:  5900      LossTrajs: 0.00025491     ContextsNorm: 0.10616124     ValIndCrit: 0.00036467
    Epoch:  6000      LossTrajs: 0.00027363     ContextsNorm: 0.10584404     ValIndCrit: 0.00036327
    Epoch:  6100      LossTrajs: 0.00035765     ContextsNorm: 0.10558789     ValIndCrit: 0.00049110
    Epoch:  6200      LossTrajs: 0.00028736     ContextsNorm: 0.10523790     ValIndCrit: 0.00039841
    Epoch:  6300      LossTrajs: 0.00024658     ContextsNorm: 0.10488290     ValIndCrit: 0.00032801
        Saving best model so far ...
    Epoch:  6400      LossTrajs: 0.00022783     ContextsNorm: 0.10466136     ValIndCrit: 0.00028829
        Saving best model so far ...
    Epoch:  6500      LossTrajs: 0.00043598     ContextsNorm: 0.10427292     ValIndCrit: 0.00062085
    Epoch:  6600      LossTrajs: 0.00022189     ContextsNorm: 0.10399261     ValIndCrit: 0.00029948
    Epoch:  6700      LossTrajs: 0.00031989     ContextsNorm: 0.10369216     ValIndCrit: 0.00037104
    Epoch:  6800      LossTrajs: 0.00022057     ContextsNorm: 0.10337255     ValIndCrit: 0.00030348
    Epoch:  6900      LossTrajs: 0.00021482     ContextsNorm: 0.10307296     ValIndCrit: 0.00028469
        Saving best model so far ...
    Epoch:  7000      LossTrajs: 0.00021594     ContextsNorm: 0.10280437     ValIndCrit: 0.00026285
        Saving best model so far ...
    Epoch:  7100      LossTrajs: 0.00021899     ContextsNorm: 0.10247418     ValIndCrit: 0.00025122
        Saving best model so far ...
    Epoch:  7200      LossTrajs: 0.00022725     ContextsNorm: 0.10218117     ValIndCrit: 0.00029104
    Epoch:  7300      LossTrajs: 0.00020912     ContextsNorm: 0.10188391     ValIndCrit: 0.00025802
    Epoch:  7400      LossTrajs: 0.00021880     ContextsNorm: 0.10161813     ValIndCrit: 0.00031332
    Epoch:  7500      LossTrajs: 0.00022466     ContextsNorm: 0.10128293     ValIndCrit: 0.00026684
    Epoch:  7600      LossTrajs: 0.00021487     ContextsNorm: 0.10094886     ValIndCrit: 0.00023435
        Saving best model so far ...
    Epoch:  7700      LossTrajs: 0.00020436     ContextsNorm: 0.10072807     ValIndCrit: 0.00025935
    Epoch:  7800      LossTrajs: 0.00019702     ContextsNorm: 0.10037880     ValIndCrit: 0.00023767
    Epoch:  7900      LossTrajs: 0.00026520     ContextsNorm: 0.10006958     ValIndCrit: 0.00023386
        Saving best model so far ...
    Epoch:  8000      LossTrajs: 0.00020061     ContextsNorm: 0.09980992     ValIndCrit: 0.00026359
    Epoch:  8100      LossTrajs: 0.00019639     ContextsNorm: 0.09949356     ValIndCrit: 0.00024366
    Epoch:  8200      LossTrajs: 0.00022883     ContextsNorm: 0.09926629     ValIndCrit: 0.00022908
        Saving best model so far ...
    Epoch:  8300      LossTrajs: 0.00020331     ContextsNorm: 0.09891465     ValIndCrit: 0.00020767
        Saving best model so far ...
    Epoch:  8400      LossTrajs: 0.00033889     ContextsNorm: 0.09862959     ValIndCrit: 0.00039706
    Epoch:  8500      LossTrajs: 0.00027041     ContextsNorm: 0.09836581     ValIndCrit: 0.00024356
    Epoch:  8600      LossTrajs: 0.00030539     ContextsNorm: 0.09808792     ValIndCrit: 0.00025031
    Epoch:  8700      LossTrajs: 0.00019786     ContextsNorm: 0.09779093     ValIndCrit: 0.00021370
    Epoch:  8800      LossTrajs: 0.00023030     ContextsNorm: 0.09751581     ValIndCrit: 0.00022517
    Epoch:  8900      LossTrajs: 0.00018717     ContextsNorm: 0.09724787     ValIndCrit: 0.00028103
    Epoch:  9000      LossTrajs: 0.00018477     ContextsNorm: 0.09695636     ValIndCrit: 0.00020746
        Saving best model so far ...
    Epoch:  9100      LossTrajs: 0.00022620     ContextsNorm: 0.09671948     ValIndCrit: 0.00018647
        Saving best model so far ...
    Epoch:  9200      LossTrajs: 0.00019010     ContextsNorm: 0.09634487     ValIndCrit: 0.00022535
    Epoch:  9300      LossTrajs: 0.00018111     ContextsNorm: 0.09605036     ValIndCrit: 0.00018254
        Saving best model so far ...
    Epoch:  9400      LossTrajs: 0.00017919     ContextsNorm: 0.09577340     ValIndCrit: 0.00022606
    Epoch:  9500      LossTrajs: 0.00025937     ContextsNorm: 0.09552208     ValIndCrit: 0.00033398
    Epoch:  9600      LossTrajs: 0.00024980     ContextsNorm: 0.09525082     ValIndCrit: 0.00022660
    Epoch:  9700      LossTrajs: 0.00026165     ContextsNorm: 0.09494888     ValIndCrit: 0.00023347
    Epoch:  9800      LossTrajs: 0.00022809     ContextsNorm: 0.09466169     ValIndCrit: 0.00028906
    Epoch:  9900      LossTrajs: 0.00016471     ContextsNorm: 0.09432578     ValIndCrit: 0.00017800
        Saving best model so far ...
    Epoch:  9999      LossTrajs: 0.00016630     ContextsNorm: 0.09416755     ValIndCrit: 0.00017764
        Saving best model so far ...

Total gradient descent training time: 0 hours 30 mins 29 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 104718
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 0.00017763743


############# Neural Context Flow #############

Jax version: 0.4.28
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./runs/10082024-101628/adapt/
 Seed: 6078


############# Neural Context Flow #############

Jax version: 0.4.28
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt_test
 Savepath: ./runs/10082024-101628/adapt/
 Seed: 6078

==  Begining in-domain visualisation ... ==
    Environment id: 4
    Trajectory id: 13
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/10082024-101628/results_in_domain.png
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 20, 2) (20,)
    Epoch:     0     LossContext: 0.01716965
    Epoch:     1     LossContext: 0.01521360
    Epoch:     2     LossContext: 0.01322358
    Epoch:     3     LossContext: 0.01122466
    Epoch:   100     LossContext: 0.00010789
    Epoch:   200     LossContext: 0.00010758
    Epoch:   300     LossContext: 0.00010758
    Epoch:   400     LossContext: 0.00010758
    Epoch:   500     LossContext: 0.00010758
    Epoch:   600     LossContext: 0.00010758
    Epoch:   700     LossContext: 0.00010758
    Epoch:   800     LossContext: 0.00010758
    Epoch:   900     LossContext: 0.00010758
    Epoch:  1000     LossContext: 0.00010758
    Epoch:  1100     LossContext: 0.00010758
    Epoch:  1200     LossContext: 0.00010758
    Epoch:  1300     LossContext: 0.00010758
    Epoch:  1400     LossContext: 0.00010758
    Epoch:  1499     LossContext: 0.00010758

Gradient descent adaptation time: 0 hours 0 mins 32 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.11380060
    Epoch:     1     LossContext: 0.10551471
    Epoch:     2     LossContext: 0.09686011
    Epoch:     3     LossContext: 0.08787935
    Epoch:   100     LossContext: 0.00045553
    Epoch:   200     LossContext: 0.00026250
    Epoch:   300     LossContext: 0.00026205
    Epoch:   400     LossContext: 0.00026205
    Epoch:   500     LossContext: 0.00026205
    Epoch:   600     LossContext: 0.00026205
    Epoch:   700     LossContext: 0.00026205
    Epoch:   800     LossContext: 0.00026205
    Epoch:   900     LossContext: 0.00026205
    Epoch:  1000     LossContext: 0.00026205
    Epoch:  1100     LossContext: 0.00026205
    Epoch:  1200     LossContext: 0.00026205
    Epoch:  1300     LossContext: 0.00026205
    Epoch:  1400     LossContext: 0.00026205
    Epoch:  1499     LossContext: 0.00026205

Gradient descent adaptation time: 0 hours 0 mins 29 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.05626295
    Epoch:     1     LossContext: 0.05059370
    Epoch:     2     LossContext: 0.04522252
    Epoch:     3     LossContext: 0.04018679
    Epoch:   100     LossContext: 0.00022007
    Epoch:   200     LossContext: 0.00021879
    Epoch:   300     LossContext: 0.00021879
    Epoch:   400     LossContext: 0.00021879
    Epoch:   500     LossContext: 0.00021879
    Epoch:   600     LossContext: 0.00021879
    Epoch:   700     LossContext: 0.00021879
    Epoch:   800     LossContext: 0.00021879
    Epoch:   900     LossContext: 0.00021879
    Epoch:  1000     LossContext: 0.00021879
    Epoch:  1100     LossContext: 0.00021879
    Epoch:  1200     LossContext: 0.00021879
    Epoch:  1300     LossContext: 0.00021879
    Epoch:  1400     LossContext: 0.00021879
    Epoch:  1499     LossContext: 0.00021879

Gradient descent adaptation time: 0 hours 0 mins 30 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.03378954
    Epoch:     1     LossContext: 0.03143114
    Epoch:     2     LossContext: 0.02915197
    Epoch:     3     LossContext: 0.02695642
    Epoch:   100     LossContext: 0.00045562
    Epoch:   200     LossContext: 0.00044170
    Epoch:   300     LossContext: 0.00044169
    Epoch:   400     LossContext: 0.00044169
    Epoch:   500     LossContext: 0.00044169
    Epoch:   600     LossContext: 0.00044169
    Epoch:   700     LossContext: 0.00044169
    Epoch:   800     LossContext: 0.00044169
    Epoch:   900     LossContext: 0.00044169
    Epoch:  1000     LossContext: 0.00044169
    Epoch:  1100     LossContext: 0.00044169
    Epoch:  1200     LossContext: 0.00044169
    Epoch:  1300     LossContext: 0.00044169
    Epoch:  1400     LossContext: 0.00044169
    Epoch:  1499     LossContext: 0.00044169

Gradient descent adaptation time: 0 hours 0 mins 30 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./runs/10082024-101628/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 0.00022198363

