
############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: train
 Savepath: ./runs/27032024-085635/
 Seed: 2026


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: test
 Savepath: ./runs/27032024-085635/
 Seed: 4052


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Run folder created successfuly: ./runs/27032024-085635/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 085637
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 085638
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 313693 


WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed


=== Beginning training with proximal alternating minimization ... ===
    Number of examples in a batch: 32
    Maximum number of steps per inner minimization: 10
    Maximum number of outer minimizations: 500
    Maximum total number of training steps: 5000

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 32, 20, 7) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 32, 20, 7) (20,)
    Outer Step:     0      LossTrajs: 0.32579279     ContextsNorm: 0.00000000     ValIndCrit: 0.27309445
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 1.81e-04
        -DiffCxt:  inf
    Outer Step:     1      LossTrajs: 0.29871950     ContextsNorm: 0.00003855     ValIndCrit: 0.25318709
        -NbInnerStepsNode:   10
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 2.30e-04
        -DiffCxt:  3.19e-03
    Outer Step:     2      LossTrajs: 0.21513705     ContextsNorm: 0.00001005     ValIndCrit: 0.22813150
        -NbInnerStepsNode:   10
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 9.20e-04
        -DiffCxt:  2.26e-02
    Outer Step:     3      LossTrajs: 0.20755473     ContextsNorm: 0.00001165     ValIndCrit: 0.22536437
        -NbInnerStepsNode:   10
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 2.38e-04
        -DiffCxt:  3.76e-03
    Outer Step:    10      LossTrajs: 0.11584342     ContextsNorm: 0.00010997     ValIndCrit: 0.11906181
        -NbInnerStepsNode:   10
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 1.07e-04
        -DiffCxt:  5.48e-04
    Outer Step:    20      LossTrajs: 0.02513273     ContextsNorm: 0.00011194     ValIndCrit: 0.03116853
        -NbInnerStepsNode:   10
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 6.90e-06
        -DiffCxt:  5.59e-06
    Outer Step:    30      LossTrajs: 0.01173239     ContextsNorm: 0.00011721     ValIndCrit: 0.01756859
        -NbInnerStepsNode:   10
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 6.41e-06
        -DiffCxt:  2.69e-04
    Outer Step:    40      LossTrajs: 0.00714010     ContextsNorm: 0.00010585     ValIndCrit: 0.01326736
        -NbInnerStepsNode:   10
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 3.86e-06
        -DiffCxt:  9.44e-05
    Outer Step:    50      LossTrajs: 0.00591082     ContextsNorm: 0.00012596     ValIndCrit: 0.01168265
        -NbInnerStepsNode:   10
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 9.63e-06
        -DiffCxt:  1.32e-04
    Outer Step:    60      LossTrajs: 0.00464370     ContextsNorm: 0.00015152     ValIndCrit: 0.01046289
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 3.21e-07
        -DiffCxt:  9.23e-06
    Outer Step:    70      LossTrajs: 0.00450974     ContextsNorm: 0.00015044     ValIndCrit: 0.01024808
        -NbInnerStepsNode:   10
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 4.76e-06
        -DiffCxt:  1.50e-05
    Outer Step:    80      LossTrajs: 0.00373605     ContextsNorm: 0.00016456     ValIndCrit: 0.00909304
        -NbInnerStepsNode:   10
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 5.68e-06
        -DiffCxt:  2.27e-05
    Outer Step:    90      LossTrajs: 0.00327724     ContextsNorm: 0.00017577     ValIndCrit: 0.00872641
        -NbInnerStepsNode:   10
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 1.16e-06
        -DiffCxt:  1.06e-05
    Outer Step:   100      LossTrajs: 0.00339198     ContextsNorm: 0.00018225     ValIndCrit: 0.00873854
        -NbInnerStepsNode:   10
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 1.48e-05
        -DiffCxt:  1.11e-05
    Outer Step:   110      LossTrajs: 0.00290254     ContextsNorm: 0.00021673     ValIndCrit: 0.00832569
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 2.02e-07
        -DiffCxt:  1.47e-06
    Outer Step:   120      LossTrajs: 0.00287265     ContextsNorm: 0.00021967     ValIndCrit: 0.00829066
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    5
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 3.69e-07
        -DiffCxt:  1.64e-06
    Outer Step:   130      LossTrajs: 0.00283259     ContextsNorm: 0.00022760     ValIndCrit: 0.00823859
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    6
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 1.98e-07
        -DiffCxt:  1.89e-06
    Outer Step:   140      LossTrajs: 0.00276654     ContextsNorm: 0.00023002     ValIndCrit: 0.00819966
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 3.78e-07
        -DiffCxt:  1.90e-05
    Outer Step:   150      LossTrajs: 0.00260705     ContextsNorm: 0.00029331     ValIndCrit: 0.00811943
        -NbInnerStepsNode:   10
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 4.69e-07
        -DiffCxt:  7.06e-05
    Outer Step:   160      LossTrajs: 0.00247166     ContextsNorm: 0.00029629     ValIndCrit: 0.00787617
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 3.83e-07
        -DiffCxt:  9.80e-07
    Outer Step:   170      LossTrajs: 0.01111474     ContextsNorm: 0.00028790     ValIndCrit: 0.01367186
        -NbInnerStepsNode:   10
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 1.32e-04
        -DiffCxt:  1.95e-04
    Outer Step:   180      LossTrajs: 0.00247694     ContextsNorm: 0.00028637     ValIndCrit: 0.00791741
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 2.07e-07
        -DiffCxt:  1.80e-06
    Outer Step:   190      LossTrajs: 0.00232886     ContextsNorm: 0.00030002     ValIndCrit: 0.00773987
        -NbInnerStepsNode:    5
        -NbInnerStepsCxt:    7
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 3.88e-07
        -DiffCxt:  1.95e-06
    Outer Step:   200      LossTrajs: 0.00227898     ContextsNorm: 0.00030180     ValIndCrit: 0.00766542
        -NbInnerStepsNode:    2
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 3.66e-07
        -DiffCxt:  5.33e-06
    Outer Step:   210      LossTrajs: 0.00223475     ContextsNorm: 0.00033949     ValIndCrit: 0.00755790
        -NbInnerStepsNode:   10
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 1.05e-06
        -DiffCxt:  4.40e-06
    Outer Step:   220      LossTrajs: 0.00214961     ContextsNorm: 0.00035088     ValIndCrit: 0.00750744
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    2
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 3.50e-07
        -DiffCxt:  1.46e-06
    Outer Step:   230      LossTrajs: 0.00242018     ContextsNorm: 0.00038214     ValIndCrit: 0.00815026
        -NbInnerStepsNode:   10
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 8.73e-06
        -DiffCxt:  2.49e-05
    Outer Step:   240      LossTrajs: 0.00205349     ContextsNorm: 0.00038083     ValIndCrit: 0.00746640
        -NbInnerStepsNode:    6
        -NbInnerStepsCxt:    6
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 3.41e-07
        -DiffCxt:  1.80e-06
    Outer Step:   250      LossTrajs: 0.00209162     ContextsNorm: 0.00038718     ValIndCrit: 0.00700814
        -NbInnerStepsNode:   10
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 5.29e-06
        -DiffCxt:  5.58e-05
    Outer Step:   260      LossTrajs: 0.00195541     ContextsNorm: 0.00047539     ValIndCrit: 0.00738906
        -NbInnerStepsNode:   10
        -NbInnerStepsCxt:    6
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 7.73e-07
        -DiffCxt:  1.86e-06
    Outer Step:   270      LossTrajs: 0.00191294     ContextsNorm: 0.00047456     ValIndCrit: 0.00738205
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 3.67e-07
        -DiffCxt:  1.87e-06
    Outer Step:   280      LossTrajs: 0.00189494     ContextsNorm: 0.00048401     ValIndCrit: 0.00730683
        -NbInnerStepsNode:    6
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 3.99e-07
        -DiffCxt:  3.73e-06
    Outer Step:   290      LossTrajs: 0.00185453     ContextsNorm: 0.00045450     ValIndCrit: 0.00693129
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    4
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 2.49e-07
        -DiffCxt:  1.69e-06
    Outer Step:   300      LossTrajs: 0.00220785     ContextsNorm: 0.00057554     ValIndCrit: 0.00783580
        -NbInnerStepsNode:   10
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 5.77e-06
        -DiffCxt:  1.33e-05
    Outer Step:   310      LossTrajs: 0.00188841     ContextsNorm: 0.00059790     ValIndCrit: 0.00730690
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    2
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 3.21e-07
        -DiffCxt:  1.70e-06
    Outer Step:   320      LossTrajs: 0.00188259     ContextsNorm: 0.00062406     ValIndCrit: 0.00688857
        -NbInnerStepsNode:   10
        -NbInnerStepsCxt:    6
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 3.58e-06
        -DiffCxt:  1.93e-06
    Outer Step:   330      LossTrajs: 0.00175299     ContextsNorm: 0.00064169     ValIndCrit: 0.00683066
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 1.99e-07
        -DiffCxt:  1.76e-06
    Outer Step:   340      LossTrajs: 0.00170630     ContextsNorm: 0.00059817     ValIndCrit: 0.00686155
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    2
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 2.07e-07
        -DiffCxt:  6.57e-07
    Outer Step:   350      LossTrajs: 0.00170769     ContextsNorm: 0.00059068     ValIndCrit: 0.00689303
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 3.83e-07
        -DiffCxt:  4.52e-06
    Outer Step:   360      LossTrajs: 0.00165492     ContextsNorm: 0.00054197     ValIndCrit: 0.00673090
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 2.92e-07
        -DiffCxt:  1.05e-05
    Outer Step:   370      LossTrajs: 0.00187794     ContextsNorm: 0.00053682     ValIndCrit: 0.00674514
        -NbInnerStepsNode:   10
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 7.52e-06
        -DiffCxt:  5.95e-06
    Outer Step:   380      LossTrajs: 0.00166205     ContextsNorm: 0.00052342     ValIndCrit: 0.00679851
        -NbInnerStepsNode:   10
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 5.28e-07
        -DiffCxt:  2.31e-06
    Outer Step:   390      LossTrajs: 0.00157187     ContextsNorm: 0.00051096     ValIndCrit: 0.00662133
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    9
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 3.79e-07
        -DiffCxt:  1.99e-06
    Outer Step:   400      LossTrajs: 0.00151520     ContextsNorm: 0.00054355     ValIndCrit: 0.00640510
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 2.70e-07
        -DiffCxt:  8.17e-06
    Outer Step:   410      LossTrajs: 0.00146814     ContextsNorm: 0.00053767     ValIndCrit: 0.00652167
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    5
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 3.35e-07
        -DiffCxt:  1.90e-06
    Outer Step:   420      LossTrajs: 0.00148505     ContextsNorm: 0.00056612     ValIndCrit: 0.00616367
        -NbInnerStepsNode:   10
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 3.63e-07
        -DiffCxt:  1.21e-06
    Outer Step:   430      LossTrajs: 0.00143497     ContextsNorm: 0.00057557     ValIndCrit: 0.00651469
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:   10
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 1.66e-07
        -DiffCxt:  4.12e-06
    Outer Step:   440      LossTrajs: 0.00144339     ContextsNorm: 0.00066775     ValIndCrit: 0.00635047
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 3.49e-07
        -DiffCxt:  1.46e-06
    Outer Step:   450      LossTrajs: 0.00139442     ContextsNorm: 0.00069380     ValIndCrit: 0.00611169
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 3.16e-07
        -DiffCxt:  1.11e-06
    Outer Step:   460      LossTrajs: 0.00137193     ContextsNorm: 0.00069639     ValIndCrit: 0.00616563
        -NbInnerStepsNode:    2
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 2.20e-07
        -DiffCxt:  1.36e-06
    Outer Step:   470      LossTrajs: 0.00139605     ContextsNorm: 0.00079929     ValIndCrit: 0.00535387
        -NbInnerStepsNode:    4
        -NbInnerStepsCxt:    2
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 3.68e-07
        -DiffCxt:  1.78e-06
    Outer Step:   480      LossTrajs: 0.00136277     ContextsNorm: 0.00081038     ValIndCrit: 0.00606019
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    4
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 9.31e-08
        -DiffCxt:  8.83e-07
    Outer Step:   490      LossTrajs: 0.00132926     ContextsNorm: 0.00087254     ValIndCrit: 0.00602338
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    5
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 3.41e-07
        -DiffCxt:  1.98e-06
    Outer Step:   499      LossTrajs: 0.00133190     ContextsNorm: 0.00087255     ValIndCrit: 0.00599730
        -NbInnerStepsNode:    2
        -NbInnerStepsCxt:    4
        -InnerToleranceNode: 4.00e-07
        -InnerToleranceCtx:  2.00e-06
        -DiffNode: 2.90e-07
        -DiffCxt:  1.74e-06

Total gradient descent training time: 3 hours 57 mins 48 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 125427
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 0.0059973043


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./runs/27032024-085635/adapt/
 Seed: 6078

==  Begining in-domain visualisation ... ==
    Environment id: 8
    Trajectory id: 31
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/27032024-085635/results_in_domain.png
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 2000
    Total number of training steps: 2000
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (4, 1, 20, 7) (20,)
    Epoch:     0     LossContext: 0.24419552
    Epoch:     1     LossContext: 0.22195610
    Epoch:     2     LossContext: 0.19772322
    Epoch:     3     LossContext: 0.17198084
    Epoch:    10     LossContext: 0.03857272
    Epoch:    20     LossContext: 0.00645718
    Epoch:    30     LossContext: 0.00916614
    Epoch:    40     LossContext: 0.00754361
    Epoch:    50     LossContext: 0.00473253
    Epoch:    60     LossContext: 0.00248481
    Epoch:    70     LossContext: 0.00144114
    Epoch:    80     LossContext: 0.00142768
    Epoch:    90     LossContext: 0.00137128
    Epoch:   100     LossContext: 0.00130213
    Epoch:   110     LossContext: 0.00130132
    Epoch:   120     LossContext: 0.00129731
    Epoch:   130     LossContext: 0.00129482
    Epoch:   140     LossContext: 0.00129418
    Epoch:   150     LossContext: 0.00129164
    Epoch:   160     LossContext: 0.00129258
    Epoch:   170     LossContext: 0.00129218
    Epoch:   180     LossContext: 0.00128972
    Epoch:   190     LossContext: 0.00129070
    Epoch:   200     LossContext: 0.00128968
    Epoch:   210     LossContext: 0.00128927
    Epoch:   220     LossContext: 0.00128924
    Epoch:   230     LossContext: 0.00128828
    Epoch:   240     LossContext: 0.00128655
    Epoch:   250     LossContext: 0.00128588
    Epoch:   260     LossContext: 0.00128470
    Epoch:   270     LossContext: 0.00128497
    Epoch:   280     LossContext: 0.00128343
    Epoch:   290     LossContext: 0.00128337
    Epoch:   300     LossContext: 0.00128305
    Epoch:   310     LossContext: 0.00128169
    Epoch:   320     LossContext: 0.00128008
    Epoch:   330     LossContext: 0.00128008
    Epoch:   340     LossContext: 0.00127939
    Epoch:   350     LossContext: 0.00128109
    Epoch:   360     LossContext: 0.00127735
    Epoch:   370     LossContext: 0.00127781
    Epoch:   380     LossContext: 0.00127724
    Epoch:   390     LossContext: 0.00127831
    Epoch:   400     LossContext: 0.00127759
    Epoch:   410     LossContext: 0.00127797
    Epoch:   420     LossContext: 0.00127737
    Epoch:   430     LossContext: 0.00127561
    Epoch:   440     LossContext: 0.00127574
    Epoch:   450     LossContext: 0.00127620
    Epoch:   460     LossContext: 0.00127613
    Epoch:   470     LossContext: 0.00127463
    Epoch:   480     LossContext: 0.00127572
    Epoch:   490     LossContext: 0.00127453
    Epoch:   500     LossContext: 0.00127348
    Epoch:   510     LossContext: 0.00127373
    Epoch:   520     LossContext: 0.00127494
    Epoch:   530     LossContext: 0.00127319
    Epoch:   540     LossContext: 0.00127248
    Epoch:   550     LossContext: 0.00127302
    Epoch:   560     LossContext: 0.00127231
    Epoch:   570     LossContext: 0.00127103
    Epoch:   580     LossContext: 0.00127174
    Epoch:   590     LossContext: 0.00127330
    Epoch:   600     LossContext: 0.00127094
    Epoch:   610     LossContext: 0.00127032
    Epoch:   620     LossContext: 0.00127108
    Epoch:   630     LossContext: 0.00127189
    Epoch:   640     LossContext: 0.00127085
    Epoch:   650     LossContext: 0.00127128
    Epoch:   660     LossContext: 0.00126847
    Epoch:   670     LossContext: 0.00126965
    Epoch:   680     LossContext: 0.00127183
    Epoch:   690     LossContext: 0.00127056
    Epoch:   700     LossContext: 0.00127130
    Epoch:   710     LossContext: 0.00126971
    Epoch:   720     LossContext: 0.00126852
    Epoch:   730     LossContext: 0.00126929
    Epoch:   740     LossContext: 0.00126795
    Epoch:   750     LossContext: 0.00126703
    Epoch:   760     LossContext: 0.00126805
    Epoch:   770     LossContext: 0.00126713
    Epoch:   780     LossContext: 0.00126920
    Epoch:   790     LossContext: 0.00126831
    Epoch:   800     LossContext: 0.00126681
    Epoch:   810     LossContext: 0.00126604
    Epoch:   820     LossContext: 0.00126718
    Epoch:   830     LossContext: 0.00126683
    Epoch:   840     LossContext: 0.00126828
    Epoch:   850     LossContext: 0.00126662
    Epoch:   860     LossContext: 0.00126696
    Epoch:   870     LossContext: 0.00126657
    Epoch:   880     LossContext: 0.00126714
    Epoch:   890     LossContext: 0.00126669
    Epoch:   900     LossContext: 0.00126480
    Epoch:   910     LossContext: 0.00126636
    Epoch:   920     LossContext: 0.00126563
    Epoch:   930     LossContext: 0.00126687
    Epoch:   940     LossContext: 0.00126465
    Epoch:   950     LossContext: 0.00126469
    Epoch:   960     LossContext: 0.00126600
    Epoch:   970     LossContext: 0.00126416
    Epoch:   980     LossContext: 0.00126535
    Epoch:   990     LossContext: 0.00126448
    Epoch:  1000     LossContext: 0.00126418
    Epoch:  1010     LossContext: 0.00126343
    Epoch:  1020     LossContext: 0.00126422
    Epoch:  1030     LossContext: 0.00126412
    Epoch:  1040     LossContext: 0.00126366
    Epoch:  1050     LossContext: 0.00126420
    Epoch:  1060     LossContext: 0.00126236
    Epoch:  1070     LossContext: 0.00126278
    Epoch:  1080     LossContext: 0.00126400
    Epoch:  1090     LossContext: 0.00126231
    Epoch:  1100     LossContext: 0.00126206
    Epoch:  1110     LossContext: 0.00126353
    Epoch:  1120     LossContext: 0.00126052
    Epoch:  1130     LossContext: 0.00126165
    Epoch:  1140     LossContext: 0.00126087
    Epoch:  1150     LossContext: 0.00126181
    Epoch:  1160     LossContext: 0.00126076
    Epoch:  1170     LossContext: 0.00126216
    Epoch:  1180     LossContext: 0.00126057
    Epoch:  1190     LossContext: 0.00125942
    Epoch:  1200     LossContext: 0.00125918
    Epoch:  1210     LossContext: 0.00125993
    Epoch:  1220     LossContext: 0.00125990
    Epoch:  1230     LossContext: 0.00126091
    Epoch:  1240     LossContext: 0.00126036
    Epoch:  1250     LossContext: 0.00125989
    Epoch:  1260     LossContext: 0.00125793
    Epoch:  1270     LossContext: 0.00125980
    Epoch:  1280     LossContext: 0.00125826
    Epoch:  1290     LossContext: 0.00125920
    Epoch:  1300     LossContext: 0.00125704
    Epoch:  1310     LossContext: 0.00125958
    Epoch:  1320     LossContext: 0.00125634
    Epoch:  1330     LossContext: 0.00125872
    Epoch:  1340     LossContext: 0.00125671
    Epoch:  1350     LossContext: 0.00125627
    Epoch:  1360     LossContext: 0.00125868
    Epoch:  1370     LossContext: 0.00125766
    Epoch:  1380     LossContext: 0.00125727
    Epoch:  1390     LossContext: 0.00125637
    Epoch:  1400     LossContext: 0.00125671
    Epoch:  1410     LossContext: 0.00125614
    Epoch:  1420     LossContext: 0.00125673
    Epoch:  1430     LossContext: 0.00125611
    Epoch:  1440     LossContext: 0.00125544
    Epoch:  1450     LossContext: 0.00125576
    Epoch:  1460     LossContext: 0.00125533
    Epoch:  1470     LossContext: 0.00125501
    Epoch:  1480     LossContext: 0.00125482
    Epoch:  1490     LossContext: 0.00125453
    Epoch:  1500     LossContext: 0.00125467
    Epoch:  1510     LossContext: 0.00125656
    Epoch:  1520     LossContext: 0.00125397
    Epoch:  1530     LossContext: 0.00125356
    Epoch:  1540     LossContext: 0.00125351
    Epoch:  1550     LossContext: 0.00125265
    Epoch:  1560     LossContext: 0.00125222
    Epoch:  1570     LossContext: 0.00125153
    Epoch:  1580     LossContext: 0.00125357
    Epoch:  1590     LossContext: 0.00125383
    Epoch:  1600     LossContext: 0.00125188
    Epoch:  1610     LossContext: 0.00125220
    Epoch:  1620     LossContext: 0.00125286
    Epoch:  1630     LossContext: 0.00125306
    Epoch:  1640     LossContext: 0.00125072
    Epoch:  1650     LossContext: 0.00125148
    Epoch:  1660     LossContext: 0.00125071
    Epoch:  1670     LossContext: 0.00124981
    Epoch:  1680     LossContext: 0.00124893
    Epoch:  1690     LossContext: 0.00125155
    Epoch:  1700     LossContext: 0.00125110
    Epoch:  1710     LossContext: 0.00124923
    Epoch:  1720     LossContext: 0.00124955
    Epoch:  1730     LossContext: 0.00124996
    Epoch:  1740     LossContext: 0.00124988
    Epoch:  1750     LossContext: 0.00125040
    Epoch:  1760     LossContext: 0.00125033
    Epoch:  1770     LossContext: 0.00125000
    Epoch:  1780     LossContext: 0.00124667
    Epoch:  1790     LossContext: 0.00124820
    Epoch:  1800     LossContext: 0.00124809
    Epoch:  1810     LossContext: 0.00124803
    Epoch:  1820     LossContext: 0.00124781
    Epoch:  1830     LossContext: 0.00124598
    Epoch:  1840     LossContext: 0.00124741
    Epoch:  1850     LossContext: 0.00124818
    Epoch:  1860     LossContext: 0.00124702
    Epoch:  1870     LossContext: 0.00124674
    Epoch:  1880     LossContext: 0.00124551
    Epoch:  1890     LossContext: 0.00124523
    Epoch:  1900     LossContext: 0.00124562
    Epoch:  1910     LossContext: 0.00124605
    Epoch:  1920     LossContext: 0.00124387
    Epoch:  1930     LossContext: 0.00124618
    Epoch:  1940     LossContext: 0.00124405
    Epoch:  1950     LossContext: 0.00124698
    Epoch:  1960     LossContext: 0.00124706
    Epoch:  1970     LossContext: 0.00124494
    Epoch:  1980     LossContext: 0.00124632
    Epoch:  1990     LossContext: 0.00124489
    Epoch:  1999     LossContext: 0.00124607

Total gradient descent adaptation time: 0 hours 2 mins 11 secs
Environment weights at the end of the adaptation: [0.25 0.25 0.25 0.25]

Saving adaptation parameters into ./runs/27032024-085635/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 4
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 0.0011034504

==  Begining out-of-distribution visualisation ... ==
    Environment id: 0
    Trajectory id: 0
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/27032024-085635/adapt/results_ood.png

Full evaluation of the model on many random seeds


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]

############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]


Mean and std of the scores across various datasets

          seed  ind_crit  ood_crit
count 2.00e+01  2.00e+01  2.00e+01
mean  4.87e+03  6.00e-03  1.10e-03
std   2.93e+03  0.00e+00  2.22e-19
