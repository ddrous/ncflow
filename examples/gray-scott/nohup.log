
############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.14
Available devices: [gpu(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: train
 Savepath: ./runs/29032024-094232/
 Seed: 2026


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.14
Available devices: [gpu(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: test
 Savepath: ./runs/29032024-094232/
 Seed: 4052

2024-03-29 09:44:02.528423: W external/xla/xla/service/hlo_rematerialization.cc:2202] Can't reduce memory use below 29.54GiB (31722356736 bytes) by rematerialization; only reduced to 33.25GiB (35703700584 bytes), down from 33.28GiB (35739201112 bytes) originally
2024-03-29 09:44:27.934116: W external/tsl/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 33.27GiB (rounded to 35719487744)requested by op 
2024-03-29 09:44:27.934506: W external/tsl/tsl/framework/bfc_allocator.cc:497] ______________________________________________________________________________________****___****xx*
2024-03-29 09:44:27.941427: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2593] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 35719487584 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:    4.70MiB
              constant allocation:        80B
        maybe_live_out allocation:    3.29MiB
     preallocated temp allocation:   33.27GiB
                 total allocation:   33.27GiB
Peak buffers:
	Buffer 1:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 2:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 3:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 4:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 5:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 6:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 7:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 8:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 9:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 10:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 11:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 12:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 13:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 14:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 15:
		Size: 82.41MiB
		Operator: op_name="jit(train_step_node)/jit(main)/broadcast_in_dim[shape=(73, 8, 32, 34, 34) broadcast_dimensions=()]" source_file="/lustre/home/br-rnzoyem/Projects/Nodax/nodax/learner.py" source_line=174
		XLA Label: broadcast
		Shape: f32[73,8,32,34,34]
		==========================



############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.14
Available devices: [gpu(id=0)]
New run folder created successfuly: ./runs/29032024-094232/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 094334
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 094334
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 287636 


WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed


=== Beginning training with proximal alternating minimization ... ===
    Number of examples in a batch: 1
    Maximum number of steps per inner minimization: 20
    Maximum number of outer minimizations: 800
    Maximum total number of training steps: 16000

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (4, 1, 10, 2048) (10,)
Traceback (most recent call last):
  File "/lustre/home/br-rnzoyem/Projects/Nodax/examples/gray-scott/main_T2.py", line 353, in <module>
    trainer.train_proximal(nb_outer_steps_max=nb_outer_steps_max,
  File "/lustre/home/br-rnzoyem/Projects/Nodax/nodax/trainer.py", line 282, in train_proximal
    node, contexts, opt_state_node, loss_node, (nb_steps_node_, term1, term2, diff_node_) = train_step_node(node, node_old, contexts, batch, weights, opt_state_node, loss_key)
  File "/lustre/home/br-rnzoyem/Projects/GraphPint/venv/lib/python3.9/site-packages/jax/_src/traceback_util.py", line 166, in reraise_with_filtered_traceback
    return fun(*args, **kwargs)
  File "/lustre/home/br-rnzoyem/Projects/GraphPint/venv/lib/python3.9/site-packages/jax/_src/pjit.py", line 253, in cache_miss
    outs, out_flat, out_tree, args_flat, jaxpr = _python_pjit_helper(
  File "/lustre/home/br-rnzoyem/Projects/GraphPint/venv/lib/python3.9/site-packages/jax/_src/pjit.py", line 166, in _python_pjit_helper
    out_flat = pjit_p.bind(*args_flat, **params)
  File "/lustre/home/br-rnzoyem/Projects/GraphPint/venv/lib/python3.9/site-packages/jax/_src/core.py", line 2596, in bind
    return self.bind_with_trace(top_trace, args, params)
  File "/lustre/home/br-rnzoyem/Projects/GraphPint/venv/lib/python3.9/site-packages/jax/_src/core.py", line 389, in bind_with_trace
    out = trace.process_primitive(self, map(trace.full_raise, args), params)
  File "/lustre/home/br-rnzoyem/Projects/GraphPint/venv/lib/python3.9/site-packages/jax/_src/core.py", line 821, in process_primitive
    return primitive.impl(*tracers, **params)
  File "/lustre/home/br-rnzoyem/Projects/GraphPint/venv/lib/python3.9/site-packages/jax/_src/pjit.py", line 1209, in _pjit_call_impl
    return xc._xla.pjit(name, f, call_impl_cache_miss, [], [], donated_argnums,
  File "/lustre/home/br-rnzoyem/Projects/GraphPint/venv/lib/python3.9/site-packages/jax/_src/pjit.py", line 1192, in call_impl_cache_miss
    out_flat, compiled = _pjit_call_impl_python(
  File "/lustre/home/br-rnzoyem/Projects/GraphPint/venv/lib/python3.9/site-packages/jax/_src/pjit.py", line 1148, in _pjit_call_impl_python
    return compiled.unsafe_call(*args), compiled
  File "/lustre/home/br-rnzoyem/Projects/GraphPint/venv/lib/python3.9/site-packages/jax/_src/profiler.py", line 314, in wrapper
    return func(*args, **kwargs)
  File "/lustre/home/br-rnzoyem/Projects/GraphPint/venv/lib/python3.9/site-packages/jax/_src/interpreters/pxla.py", line 1229, in __call__
    results = self.xla_executable.execute_sharded(input_bufs)
jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 35719487584 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:    4.70MiB
              constant allocation:        80B
        maybe_live_out allocation:    3.29MiB
     preallocated temp allocation:   33.27GiB
                 total allocation:   33.27GiB
Peak buffers:
	Buffer 1:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 2:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 3:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 4:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 5:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 6:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 7:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 8:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 9:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 10:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 11:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 12:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 13:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 14:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 15:
		Size: 82.41MiB
		Operator: op_name="jit(train_step_node)/jit(main)/broadcast_in_dim[shape=(73, 8, 32, 34, 34) broadcast_dimensions=()]" source_file="/lustre/home/br-rnzoyem/Projects/Nodax/nodax/learner.py" source_line=174
		XLA Label: broadcast
		Shape: f32[73,8,32,34,34]
		==========================

The stack trace below excludes JAX-internal frames.
The preceding is the original exception that occurred, unmodified.

--------------------

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/lustre/home/br-rnzoyem/Projects/Nodax/examples/gray-scott/main_T2.py", line 353, in <module>
    trainer.train_proximal(nb_outer_steps_max=nb_outer_steps_max,
  File "/lustre/home/br-rnzoyem/Projects/Nodax/nodax/trainer.py", line 282, in train_proximal
    node, contexts, opt_state_node, loss_node, (nb_steps_node_, term1, term2, diff_node_) = train_step_node(node, node_old, contexts, batch, weights, opt_state_node, loss_key)
  File "/lustre/home/br-rnzoyem/Projects/GraphPint/venv/lib/python3.9/site-packages/equinox/_jit.py", line 206, in __call__
    return self._call(False, args, kwargs)
  File "/lustre/home/br-rnzoyem/Projects/GraphPint/venv/lib/python3.9/site-packages/equinox/_module.py", line 875, in __call__
    return self.__func__(self.__self__, *args, **kwargs)
  File "/lustre/home/br-rnzoyem/Projects/GraphPint/venv/lib/python3.9/site-packages/equinox/_jit.py", line 200, in _call
    out = self._cached(dynamic_donate, dynamic_nodonate, static)
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 35719487584 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:    4.70MiB
              constant allocation:        80B
        maybe_live_out allocation:    3.29MiB
     preallocated temp allocation:   33.27GiB
                 total allocation:   33.27GiB
Peak buffers:
	Buffer 1:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 2:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 3:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 4:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 5:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 6:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 7:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 8:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 9:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 10:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 11:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 12:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 13:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 14:
		Size: 82.41MiB
		XLA Label: fusion
		Shape: f32[73,8,32,34,34]
		==========================

	Buffer 15:
		Size: 82.41MiB
		Operator: op_name="jit(train_step_node)/jit(main)/broadcast_in_dim[shape=(73, 8, 32, 34, 34) broadcast_dimensions=()]" source_file="/lustre/home/br-rnzoyem/Projects/Nodax/nodax/learner.py" source_line=174
		XLA Label: broadcast
		Shape: f32[73,8,32,34,34]
		==========================


