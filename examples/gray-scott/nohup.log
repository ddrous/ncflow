Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: train
 Savepath: ./runs/21022024-121527/
 Seed: 2026

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: test
 Savepath: ./runs/21022024-121527/
 Seed: 4052

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./runs/21022024-121527/adapt/
 Seed: 6078


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.23
Available devices: [cuda(id=0)]
Data folder created successfuly: ./runs/21022024-121527/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 121535
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: You did not provide a dataloader id. A new one has been generated: 121536
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 4200374 


WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed


=== Beginning training ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 18000
    Total number of training steps: 18000

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (4, 1, 10, 2048) (10,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (4, 1, 10, 2048) (10,)
    Epoch:     0      LossTrajs: 8937.43457031     ContextsNorm: 0.00000000     ValIndCrit: 21615.28125000
    Epoch:     1      LossTrajs: 21615.42968750     ContextsNorm: 0.00999988     ValIndCrit: 10067.46289062
    Epoch:     2      LossTrajs: 10067.57812500     ContextsNorm: 0.01846115     ValIndCrit: 47097.55859375
    Epoch:     3      LossTrajs: 47097.61328125     ContextsNorm: 0.01237317     ValIndCrit: 1540.37536621
    Epoch:  1000      LossTrajs: 0.05446807     ContextsNorm: 0.12620294     ValIndCrit: 0.05316104
    Epoch:  2000      LossTrajs: 0.05446807     ContextsNorm: 0.12620294     ValIndCrit: 0.05316104
    Epoch:  3000      LossTrajs: 0.05446807     ContextsNorm: 0.12620294     ValIndCrit: 0.05316104
    Epoch:  4000      LossTrajs: 0.05446807     ContextsNorm: 0.12620294     ValIndCrit: 0.05316104
    Epoch:  5000      LossTrajs: 0.05446807     ContextsNorm: 0.12620294     ValIndCrit: 0.05316104
    Epoch:  6000      LossTrajs: 0.05446807     ContextsNorm: 0.12620294     ValIndCrit: 0.05316104
    Epoch:  7000      LossTrajs: 0.05446807     ContextsNorm: 0.12620294     ValIndCrit: 0.05316104
    Epoch:  8000      LossTrajs: 0.05446807     ContextsNorm: 0.12620294     ValIndCrit: 0.05316104
    Epoch:  9000      LossTrajs: 0.05446807     ContextsNorm: 0.12620294     ValIndCrit: 0.05316104
    Epoch: 10000      LossTrajs: 0.05446807     ContextsNorm: 0.12620294     ValIndCrit: 0.05316104
    Epoch: 11000      LossTrajs: 0.05446807     ContextsNorm: 0.12620294     ValIndCrit: 0.05316104
    Epoch: 12000      LossTrajs: 0.05446807     ContextsNorm: 0.12620294     ValIndCrit: 0.05316104
    Epoch: 13000      LossTrajs: 0.05446807     ContextsNorm: 0.12620294     ValIndCrit: 0.05316104
    Epoch: 14000      LossTrajs: 0.05446807     ContextsNorm: 0.12620294     ValIndCrit: 0.05316104
    Epoch: 15000      LossTrajs: 0.05446807     ContextsNorm: 0.12620294     ValIndCrit: 0.05316104
    Epoch: 16000      LossTrajs: 0.05446807     ContextsNorm: 0.12620294     ValIndCrit: 0.05316104
    Epoch: 17000      LossTrajs: 0.05446807     ContextsNorm: 0.12620294     ValIndCrit: 0.05316104
    Epoch: 17999      LossTrajs: 0.05446807     ContextsNorm: 0.12620294     ValIndCrit: 0.05316104

Total gradient descent training time: 0 hours 25 mins 43 secs
Environment weights at the end of the training: [0.25 0.25 0.25 0.25]
WARNING: You did not provide a dataloader id. A new one has been generated: 124122
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 4
    Final length of the training trajectories: 10
    Length of the testing trajectories: 10
Test Score (In-Domain): 0.05316104

==  Begining in-domain visualisation ... ==
    Environment id: 2
    Trajectory id: 0
    Final length of the training trajectories: 10
    Length of the testing trajectories: 10
Testing finished. Figure saved in: ./runs/21022024-121527/results_in_domain.png
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.


=== Beginning adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 18000
    Total number of training steps: 18000
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (4, 1, 10, 2048) (10,)
    Epoch:     0     LossContext: 0.05053605
    Epoch:     1     LossContext: 0.05032079
    Epoch:     2     LossContext: 0.05032144
    Epoch:     3     LossContext: 0.05032148
    Epoch:  1000     LossContext: 0.05032149
    Epoch:  2000     LossContext: 0.05032149
    Epoch:  3000     LossContext: 0.05032149
    Epoch:  4000     LossContext: 0.05032149
    Epoch:  5000     LossContext: 0.05032149
    Epoch:  6000     LossContext: 0.05032149
    Epoch:  7000     LossContext: 0.05032149
    Epoch:  8000     LossContext: 0.05032149
    Epoch:  9000     LossContext: 0.05032149
    Epoch: 10000     LossContext: 0.05032149
    Epoch: 11000     LossContext: 0.05032149
    Epoch: 12000     LossContext: 0.05032149
    Epoch: 13000     LossContext: 0.05032149
    Epoch: 14000     LossContext: 0.05032149
    Epoch: 15000     LossContext: 0.05032149
    Epoch: 16000     LossContext: 0.05032149
    Epoch: 17000     LossContext: 0.05032149
    Epoch: 17999     LossContext: 0.05032149

Total gradient descent adaptation time: 0 hours 6 mins 58 secs
Environment weights at the end of the adaptation: [0.25 0.25 0.25 0.25]

Saving adaptation parameters into ./runs/21022024-121527/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 4
    Number of adaptation environments: 4
    Final length of the training trajectories: 10
    Length of the testing trajectories: 10
Test Score (OOD): 0.050321486

==  Begining out-of-distribution visualisation ... ==
    Environment id: 1
    Trajectory id: 0
    Final length of the training trajectories: 10
    Length of the testing trajectories: 10
Testing finished. Figure saved in: ./runs/21022024-121527/adapt/results_ood.png

Full evaluation of the model on 10 random seeds

          seed  ind_crit  ood_crit
count 1.00e+01  1.00e+01  1.00e+01
mean  5.25e+03  5.25e-02  5.04e-02
std   3.39e+03  2.50e-03  5.76e-04
