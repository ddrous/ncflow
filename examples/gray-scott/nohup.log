
############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.14
Available devices: [gpu(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: train
 Savepath: ./runs/06042024-102728/
 Seed: 270


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.14
Available devices: [gpu(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: test
 Savepath: ./runs/06042024-102728/
 Seed: 540


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.14
Available devices: [gpu(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./runs/06042024-102728/adapt/
 Seed: 810


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.14
Available devices: [gpu(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt_test
 Savepath: ./runs/06042024-102728/adapt/
 Seed: 810


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.14
Available devices: [gpu(id=0)]
Run folder created successfuly: ./runs/06042024-102728/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 102826
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 102826
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 610942 


WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed


=== Beginning training with proximal alternating minimization ... ===
    Number of examples in a batch: 1
    Maximum number of steps per inner minimization: 20
    Maximum number of outer minimizations: 700
    Maximum total number of training steps: 14000

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (4, 1, 10, 2048) (10,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (4, 1, 10, 2048) (10,)
    Outer Step:     0      LossTrajs: 0.09839233     ContextsNorm: 0.00000000     ValIndCrit: 0.07819767
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.12e-04
        -DiffCxt:  inf
    Outer Step:     1      LossTrajs: 0.04987843     ContextsNorm: 0.00089177     ValIndCrit: 0.04659543
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.25e-05
        -DiffCxt:  5.29e-03
    Outer Step:     2      LossTrajs: 0.03632214     ContextsNorm: 0.00176245     ValIndCrit: 0.03739737
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.43e-05
        -DiffCxt:  5.48e-03
    Outer Step:     3      LossTrajs: 0.02472860     ContextsNorm: 0.01879551     ValIndCrit: 0.04398608
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.40e-04
        -DiffCxt:  1.35e-03
    Outer Step:    10      LossTrajs: 0.00458197     ContextsNorm: 0.02198766     ValIndCrit: 0.03503823
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.85e-05
        -DiffCxt:  1.88e-05
    Outer Step:    20      LossTrajs: 0.00283370     ContextsNorm: 0.02291155     ValIndCrit: 0.03245465
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.54e-07
        -DiffCxt:  7.12e-06
    Outer Step:    30      LossTrajs: 0.00236551     ContextsNorm: 0.02326236     ValIndCrit: 0.03113317
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.25e-07
        -DiffCxt:  1.65e-06
    Outer Step:    40      LossTrajs: 0.00209438     ContextsNorm: 0.02443348     ValIndCrit: 0.02985148
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.06e-07
        -DiffCxt:  4.23e-06
    Outer Step:    50      LossTrajs: 0.00182594     ContextsNorm: 0.02592364     ValIndCrit: 0.02867980
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.22e-07
        -DiffCxt:  3.20e-07
    Outer Step:    60      LossTrajs: 0.00229716     ContextsNorm: 0.02605093     ValIndCrit: 0.02632829
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.67e-06
        -DiffCxt:  1.44e-05
    Outer Step:    70      LossTrajs: 0.00146447     ContextsNorm: 0.02709251     ValIndCrit: 0.02620072
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.73e-07
        -DiffCxt:  4.43e-07
    Outer Step:    80      LossTrajs: 0.00131344     ContextsNorm: 0.02795243     ValIndCrit: 0.02527013
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.62e-07
        -DiffCxt:  8.15e-07
    Outer Step:    90      LossTrajs: 0.00128267     ContextsNorm: 0.02967824     ValIndCrit: 0.02510373
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.45e-07
        -DiffCxt:  1.21e-06
    Outer Step:   100      LossTrajs: 0.00118032     ContextsNorm: 0.03074960     ValIndCrit: 0.02392285
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.64e-07
        -DiffCxt:  9.78e-07
    Outer Step:   110      LossTrajs: 0.00111289     ContextsNorm: 0.03087114     ValIndCrit: 0.02261589
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.56e-07
        -DiffCxt:  1.03e-06
    Outer Step:   120      LossTrajs: 0.00102294     ContextsNorm: 0.03104953     ValIndCrit: 0.02127648
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.82e-07
        -DiffCxt:  2.80e-07
    Outer Step:   130      LossTrajs: 0.00095905     ContextsNorm: 0.03088586     ValIndCrit: 0.02006780
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.33e-07
        -DiffCxt:  5.36e-07
    Outer Step:   140      LossTrajs: 0.00089646     ContextsNorm: 0.03005989     ValIndCrit: 0.01874654
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.46e-07
        -DiffCxt:  3.06e-06
    Outer Step:   150      LossTrajs: 0.00078417     ContextsNorm: 0.03072004     ValIndCrit: 0.01771433
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.03e-07
        -DiffCxt:  2.11e-06
    Outer Step:   160      LossTrajs: 0.00069444     ContextsNorm: 0.03133887     ValIndCrit: 0.01730578
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.41e-07
        -DiffCxt:  4.59e-07
    Outer Step:   170      LossTrajs: 0.00055086     ContextsNorm: 0.03106993     ValIndCrit: 0.01685256
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.97e-08
        -DiffCxt:  2.05e-07
    Outer Step:   180      LossTrajs: 0.00048463     ContextsNorm: 0.03095303     ValIndCrit: 0.01720381
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.62e-08
        -DiffCxt:  4.18e-07
    Outer Step:   190      LossTrajs: 0.00037716     ContextsNorm: 0.03003301     ValIndCrit: 0.01708343
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.09e-07
        -DiffCxt:  4.39e-07
    Outer Step:   200      LossTrajs: 0.00032126     ContextsNorm: 0.03019883     ValIndCrit: 0.01672815
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.68e-08
        -DiffCxt:  2.79e-07
    Outer Step:   210      LossTrajs: 0.00034016     ContextsNorm: 0.03000768     ValIndCrit: 0.01603519
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.52e-07
        -DiffCxt:  1.40e-06
    Outer Step:   220      LossTrajs: 0.00026587     ContextsNorm: 0.02910461     ValIndCrit: 0.01558838
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.07e-08
        -DiffCxt:  1.34e-06
    Outer Step:   230      LossTrajs: 0.00024137     ContextsNorm: 0.02820931     ValIndCrit: 0.01544549
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.87e-08
        -DiffCxt:  4.49e-07
    Outer Step:   240      LossTrajs: 0.00022809     ContextsNorm: 0.02800588     ValIndCrit: 0.01476818
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.37e-07
        -DiffCxt:  7.44e-06
    Outer Step:   250      LossTrajs: 0.00021023     ContextsNorm: 0.02795589     ValIndCrit: 0.01440355
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.77e-08
        -DiffCxt:  2.51e-06
    Outer Step:   260      LossTrajs: 0.00022487     ContextsNorm: 0.02756490     ValIndCrit: 0.01392502
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.76e-07
        -DiffCxt:  2.11e-06
    Outer Step:   270      LossTrajs: 0.00019343     ContextsNorm: 0.02686343     ValIndCrit: 0.01374507
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.59e-08
        -DiffCxt:  2.57e-06
    Outer Step:   280      LossTrajs: 0.00031611     ContextsNorm: 0.02762160     ValIndCrit: 0.01282613
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.75e-07
        -DiffCxt:  4.63e-06
    Outer Step:   290      LossTrajs: 0.00021441     ContextsNorm: 0.03043890     ValIndCrit: 0.01222532
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.07e-07
        -DiffCxt:  7.97e-07
    Outer Step:   300      LossTrajs: 0.00016457     ContextsNorm: 0.02916516     ValIndCrit: 0.01208298
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.42e-08
        -DiffCxt:  1.58e-06
    Outer Step:   310      LossTrajs: 0.00015718     ContextsNorm: 0.03016588     ValIndCrit: 0.01145426
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.53e-08
        -DiffCxt:  9.49e-07
    Outer Step:   320      LossTrajs: 0.00019953     ContextsNorm: 0.03142169     ValIndCrit: 0.01108816
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 8.02e-07
        -DiffCxt:  5.05e-07
    Outer Step:   330      LossTrajs: 0.00015766     ContextsNorm: 0.03138417     ValIndCrit: 0.01039681
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.98e-08
        -DiffCxt:  2.36e-07
    Outer Step:   340      LossTrajs: 0.00016417     ContextsNorm: 0.03125758     ValIndCrit: 0.01023607
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.27e-07
        -DiffCxt:  1.64e-06
    Outer Step:   350      LossTrajs: 0.00016460     ContextsNorm: 0.03191061     ValIndCrit: 0.00993732
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.82e-07
        -DiffCxt:  4.25e-06
    Outer Step:   360      LossTrajs: 0.00021378     ContextsNorm: 0.03400873     ValIndCrit: 0.00970103
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.30e-07
        -DiffCxt:  1.96e-06
    Outer Step:   370      LossTrajs: 0.00016619     ContextsNorm: 0.03282250     ValIndCrit: 0.00992799
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.25e-07
        -DiffCxt:  1.03e-06
    Outer Step:   380      LossTrajs: 0.00016362     ContextsNorm: 0.03304374     ValIndCrit: 0.00969042
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.31e-07
        -DiffCxt:  3.29e-06
    Outer Step:   390      LossTrajs: 0.00015340     ContextsNorm: 0.03430436     ValIndCrit: 0.00954315
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.29e-07
        -DiffCxt:  2.32e-06
    Outer Step:   400      LossTrajs: 0.00014955     ContextsNorm: 0.03423672     ValIndCrit: 0.00956705
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.32e-08
        -DiffCxt:  4.25e-07
    Outer Step:   410      LossTrajs: 0.00014403     ContextsNorm: 0.03392914     ValIndCrit: 0.00931136
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.57e-07
        -DiffCxt:  1.51e-06
    Outer Step:   420      LossTrajs: 0.00019543     ContextsNorm: 0.03193710     ValIndCrit: 0.00959769
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.10e-07
        -DiffCxt:  3.56e-06
    Outer Step:   430      LossTrajs: 0.00013902     ContextsNorm: 0.03147683     ValIndCrit: 0.00916894
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.21e-08
        -DiffCxt:  5.80e-07
    Outer Step:   440      LossTrajs: 0.00011652     ContextsNorm: 0.03167839     ValIndCrit: 0.00902791
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.44e-08
        -DiffCxt:  3.30e-07
    Outer Step:   450      LossTrajs: 0.00020034     ContextsNorm: 0.03297991     ValIndCrit: 0.00853985
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.02e-07
        -DiffCxt:  2.79e-06
    Outer Step:   460      LossTrajs: 0.00012857     ContextsNorm: 0.03273459     ValIndCrit: 0.00875188
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.92e-08
        -DiffCxt:  9.16e-07
    Outer Step:   470      LossTrajs: 0.00014580     ContextsNorm: 0.03472921     ValIndCrit: 0.00874217
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.18e-07
        -DiffCxt:  5.21e-06
    Outer Step:   480      LossTrajs: 0.00011472     ContextsNorm: 0.03499252     ValIndCrit: 0.00877873
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.75e-08
        -DiffCxt:  1.60e-07
    Outer Step:   490      LossTrajs: 0.00021946     ContextsNorm: 0.03598739     ValIndCrit: 0.00878521
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.02e-07
        -DiffCxt:  3.51e-06
    Outer Step:   500      LossTrajs: 0.00015148     ContextsNorm: 0.03658524     ValIndCrit: 0.00826900
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.34e-07
        -DiffCxt:  1.37e-06
    Outer Step:   510      LossTrajs: 0.00013230     ContextsNorm: 0.03602824     ValIndCrit: 0.00814116
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.59e-08
        -DiffCxt:  1.11e-06
    Outer Step:   520      LossTrajs: 0.00018785     ContextsNorm: 0.03627728     ValIndCrit: 0.01138323
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.83e-08
        -DiffCxt:  7.32e-08
    Outer Step:   530      LossTrajs: 0.00012826     ContextsNorm: 0.03554079     ValIndCrit: 0.00960307
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.51e-08
        -DiffCxt:  5.50e-08
    Outer Step:   540      LossTrajs: 0.00012410     ContextsNorm: 0.03499467     ValIndCrit: 0.00891884
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.98e-08
        -DiffCxt:  3.40e-07
    Outer Step:   550      LossTrajs: 0.00012473     ContextsNorm: 0.03428233     ValIndCrit: 0.00847821
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.37e-08
        -DiffCxt:  2.37e-07
    Outer Step:   560      LossTrajs: 0.00010350     ContextsNorm: 0.03430763     ValIndCrit: 0.00800588
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.19e-08
        -DiffCxt:  1.40e-07
    Outer Step:   570      LossTrajs: 0.00016825     ContextsNorm: 0.03342247     ValIndCrit: 0.00764581
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.29e-08
        -DiffCxt:  3.32e-07
    Outer Step:   580      LossTrajs: 0.00009897     ContextsNorm: 0.03231409     ValIndCrit: 0.00736733
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 9.29e-09
        -DiffCxt:  6.39e-08
    Outer Step:   590      LossTrajs: 0.00019860     ContextsNorm: 0.03301364     ValIndCrit: 0.00709488
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.93e-07
        -DiffCxt:  7.04e-07
    Outer Step:   600      LossTrajs: 0.00010351     ContextsNorm: 0.03347761     ValIndCrit: 0.00719187
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.28e-08
        -DiffCxt:  5.04e-07
    Outer Step:   610      LossTrajs: 0.00011961     ContextsNorm: 0.03319368     ValIndCrit: 0.00683794
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.37e-07
        -DiffCxt:  1.46e-06
    Outer Step:   620      LossTrajs: 0.00016342     ContextsNorm: 0.03285415     ValIndCrit: 0.00714732
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 7.90e-08
        -DiffCxt:  1.29e-06
    Outer Step:   630      LossTrajs: 0.00013021     ContextsNorm: 0.03226090     ValIndCrit: 0.00705787
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 5.03e-08
        -DiffCxt:  6.85e-07
    Outer Step:   640      LossTrajs: 0.00010854     ContextsNorm: 0.03199641     ValIndCrit: 0.00691137
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 4.61e-08
        -DiffCxt:  5.07e-07
    Outer Step:   650      LossTrajs: 0.00009074     ContextsNorm: 0.03180183     ValIndCrit: 0.00693169
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.80e-08
        -DiffCxt:  1.63e-07
    Outer Step:   660      LossTrajs: 0.00011649     ContextsNorm: 0.03175983     ValIndCrit: 0.00686742
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.79e-07
        -DiffCxt:  1.48e-06
    Outer Step:   670      LossTrajs: 0.00009162     ContextsNorm: 0.03206043     ValIndCrit: 0.00693926
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 2.85e-08
        -DiffCxt:  5.30e-07
    Outer Step:   680      LossTrajs: 0.00010636     ContextsNorm: 0.03240077     ValIndCrit: 0.00699216
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 3.88e-08
        -DiffCxt:  5.62e-07
    Outer Step:   690      LossTrajs: 0.00010251     ContextsNorm: 0.03214731     ValIndCrit: 0.00687178
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 1.24e-07
        -DiffCxt:  1.26e-06
    Outer Step:   699      LossTrajs: 0.00009227     ContextsNorm: 0.03171422     ValIndCrit: 0.00663863
        Saving best model so far ...
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  1.00e-08
        -DiffNode: 6.78e-08
        -DiffCxt:  4.53e-07

Total gradient descent training time: 6 hours 48 mins 52 secs
Environment weights at the end of the training: [0.25 0.25 0.25 0.25]
WARNING: You did not provide a dataloader id. A new one has been generated: 171723
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 4
    Final length of the training trajectories: 10
    Length of the testing trajectories: 10
Test Score (In-Domain): 0.0066386256

==  Begining in-domain visualisation ... ==
    Environment id: 2
    Trajectory id: 20
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 10
    Length of the testing trajectories: 10
Testing finished. Figure saved in: ./runs/06042024-102728/results_in_domain.png
==  Begining in-domain 2D visualisation ... ==
    Environment id: 2
    Trajectory id: 19
    Length of the testing trajectories: 10
Testing finished. Figure saved in: ./runs/06042024-102728/results_2D_ind.png
==  Begining in-domain 2D visualisation ... ==
    Environment id: 1
    Trajectory id: 0
    Length of the testing trajectories: 10
Testing finished. Figure saved in: ./runs/06042024-102728/results_2D_ind_train.png
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 10, 2048) (10,)
    Epoch:     0     LossContext: 0.01011926
    Epoch:     1     LossContext: 0.00931397
    Epoch:     2     LossContext: 0.00847497
    Epoch:     3     LossContext: 0.00760719
    Epoch:    10     LossContext: 0.00322126
    Epoch:    20     LossContext: 0.00302318
    Epoch:    30     LossContext: 0.00185934
    Epoch:    40     LossContext: 0.00119018
    Epoch:    50     LossContext: 0.00073666
    Epoch:    60     LossContext: 0.00051193
    Epoch:    70     LossContext: 0.00039170
    Epoch:    80     LossContext: 0.00033405
    Epoch:    90     LossContext: 0.00029517
    Epoch:   100     LossContext: 0.00026801
    Epoch:   110     LossContext: 0.00024682
    Epoch:   120     LossContext: 0.00022970
    Epoch:   130     LossContext: 0.00021535
    Epoch:   140     LossContext: 0.00020328
    Epoch:   150     LossContext: 0.00019305
    Epoch:   160     LossContext: 0.00018457
    Epoch:   170     LossContext: 0.00017733
    Epoch:   180     LossContext: 0.00017128
    Epoch:   190     LossContext: 0.00016613
    Epoch:   200     LossContext: 0.00016180
    Epoch:   210     LossContext: 0.00015808
    Epoch:   220     LossContext: 0.00015496
    Epoch:   230     LossContext: 0.00015220
    Epoch:   240     LossContext: 0.00014976
    Epoch:   250     LossContext: 0.00014758
    Epoch:   260     LossContext: 0.00014568
    Epoch:   270     LossContext: 0.00014392
    Epoch:   280     LossContext: 0.00014224
    Epoch:   290     LossContext: 0.00014076
    Epoch:   300     LossContext: 0.00013935
    Epoch:   310     LossContext: 0.00013801
    Epoch:   320     LossContext: 0.00013675
    Epoch:   330     LossContext: 0.00013553
    Epoch:   340     LossContext: 0.00013442
    Epoch:   350     LossContext: 0.00013330
    Epoch:   360     LossContext: 0.00013221
    Epoch:   370     LossContext: 0.00013121
    Epoch:   380     LossContext: 0.00013028
    Epoch:   390     LossContext: 0.00012935
    Epoch:   400     LossContext: 0.00012844
    Epoch:   410     LossContext: 0.00012763
    Epoch:   420     LossContext: 0.00012676
    Epoch:   430     LossContext: 0.00012599
    Epoch:   440     LossContext: 0.00012523
    Epoch:   450     LossContext: 0.00012453
    Epoch:   460     LossContext: 0.00012381
    Epoch:   470     LossContext: 0.00012313
    Epoch:   480     LossContext: 0.00012249
    Epoch:   490     LossContext: 0.00012186
    Epoch:   500     LossContext: 0.00012123
    Epoch:   510     LossContext: 0.00012067
    Epoch:   520     LossContext: 0.00012007
    Epoch:   530     LossContext: 0.00011955
    Epoch:   540     LossContext: 0.00011904
    Epoch:   550     LossContext: 0.00011852
    Epoch:   560     LossContext: 0.00011800
    Epoch:   570     LossContext: 0.00011751
    Epoch:   580     LossContext: 0.00011706
    Epoch:   590     LossContext: 0.00011659
    Epoch:   600     LossContext: 0.00011617
    Epoch:   610     LossContext: 0.00011577
    Epoch:   620     LossContext: 0.00011536
    Epoch:   630     LossContext: 0.00011495
    Epoch:   640     LossContext: 0.00011457
    Epoch:   650     LossContext: 0.00011419
    Epoch:   660     LossContext: 0.00011381
    Epoch:   670     LossContext: 0.00011349
    Epoch:   680     LossContext: 0.00011312
    Epoch:   690     LossContext: 0.00011279
    Epoch:   700     LossContext: 0.00011246
    Epoch:   710     LossContext: 0.00011216
    Epoch:   720     LossContext: 0.00011183
    Epoch:   730     LossContext: 0.00011153
    Epoch:   740     LossContext: 0.00011122
    Epoch:   750     LossContext: 0.00011096
    Epoch:   760     LossContext: 0.00011067
    Epoch:   770     LossContext: 0.00011037
    Epoch:   780     LossContext: 0.00011008
    Epoch:   790     LossContext: 0.00010984
    Epoch:   800     LossContext: 0.00010960
    Epoch:   810     LossContext: 0.00010935
    Epoch:   820     LossContext: 0.00010907
    Epoch:   830     LossContext: 0.00010884
    Epoch:   840     LossContext: 0.00010862
    Epoch:   850     LossContext: 0.00010836
    Epoch:   860     LossContext: 0.00010815
    Epoch:   870     LossContext: 0.00010792
    Epoch:   880     LossContext: 0.00010772
    Epoch:   890     LossContext: 0.00010752
    Epoch:   900     LossContext: 0.00010729
    Epoch:   910     LossContext: 0.00010708
    Epoch:   920     LossContext: 0.00010690
    Epoch:   930     LossContext: 0.00010671
    Epoch:   940     LossContext: 0.00010652
    Epoch:   950     LossContext: 0.00010636
    Epoch:   960     LossContext: 0.00010617
    Epoch:   970     LossContext: 0.00010599
    Epoch:   980     LossContext: 0.00010586
    Epoch:   990     LossContext: 0.00010566
    Epoch:  1000     LossContext: 0.00010550
    Epoch:  1010     LossContext: 0.00010537
    Epoch:  1020     LossContext: 0.00010521
    Epoch:  1030     LossContext: 0.00010505
    Epoch:  1040     LossContext: 0.00010487
    Epoch:  1050     LossContext: 0.00010474
    Epoch:  1060     LossContext: 0.00010460
    Epoch:  1070     LossContext: 0.00010443
    Epoch:  1080     LossContext: 0.00010433
    Epoch:  1090     LossContext: 0.00010419
    Epoch:  1100     LossContext: 0.00010403
    Epoch:  1110     LossContext: 0.00010389
    Epoch:  1120     LossContext: 0.00010378
    Epoch:  1130     LossContext: 0.00010365
    Epoch:  1140     LossContext: 0.00010354
    Epoch:  1150     LossContext: 0.00010340
    Epoch:  1160     LossContext: 0.00010331
    Epoch:  1170     LossContext: 0.00010315
    Epoch:  1180     LossContext: 0.00010306
    Epoch:  1190     LossContext: 0.00010296
    Epoch:  1200     LossContext: 0.00010283
    Epoch:  1210     LossContext: 0.00010275
    Epoch:  1220     LossContext: 0.00010261
    Epoch:  1230     LossContext: 0.00010248
    Epoch:  1240     LossContext: 0.00010240
    Epoch:  1250     LossContext: 0.00010233
    Epoch:  1260     LossContext: 0.00010221
    Epoch:  1270     LossContext: 0.00010212
    Epoch:  1280     LossContext: 0.00010204
    Epoch:  1290     LossContext: 0.00010192
    Epoch:  1300     LossContext: 0.00010185
    Epoch:  1310     LossContext: 0.00010176
    Epoch:  1320     LossContext: 0.00010164
    Epoch:  1330     LossContext: 0.00010154
    Epoch:  1340     LossContext: 0.00010145
    Epoch:  1350     LossContext: 0.00010134
    Epoch:  1360     LossContext: 0.00010129
    Epoch:  1370     LossContext: 0.00010120
    Epoch:  1380     LossContext: 0.00010109
    Epoch:  1390     LossContext: 0.00010101
    Epoch:  1400     LossContext: 0.00010093
    Epoch:  1410     LossContext: 0.00010085
    Epoch:  1420     LossContext: 0.00010080
    Epoch:  1430     LossContext: 0.00010072
    Epoch:  1440     LossContext: 0.00010065
    Epoch:  1450     LossContext: 0.00010057
    Epoch:  1460     LossContext: 0.00010049
    Epoch:  1470     LossContext: 0.00010044
    Epoch:  1480     LossContext: 0.00010034
    Epoch:  1490     LossContext: 0.00010031
    Epoch:  1499     LossContext: 0.00010022

Gradient descent adaptation time: 0 hours 10 mins 10 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.02705031
    Epoch:     1     LossContext: 0.02577960
    Epoch:     2     LossContext: 0.02442854
    Epoch:     3     LossContext: 0.02298953
    Epoch:    10     LossContext: 0.01123152
    Epoch:    20     LossContext: 0.00324219
    Epoch:    30     LossContext: 0.00311632
    Epoch:    40     LossContext: 0.00218007
    Epoch:    50     LossContext: 0.00206730
    Epoch:    60     LossContext: 0.00183137
    Epoch:    70     LossContext: 0.00161883
    Epoch:    80     LossContext: 0.00139289
    Epoch:    90     LossContext: 0.00116852
    Epoch:   100     LossContext: 0.00095517
    Epoch:   110     LossContext: 0.00076865
    Epoch:   120     LossContext: 0.00061837
    Epoch:   130     LossContext: 0.00051276
    Epoch:   140     LossContext: 0.00044306
    Epoch:   150     LossContext: 0.00038950
    Epoch:   160     LossContext: 0.00034455
    Epoch:   170     LossContext: 0.00030765
    Epoch:   180     LossContext: 0.00027793
    Epoch:   190     LossContext: 0.00025402
    Epoch:   200     LossContext: 0.00023497
    Epoch:   210     LossContext: 0.00021963
    Epoch:   220     LossContext: 0.00020732
    Epoch:   230     LossContext: 0.00019734
    Epoch:   240     LossContext: 0.00018924
    Epoch:   250     LossContext: 0.00018237
    Epoch:   260     LossContext: 0.00017671
    Epoch:   270     LossContext: 0.00017184
    Epoch:   280     LossContext: 0.00016764
    Epoch:   290     LossContext: 0.00016389
    Epoch:   300     LossContext: 0.00016062
    Epoch:   310     LossContext: 0.00015757
    Epoch:   320     LossContext: 0.00015490
    Epoch:   330     LossContext: 0.00015243
    Epoch:   340     LossContext: 0.00015022
    Epoch:   350     LossContext: 0.00014814
    Epoch:   360     LossContext: 0.00014624
    Epoch:   370     LossContext: 0.00014439
    Epoch:   380     LossContext: 0.00014272
    Epoch:   390     LossContext: 0.00014110
    Epoch:   400     LossContext: 0.00013970
    Epoch:   410     LossContext: 0.00013832
    Epoch:   420     LossContext: 0.00013704
    Epoch:   430     LossContext: 0.00013583
    Epoch:   440     LossContext: 0.00013473
    Epoch:   450     LossContext: 0.00013365
    Epoch:   460     LossContext: 0.00013258
    Epoch:   470     LossContext: 0.00013164
    Epoch:   480     LossContext: 0.00013077
    Epoch:   490     LossContext: 0.00012979
    Epoch:   500     LossContext: 0.00012907
    Epoch:   510     LossContext: 0.00012825
    Epoch:   520     LossContext: 0.00012746
    Epoch:   530     LossContext: 0.00012676
    Epoch:   540     LossContext: 0.00012607
    Epoch:   550     LossContext: 0.00012548
    Epoch:   560     LossContext: 0.00012485
    Epoch:   570     LossContext: 0.00012421
    Epoch:   580     LossContext: 0.00012363
    Epoch:   590     LossContext: 0.00012309
    Epoch:   600     LossContext: 0.00012256
    Epoch:   610     LossContext: 0.00012212
    Epoch:   620     LossContext: 0.00012164
    Epoch:   630     LossContext: 0.00012114
    Epoch:   640     LossContext: 0.00012070
    Epoch:   650     LossContext: 0.00012030
    Epoch:   660     LossContext: 0.00011981
    Epoch:   670     LossContext: 0.00011941
    Epoch:   680     LossContext: 0.00011902
    Epoch:   690     LossContext: 0.00011865
    Epoch:   700     LossContext: 0.00011829
    Epoch:   710     LossContext: 0.00011793
    Epoch:   720     LossContext: 0.00011756
    Epoch:   730     LossContext: 0.00011724
    Epoch:   740     LossContext: 0.00011692
    Epoch:   750     LossContext: 0.00011663
    Epoch:   760     LossContext: 0.00011629
    Epoch:   770     LossContext: 0.00011603
    Epoch:   780     LossContext: 0.00011580
    Epoch:   790     LossContext: 0.00011549
    Epoch:   800     LossContext: 0.00011520
    Epoch:   810     LossContext: 0.00011498
    Epoch:   820     LossContext: 0.00011474
    Epoch:   830     LossContext: 0.00011444
    Epoch:   840     LossContext: 0.00011431
    Epoch:   850     LossContext: 0.00011401
    Epoch:   860     LossContext: 0.00011380
    Epoch:   870     LossContext: 0.00011352
    Epoch:   880     LossContext: 0.00011336
    Epoch:   890     LossContext: 0.00011318
    Epoch:   900     LossContext: 0.00011295
    Epoch:   910     LossContext: 0.00011268
    Epoch:   920     LossContext: 0.00011250
    Epoch:   930     LossContext: 0.00011228
    Epoch:   940     LossContext: 0.00011207
    Epoch:   950     LossContext: 0.00011188
    Epoch:   960     LossContext: 0.00011175
    Epoch:   970     LossContext: 0.00011150
    Epoch:   980     LossContext: 0.00011131
    Epoch:   990     LossContext: 0.00011116
    Epoch:  1000     LossContext: 0.00011098
    Epoch:  1010     LossContext: 0.00011083
    Epoch:  1020     LossContext: 0.00011068
    Epoch:  1030     LossContext: 0.00011048
    Epoch:  1040     LossContext: 0.00011033
    Epoch:  1050     LossContext: 0.00011018
    Epoch:  1060     LossContext: 0.00011002
    Epoch:  1070     LossContext: 0.00010983
    Epoch:  1080     LossContext: 0.00010968
    Epoch:  1090     LossContext: 0.00010954
    Epoch:  1100     LossContext: 0.00010935
    Epoch:  1110     LossContext: 0.00010925
    Epoch:  1120     LossContext: 0.00010903
    Epoch:  1130     LossContext: 0.00010895
    Epoch:  1140     LossContext: 0.00010879
    Epoch:  1150     LossContext: 0.00010864
    Epoch:  1160     LossContext: 0.00010847
    Epoch:  1170     LossContext: 0.00010830
    Epoch:  1180     LossContext: 0.00010825
    Epoch:  1190     LossContext: 0.00010809
    Epoch:  1200     LossContext: 0.00010791
    Epoch:  1210     LossContext: 0.00010783
    Epoch:  1220     LossContext: 0.00010764
    Epoch:  1230     LossContext: 0.00010752
    Epoch:  1240     LossContext: 0.00010738
    Epoch:  1250     LossContext: 0.00010728
    Epoch:  1260     LossContext: 0.00010716
    Epoch:  1270     LossContext: 0.00010703
    Epoch:  1280     LossContext: 0.00010691
    Epoch:  1290     LossContext: 0.00010679
    Epoch:  1300     LossContext: 0.00010665
    Epoch:  1310     LossContext: 0.00010654
    Epoch:  1320     LossContext: 0.00010639
    Epoch:  1330     LossContext: 0.00010627
    Epoch:  1340     LossContext: 0.00010615
    Epoch:  1350     LossContext: 0.00010605
    Epoch:  1360     LossContext: 0.00010597
    Epoch:  1370     LossContext: 0.00010582
    Epoch:  1380     LossContext: 0.00010574
    Epoch:  1390     LossContext: 0.00010563
    Epoch:  1400     LossContext: 0.00010548
    Epoch:  1410     LossContext: 0.00010539
    Epoch:  1420     LossContext: 0.00010536
    Epoch:  1430     LossContext: 0.00010521
    Epoch:  1440     LossContext: 0.00010514
    Epoch:  1450     LossContext: 0.00010502
    Epoch:  1460     LossContext: 0.00010491
    Epoch:  1470     LossContext: 0.00010486
    Epoch:  1480     LossContext: 0.00010475
    Epoch:  1490     LossContext: 0.00010465
    Epoch:  1499     LossContext: 0.00010458

Gradient descent adaptation time: 0 hours 10 mins 4 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.00628815
    Epoch:     1     LossContext: 0.00570374
    Epoch:     2     LossContext: 0.00511039
    Epoch:     3     LossContext: 0.00451632
    Epoch:    10     LossContext: 0.00267211
    Epoch:    20     LossContext: 0.00185294
    Epoch:    30     LossContext: 0.00120693
    Epoch:    40     LossContext: 0.00072329
    Epoch:    50     LossContext: 0.00047092
    Epoch:    60     LossContext: 0.00034918
    Epoch:    70     LossContext: 0.00029239
    Epoch:    80     LossContext: 0.00025981
    Epoch:    90     LossContext: 0.00023622
    Epoch:   100     LossContext: 0.00022007
    Epoch:   110     LossContext: 0.00020817
    Epoch:   120     LossContext: 0.00019914
    Epoch:   130     LossContext: 0.00019180
    Epoch:   140     LossContext: 0.00018557
    Epoch:   150     LossContext: 0.00018037
    Epoch:   160     LossContext: 0.00017595
    Epoch:   170     LossContext: 0.00017203
    Epoch:   180     LossContext: 0.00016861
    Epoch:   190     LossContext: 0.00016568
    Epoch:   200     LossContext: 0.00016307
    Epoch:   210     LossContext: 0.00016077
    Epoch:   220     LossContext: 0.00015871
    Epoch:   230     LossContext: 0.00015675
    Epoch:   240     LossContext: 0.00015504
    Epoch:   250     LossContext: 0.00015344
    Epoch:   260     LossContext: 0.00015201
    Epoch:   270     LossContext: 0.00015065
    Epoch:   280     LossContext: 0.00014946
    Epoch:   290     LossContext: 0.00014827
    Epoch:   300     LossContext: 0.00014714
    Epoch:   310     LossContext: 0.00014611
    Epoch:   320     LossContext: 0.00014518
    Epoch:   330     LossContext: 0.00014423
    Epoch:   340     LossContext: 0.00014337
    Epoch:   350     LossContext: 0.00014254
    Epoch:   360     LossContext: 0.00014175
    Epoch:   370     LossContext: 0.00014102
    Epoch:   380     LossContext: 0.00014031
    Epoch:   390     LossContext: 0.00013968
    Epoch:   400     LossContext: 0.00013898
    Epoch:   410     LossContext: 0.00013842
    Epoch:   420     LossContext: 0.00013778
    Epoch:   430     LossContext: 0.00013723
    Epoch:   440     LossContext: 0.00013670
    Epoch:   450     LossContext: 0.00013618
    Epoch:   460     LossContext: 0.00013565
    Epoch:   470     LossContext: 0.00013519
    Epoch:   480     LossContext: 0.00013474
    Epoch:   490     LossContext: 0.00013432
    Epoch:   500     LossContext: 0.00013389
    Epoch:   510     LossContext: 0.00013351
    Epoch:   520     LossContext: 0.00013315
    Epoch:   530     LossContext: 0.00013275
    Epoch:   540     LossContext: 0.00013240
    Epoch:   550     LossContext: 0.00013209
    Epoch:   560     LossContext: 0.00013174
    Epoch:   570     LossContext: 0.00013142
    Epoch:   580     LossContext: 0.00013112
    Epoch:   590     LossContext: 0.00013085
    Epoch:   600     LossContext: 0.00013055
    Epoch:   610     LossContext: 0.00013026
    Epoch:   620     LossContext: 0.00013001
    Epoch:   630     LossContext: 0.00012973
    Epoch:   640     LossContext: 0.00012949
    Epoch:   650     LossContext: 0.00012928
    Epoch:   660     LossContext: 0.00012909
    Epoch:   670     LossContext: 0.00012882
    Epoch:   680     LossContext: 0.00012862
    Epoch:   690     LossContext: 0.00012841
    Epoch:   700     LossContext: 0.00012822
    Epoch:   710     LossContext: 0.00012803
    Epoch:   720     LossContext: 0.00012782
    Epoch:   730     LossContext: 0.00012765
    Epoch:   740     LossContext: 0.00012744
    Epoch:   750     LossContext: 0.00012727
    Epoch:   760     LossContext: 0.00012709
    Epoch:   770     LossContext: 0.00012693
    Epoch:   780     LossContext: 0.00012674
    Epoch:   790     LossContext: 0.00012660
    Epoch:   800     LossContext: 0.00012642
    Epoch:   810     LossContext: 0.00012627
    Epoch:   820     LossContext: 0.00012613
    Epoch:   830     LossContext: 0.00012595
    Epoch:   840     LossContext: 0.00012581
    Epoch:   850     LossContext: 0.00012565
    Epoch:   860     LossContext: 0.00012554
    Epoch:   870     LossContext: 0.00012538
    Epoch:   880     LossContext: 0.00012524
    Epoch:   890     LossContext: 0.00012511
    Epoch:   900     LossContext: 0.00012496
    Epoch:   910     LossContext: 0.00012483
    Epoch:   920     LossContext: 0.00012471
    Epoch:   930     LossContext: 0.00012458
    Epoch:   940     LossContext: 0.00012447
    Epoch:   950     LossContext: 0.00012434
    Epoch:   960     LossContext: 0.00012423
    Epoch:   970     LossContext: 0.00012410
    Epoch:   980     LossContext: 0.00012398
    Epoch:   990     LossContext: 0.00012388
    Epoch:  1000     LossContext: 0.00012374
    Epoch:  1010     LossContext: 0.00012365
    Epoch:  1020     LossContext: 0.00012354
    Epoch:  1030     LossContext: 0.00012343
    Epoch:  1040     LossContext: 0.00012335
    Epoch:  1050     LossContext: 0.00012326
    Epoch:  1060     LossContext: 0.00012313
    Epoch:  1070     LossContext: 0.00012305
    Epoch:  1080     LossContext: 0.00012296
    Epoch:  1090     LossContext: 0.00012286
    Epoch:  1100     LossContext: 0.00012276
    Epoch:  1110     LossContext: 0.00012268
    Epoch:  1120     LossContext: 0.00012258
    Epoch:  1130     LossContext: 0.00012252
    Epoch:  1140     LossContext: 0.00012241
    Epoch:  1150     LossContext: 0.00012233
    Epoch:  1160     LossContext: 0.00012224
    Epoch:  1170     LossContext: 0.00012216
    Epoch:  1180     LossContext: 0.00012209
    Epoch:  1190     LossContext: 0.00012200
    Epoch:  1200     LossContext: 0.00012189
    Epoch:  1210     LossContext: 0.00012185
    Epoch:  1220     LossContext: 0.00012175
    Epoch:  1230     LossContext: 0.00012168
    Epoch:  1240     LossContext: 0.00012163
    Epoch:  1250     LossContext: 0.00012155
    Epoch:  1260     LossContext: 0.00012147
    Epoch:  1270     LossContext: 0.00012140
    Epoch:  1280     LossContext: 0.00012137
    Epoch:  1290     LossContext: 0.00012125
    Epoch:  1300     LossContext: 0.00012120
    Epoch:  1310     LossContext: 0.00012113
    Epoch:  1320     LossContext: 0.00012104
    Epoch:  1330     LossContext: 0.00012100
    Epoch:  1340     LossContext: 0.00012091
    Epoch:  1350     LossContext: 0.00012085
    Epoch:  1360     LossContext: 0.00012078
    Epoch:  1370     LossContext: 0.00012073
    Epoch:  1380     LossContext: 0.00012068
    Epoch:  1390     LossContext: 0.00012060
    Epoch:  1400     LossContext: 0.00012054
    Epoch:  1410     LossContext: 0.00012049
    Epoch:  1420     LossContext: 0.00012043
    Epoch:  1430     LossContext: 0.00012035
    Epoch:  1440     LossContext: 0.00012031
    Epoch:  1450     LossContext: 0.00012025
    Epoch:  1460     LossContext: 0.00012021
    Epoch:  1470     LossContext: 0.00012013
    Epoch:  1480     LossContext: 0.00012012
    Epoch:  1490     LossContext: 0.00012005
    Epoch:  1499     LossContext: 0.00012000

Gradient descent adaptation time: 0 hours 9 mins 30 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.02160805
    Epoch:     1     LossContext: 0.02040173
    Epoch:     2     LossContext: 0.01912037
    Epoch:     3     LossContext: 0.01776053
    Epoch:    10     LossContext: 0.00734169
    Epoch:    20     LossContext: 0.00287238
    Epoch:    30     LossContext: 0.00189796
    Epoch:    40     LossContext: 0.00121477
    Epoch:    50     LossContext: 0.00105647
    Epoch:    60     LossContext: 0.00084756
    Epoch:    70     LossContext: 0.00070258
    Epoch:    80     LossContext: 0.00056593
    Epoch:    90     LossContext: 0.00045747
    Epoch:   100     LossContext: 0.00037320
    Epoch:   110     LossContext: 0.00031190
    Epoch:   120     LossContext: 0.00026605
    Epoch:   130     LossContext: 0.00023125
    Epoch:   140     LossContext: 0.00020491
    Epoch:   150     LossContext: 0.00018487
    Epoch:   160     LossContext: 0.00016945
    Epoch:   170     LossContext: 0.00015730
    Epoch:   180     LossContext: 0.00014758
    Epoch:   190     LossContext: 0.00013978
    Epoch:   200     LossContext: 0.00013335
    Epoch:   210     LossContext: 0.00012808
    Epoch:   220     LossContext: 0.00012370
    Epoch:   230     LossContext: 0.00012010
    Epoch:   240     LossContext: 0.00011701
    Epoch:   250     LossContext: 0.00011438
    Epoch:   260     LossContext: 0.00011217
    Epoch:   270     LossContext: 0.00011020
    Epoch:   280     LossContext: 0.00010854
    Epoch:   290     LossContext: 0.00010709
    Epoch:   300     LossContext: 0.00010583
    Epoch:   310     LossContext: 0.00010468
    Epoch:   320     LossContext: 0.00010365
    Epoch:   330     LossContext: 0.00010271
    Epoch:   340     LossContext: 0.00010191
    Epoch:   350     LossContext: 0.00010116
    Epoch:   360     LossContext: 0.00010047
    Epoch:   370     LossContext: 0.00009986
    Epoch:   380     LossContext: 0.00009931
    Epoch:   390     LossContext: 0.00009878
    Epoch:   400     LossContext: 0.00009829
    Epoch:   410     LossContext: 0.00009785
    Epoch:   420     LossContext: 0.00009743
    Epoch:   430     LossContext: 0.00009707
    Epoch:   440     LossContext: 0.00009669
    Epoch:   450     LossContext: 0.00009639
    Epoch:   460     LossContext: 0.00009605
    Epoch:   470     LossContext: 0.00009575
    Epoch:   480     LossContext: 0.00009548
    Epoch:   490     LossContext: 0.00009521
    Epoch:   500     LossContext: 0.00009494
    Epoch:   510     LossContext: 0.00009470
    Epoch:   520     LossContext: 0.00009449
    Epoch:   530     LossContext: 0.00009424
    Epoch:   540     LossContext: 0.00009403
    Epoch:   550     LossContext: 0.00009381
    Epoch:   560     LossContext: 0.00009363
    Epoch:   570     LossContext: 0.00009342
    Epoch:   580     LossContext: 0.00009322
    Epoch:   590     LossContext: 0.00009305
    Epoch:   600     LossContext: 0.00009288
    Epoch:   610     LossContext: 0.00009270
    Epoch:   620     LossContext: 0.00009253
    Epoch:   630     LossContext: 0.00009236
    Epoch:   640     LossContext: 0.00009221
    Epoch:   650     LossContext: 0.00009204
    Epoch:   660     LossContext: 0.00009186
    Epoch:   670     LossContext: 0.00009172
    Epoch:   680     LossContext: 0.00009156
    Epoch:   690     LossContext: 0.00009143
    Epoch:   700     LossContext: 0.00009126
    Epoch:   710     LossContext: 0.00009113
    Epoch:   720     LossContext: 0.00009097
    Epoch:   730     LossContext: 0.00009084
    Epoch:   740     LossContext: 0.00009070
    Epoch:   750     LossContext: 0.00009056
    Epoch:   760     LossContext: 0.00009044
    Epoch:   770     LossContext: 0.00009029
    Epoch:   780     LossContext: 0.00009017
    Epoch:   790     LossContext: 0.00009004
    Epoch:   800     LossContext: 0.00008991
    Epoch:   810     LossContext: 0.00008978
    Epoch:   820     LossContext: 0.00008966
    Epoch:   830     LossContext: 0.00008957
    Epoch:   840     LossContext: 0.00008943
    Epoch:   850     LossContext: 0.00008930
    Epoch:   860     LossContext: 0.00008920
    Epoch:   870     LossContext: 0.00008907
    Epoch:   880     LossContext: 0.00008897
    Epoch:   890     LossContext: 0.00008884
    Epoch:   900     LossContext: 0.00008873
    Epoch:   910     LossContext: 0.00008863
    Epoch:   920     LossContext: 0.00008851
    Epoch:   930     LossContext: 0.00008840
    Epoch:   940     LossContext: 0.00008830
    Epoch:   950     LossContext: 0.00008820
    Epoch:   960     LossContext: 0.00008807
    Epoch:   970     LossContext: 0.00008796
    Epoch:   980     LossContext: 0.00008787
    Epoch:   990     LossContext: 0.00008776
    Epoch:  1000     LossContext: 0.00008767
    Epoch:  1010     LossContext: 0.00008758
    Epoch:  1020     LossContext: 0.00008748
    Epoch:  1030     LossContext: 0.00008737
    Epoch:  1040     LossContext: 0.00008729
    Epoch:  1050     LossContext: 0.00008720
    Epoch:  1060     LossContext: 0.00008711
    Epoch:  1070     LossContext: 0.00008699
    Epoch:  1080     LossContext: 0.00008691
    Epoch:  1090     LossContext: 0.00008683
    Epoch:  1100     LossContext: 0.00008674
    Epoch:  1110     LossContext: 0.00008667
    Epoch:  1120     LossContext: 0.00008659
    Epoch:  1130     LossContext: 0.00008649
    Epoch:  1140     LossContext: 0.00008643
    Epoch:  1150     LossContext: 0.00008636
    Epoch:  1160     LossContext: 0.00008626
    Epoch:  1170     LossContext: 0.00008619
    Epoch:  1180     LossContext: 0.00008612
    Epoch:  1190     LossContext: 0.00008604
    Epoch:  1200     LossContext: 0.00008597
    Epoch:  1210     LossContext: 0.00008590
    Epoch:  1220     LossContext: 0.00008583
    Epoch:  1230     LossContext: 0.00008575
    Epoch:  1240     LossContext: 0.00008569
    Epoch:  1250     LossContext: 0.00008564
    Epoch:  1260     LossContext: 0.00008556
    Epoch:  1270     LossContext: 0.00008550
    Epoch:  1280     LossContext: 0.00008544
    Epoch:  1290     LossContext: 0.00008536
    Epoch:  1300     LossContext: 0.00008531
    Epoch:  1310     LossContext: 0.00008523
    Epoch:  1320     LossContext: 0.00008517
    Epoch:  1330     LossContext: 0.00008511
    Epoch:  1340     LossContext: 0.00008505
    Epoch:  1350     LossContext: 0.00008499
    Epoch:  1360     LossContext: 0.00008493
    Epoch:  1370     LossContext: 0.00008486
    Epoch:  1380     LossContext: 0.00008480
    Epoch:  1390     LossContext: 0.00008475
    Epoch:  1400     LossContext: 0.00008469
    Epoch:  1410     LossContext: 0.00008463
    Epoch:  1420     LossContext: 0.00008459
    Epoch:  1430     LossContext: 0.00008451
    Epoch:  1440     LossContext: 0.00008447
    Epoch:  1450     LossContext: 0.00008443
    Epoch:  1460     LossContext: 0.00008435
    Epoch:  1470     LossContext: 0.00008432
    Epoch:  1480     LossContext: 0.00008425
    Epoch:  1490     LossContext: 0.00008420
    Epoch:  1499     LossContext: 0.00008416

Gradient descent adaptation time: 0 hours 9 mins 32 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./runs/06042024-102728/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 4
    Number of adaptation environments: 4
    Final length of the training trajectories: 10
    Length of the testing trajectories: 10
Test Score (OOD): 0.0034027726

Couldn't get a file descriptor referring to the console
==  Begining out-of-distribution visualisation ... ==
    Environment id: 0
    Trajectory id: 0
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 10
    Length of the testing trajectories: 10
Testing finished. Figure saved in: ./runs/06042024-102728/adapt/results_ood.png
