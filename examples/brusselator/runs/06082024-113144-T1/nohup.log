Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: train
 Savepath: ./runs/06082024-113144/
 Seed: 2026

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: test
 Savepath: ./runs/06082024-113144/
 Seed: 4052

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./runs/06082024-113144/adapt/
 Seed: 6078

Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt_test
 Savepath: ./runs/06082024-113144/adapt/
 Seed: 6078


############# Neural Context Flow #############

Jax version: 0.4.28
Available devices: [cuda(id=0)]
Run folder created successfuly: ./runs/06082024-113144/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 113159
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 113159
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 117502 


WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed


=== Beginning training ... ===
    Number of examples in a batch: 4
    Number of train steps per epoch: 1
    Number of training epochs: 1000
    Total number of training steps: 1000

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (9, 4, 20, 128) (20,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (9, 4, 20, 128) (20,)
    Epoch:     0      LossTrajs: 2.96657920     ContextsNorm: 0.00000000     ValIndCrit: 4.31228971
    Epoch:     1      LossTrajs: 2.96535134     ContextsNorm: 0.00023681     ValIndCrit: 4.31180143
    Epoch:     2      LossTrajs: 2.96394992     ContextsNorm: 0.00024275     ValIndCrit: 4.31123734
    Epoch:     3      LossTrajs: 2.96225572     ContextsNorm: 0.00019675     ValIndCrit: 4.31054926
    Epoch:    10      LossTrajs: 2.92379498     ContextsNorm: 0.00057306     ValIndCrit: 4.29780436
    Epoch:    20      LossTrajs: 2.57428408     ContextsNorm: 0.00263551     ValIndCrit: 4.30576468
    Epoch:    30      LossTrajs: 2.22354937     ContextsNorm: 0.00450833     ValIndCrit: 4.65454865
    Epoch:    40      LossTrajs: 2.09698176     ContextsNorm: 0.00957086     ValIndCrit: 4.07350349
    Epoch:    50      LossTrajs: 2.02361131     ContextsNorm: 0.01785900     ValIndCrit: 3.95325017
    Epoch:    60      LossTrajs: 1.92394316     ContextsNorm: 0.02851897     ValIndCrit: 3.58989882
    Epoch:    70      LossTrajs: 1.44944310     ContextsNorm: 0.03673231     ValIndCrit: 2.77982688
    Epoch:    80      LossTrajs: 1.23163891     ContextsNorm: 0.04181819     ValIndCrit: 2.61010671
    Epoch:    90      LossTrajs: 1.15972185     ContextsNorm: 0.03915162     ValIndCrit: 2.47098255
    Epoch:   100      LossTrajs: 1.14101267     ContextsNorm: 0.03746340     ValIndCrit: 2.42717218
    Epoch:   110      LossTrajs: 1.10888994     ContextsNorm: 0.03784049     ValIndCrit: 2.45538425
    Epoch:   120      LossTrajs: 1.08592725     ContextsNorm: 0.03751643     ValIndCrit: 2.45937395
    Epoch:   130      LossTrajs: 1.06275368     ContextsNorm: 0.03690989     ValIndCrit: 2.33357739
    Epoch:   140      LossTrajs: 1.04056048     ContextsNorm: 0.03637589     ValIndCrit: 2.14517713
    Epoch:   150      LossTrajs: 1.02246106     ContextsNorm: 0.03577185     ValIndCrit: 2.07877016
    Epoch:   160      LossTrajs: 1.00847125     ContextsNorm: 0.03531240     ValIndCrit: 2.04833293
    Epoch:   170      LossTrajs: 0.99709594     ContextsNorm: 0.03491741     ValIndCrit: 2.03337598
    Epoch:   180      LossTrajs: 0.98813587     ContextsNorm: 0.03447260     ValIndCrit: 2.00505114
    Epoch:   190      LossTrajs: 0.98089874     ContextsNorm: 0.03416288     ValIndCrit: 2.01543164
    Epoch:   200      LossTrajs: 0.97485483     ContextsNorm: 0.03385143     ValIndCrit: 1.98686838
    Epoch:   210      LossTrajs: 0.96998936     ContextsNorm: 0.03356523     ValIndCrit: 1.97349131
    Epoch:   220      LossTrajs: 0.96539342     ContextsNorm: 0.03328414     ValIndCrit: 1.95794153
    Epoch:   230      LossTrajs: 0.96104252     ContextsNorm: 0.03304978     ValIndCrit: 1.92665315
    Epoch:   240      LossTrajs: 0.95676869     ContextsNorm: 0.03284739     ValIndCrit: 1.89069140
    Epoch:   250      LossTrajs: 0.95344126     ContextsNorm: 0.03254027     ValIndCrit: 1.82145274
    Epoch:   260      LossTrajs: 0.95419341     ContextsNorm: 0.03221190     ValIndCrit: 1.82717705
    Epoch:   270      LossTrajs: 0.94905895     ContextsNorm: 0.03197826     ValIndCrit: 1.81359529
    Epoch:   280      LossTrajs: 0.94513828     ContextsNorm: 0.03189097     ValIndCrit: 1.80807364
    Epoch:   290      LossTrajs: 0.94090295     ContextsNorm: 0.03188140     ValIndCrit: 1.81428909
    Epoch:   300      LossTrajs: 0.93603939     ContextsNorm: 0.03190421     ValIndCrit: 1.84237289
    Epoch:   310      LossTrajs: 0.92882490     ContextsNorm: 0.03194761     ValIndCrit: 1.89721048
    Epoch:   320      LossTrajs: 0.92254478     ContextsNorm: 0.03204147     ValIndCrit: 2.02482724
    Epoch:   330      LossTrajs: 0.90949249     ContextsNorm: 0.03209807     ValIndCrit: 2.21019578
    Epoch:   340      LossTrajs: 0.89343095     ContextsNorm: 0.03189926     ValIndCrit: 2.58091545
    Epoch:   350      LossTrajs: 0.88207883     ContextsNorm: 0.03166011     ValIndCrit: 2.73204660
    Epoch:   360      LossTrajs: 0.87450892     ContextsNorm: 0.03160124     ValIndCrit: 2.75260019
    Epoch:   370      LossTrajs: 0.86180109     ContextsNorm: 0.03161482     ValIndCrit: 2.70504475
    Epoch:   380      LossTrajs: 0.85482115     ContextsNorm: 0.03231253     ValIndCrit: 2.55574155
    Epoch:   390      LossTrajs: 0.83771813     ContextsNorm: 0.03274770     ValIndCrit: 2.47932744
    Epoch:   400      LossTrajs: 0.81228477     ContextsNorm: 0.03269789     ValIndCrit: 2.33018184
    Epoch:   410      LossTrajs: 0.74241173     ContextsNorm: 0.03238569     ValIndCrit: 2.21778035
    Epoch:   420      LossTrajs: 0.59613025     ContextsNorm: 0.03223052     ValIndCrit: 1.99521983
    Epoch:   430      LossTrajs: 0.56869763     ContextsNorm: 0.03201789     ValIndCrit: 1.50007832
    Epoch:   440      LossTrajs: 0.43038845     ContextsNorm: 0.03124227     ValIndCrit: 1.53070521
    Epoch:   450      LossTrajs: 0.34965289     ContextsNorm: 0.03077942     ValIndCrit: 1.24966455
    Epoch:   460      LossTrajs: 0.32316777     ContextsNorm: 0.03060853     ValIndCrit: 1.24548018
    Epoch:   470      LossTrajs: 0.29961997     ContextsNorm: 0.03073992     ValIndCrit: 1.09935033
    Epoch:   480      LossTrajs: 0.28382558     ContextsNorm: 0.03084231     ValIndCrit: 1.10385644
    Epoch:   490      LossTrajs: 0.27202669     ContextsNorm: 0.03073433     ValIndCrit: 1.04722846
    Epoch:   500      LossTrajs: 0.27099732     ContextsNorm: 0.03097088     ValIndCrit: 1.00335383
    Epoch:   510      LossTrajs: 0.25826737     ContextsNorm: 0.03109544     ValIndCrit: 1.05550420
    Epoch:   520      LossTrajs: 0.24639836     ContextsNorm: 0.03118435     ValIndCrit: 1.03016508
    Epoch:   530      LossTrajs: 0.24036548     ContextsNorm: 0.03134571     ValIndCrit: 1.05299759
    Epoch:   540      LossTrajs: 0.24201030     ContextsNorm: 0.03141923     ValIndCrit: 1.03330886
    Epoch:   550      LossTrajs: 0.23135668     ContextsNorm: 0.03137035     ValIndCrit: 0.97625691
    Epoch:   560      LossTrajs: 0.22712240     ContextsNorm: 0.03144689     ValIndCrit: 1.03701997
    Epoch:   570      LossTrajs: 0.24099165     ContextsNorm: 0.03162132     ValIndCrit: 1.03161657
    Epoch:   580      LossTrajs: 0.21940668     ContextsNorm: 0.03180945     ValIndCrit: 1.06839907
    Epoch:   590      LossTrajs: 0.21575941     ContextsNorm: 0.03212020     ValIndCrit: 1.15427148
    Epoch:   600      LossTrajs: 0.21617991     ContextsNorm: 0.03232382     ValIndCrit: 1.03740501
    Epoch:   610      LossTrajs: 0.20826824     ContextsNorm: 0.03243298     ValIndCrit: 1.00159574
    Epoch:   620      LossTrajs: 0.19777325     ContextsNorm: 0.03257968     ValIndCrit: 1.01774454
    Epoch:   630      LossTrajs: 0.19162787     ContextsNorm: 0.03295825     ValIndCrit: 0.98145020
    Epoch:   640      LossTrajs: 0.19832063     ContextsNorm: 0.03306622     ValIndCrit: 0.90631759
    Epoch:   650      LossTrajs: 0.19452924     ContextsNorm: 0.03324162     ValIndCrit: 0.95209694
    Epoch:   660      LossTrajs: 0.19676659     ContextsNorm: 0.03335970     ValIndCrit: 0.99014568
    Epoch:   670      LossTrajs: 0.17497967     ContextsNorm: 0.03345307     ValIndCrit: 0.94564968
    Epoch:   680      LossTrajs: 0.18962446     ContextsNorm: 0.03361452     ValIndCrit: 1.01400328
    Epoch:   690      LossTrajs: 0.17346096     ContextsNorm: 0.03393580     ValIndCrit: 0.98491806
    Epoch:   700      LossTrajs: 0.18014570     ContextsNorm: 0.03418161     ValIndCrit: 0.93882698
    Epoch:   710      LossTrajs: 0.24720085     ContextsNorm: 0.03453652     ValIndCrit: 1.12939417
    Epoch:   720      LossTrajs: 0.22549307     ContextsNorm: 0.03509014     ValIndCrit: 1.21247077
    Epoch:   730      LossTrajs: 0.17601101     ContextsNorm: 0.03522171     ValIndCrit: 0.95967793
    Epoch:   740      LossTrajs: 0.17046872     ContextsNorm: 0.03543448     ValIndCrit: 0.92778760
    Epoch:   750      LossTrajs: 0.15981305     ContextsNorm: 0.03551470     ValIndCrit: 0.89026535
    Epoch:   760      LossTrajs: 0.15443559     ContextsNorm: 0.03569363     ValIndCrit: 0.99846375
    Epoch:   770      LossTrajs: 0.15777779     ContextsNorm: 0.03621364     ValIndCrit: 0.99715477
    Epoch:   780      LossTrajs: 0.14755981     ContextsNorm: 0.03649063     ValIndCrit: 0.91098148
    Epoch:   790      LossTrajs: 0.14538655     ContextsNorm: 0.03675451     ValIndCrit: 0.94636518
    Epoch:   800      LossTrajs: 0.14310724     ContextsNorm: 0.03696182     ValIndCrit: 0.97342992
    Epoch:   810      LossTrajs: 0.13475455     ContextsNorm: 0.03716468     ValIndCrit: 1.01033652
    Epoch:   820      LossTrajs: 0.13378830     ContextsNorm: 0.03734578     ValIndCrit: 0.98966491
    Epoch:   830      LossTrajs: 0.13099739     ContextsNorm: 0.03749011     ValIndCrit: 0.99386436
    Epoch:   840      LossTrajs: 0.13304797     ContextsNorm: 0.03769522     ValIndCrit: 0.99002701
    Epoch:   850      LossTrajs: 0.12490702     ContextsNorm: 0.03789361     ValIndCrit: 0.98897892
    Epoch:   860      LossTrajs: 0.12389643     ContextsNorm: 0.03798007     ValIndCrit: 1.01492202
    Epoch:   870      LossTrajs: 0.12152608     ContextsNorm: 0.03809150     ValIndCrit: 0.98846519
    Epoch:   880      LossTrajs: 0.12611116     ContextsNorm: 0.03823065     ValIndCrit: 0.97855359
    Epoch:   890      LossTrajs: 0.11898866     ContextsNorm: 0.03832072     ValIndCrit: 1.00135541
    Epoch:   900      LossTrajs: 0.11854881     ContextsNorm: 0.03850672     ValIndCrit: 0.98975497
    Epoch:   910      LossTrajs: 0.11375000     ContextsNorm: 0.03871061     ValIndCrit: 0.99481136
    Epoch:   920      LossTrajs: 0.11300386     ContextsNorm: 0.03877473     ValIndCrit: 1.01912558
    Epoch:   930      LossTrajs: 0.11508863     ContextsNorm: 0.03890702     ValIndCrit: 0.97007495
    Epoch:   940      LossTrajs: 0.11766762     ContextsNorm: 0.03913971     ValIndCrit: 1.04889488
    Epoch:   950      LossTrajs: 0.10892008     ContextsNorm: 0.03943189     ValIndCrit: 1.06637824
    Epoch:   960      LossTrajs: 0.11036757     ContextsNorm: 0.03950350     ValIndCrit: 1.05086303
    Epoch:   970      LossTrajs: 0.10348183     ContextsNorm: 0.03961186     ValIndCrit: 1.06230223
    Epoch:   980      LossTrajs: 0.10886505     ContextsNorm: 0.03970917     ValIndCrit: 1.18518364
    Epoch:   990      LossTrajs: 0.10860290     ContextsNorm: 0.03985561     ValIndCrit: 2.56400990
    Epoch:   999      LossTrajs: 0.10964330     ContextsNorm: 0.04004443     ValIndCrit: 74.39258575

Total gradient descent training time: 0 hours 9 mins 7 secs
Environment weights at the end of the training: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111
 0.11111111 0.11111111 0.11111111]
WARNING: You did not provide a dataloader id. A new one has been generated: 114108
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 9
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (In-Domain): 74.392586


Per-environment IND scores: [  1.5387158    1.9571486    2.2605436    0.5166643    0.6412096
   0.89017785 658.8051       2.021557     0.90218806]
==  Begining in-domain visualisation ... ==
    Environment id: 0
    Trajectory id: 18
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Testing finished. Figure saved in: ./runs/06082024-113144/results_in_domain.png
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 20, 128) (20,)
    Epoch:     0     LossContext: 2.74624395
    Epoch:     1     LossContext: 2.74464321
    Epoch:     2     LossContext: 2.74683452
    Epoch:     3     LossContext: 2.74717212
    Epoch:   100     LossContext: 0.42621240
    Epoch:   200     LossContext: 0.42406633
    Epoch:   300     LossContext: 0.42406970
    Epoch:   400     LossContext: 0.42406714
    Epoch:   500     LossContext: 0.42406884
    Epoch:   600     LossContext: 0.42406395
    Epoch:   700     LossContext: 0.42406145
    Epoch:   800     LossContext: 0.42405429
    Epoch:   900     LossContext: 0.42405236
    Epoch:  1000     LossContext: 0.42404771
    Epoch:  1100     LossContext: 0.42405105
    Epoch:  1200     LossContext: 0.42403874
    Epoch:  1300     LossContext: 0.42403248
    Epoch:  1400     LossContext: 0.42403129
    Epoch:  1499     LossContext: 0.42402500

Gradient descent adaptation time: 0 hours 1 mins 56 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 3.11703062
    Epoch:     1     LossContext: 3.10864091
    Epoch:     2     LossContext: 3.09927011
    Epoch:     3     LossContext: 3.08471107
    Epoch:   100     LossContext: 0.19992505
    Epoch:   200     LossContext: 0.20097397
    Epoch:   300     LossContext: 0.20097409
    Epoch:   400     LossContext: 0.20097587
    Epoch:   500     LossContext: 0.20098110
    Epoch:   600     LossContext: 0.20096858
    Epoch:   700     LossContext: 0.20095909
    Epoch:   800     LossContext: 0.20095353
    Epoch:   900     LossContext: 0.20099378
    Epoch:  1000     LossContext: 0.20098099
    Epoch:  1100     LossContext: 0.20095102
    Epoch:  1200     LossContext: 0.20097204
    Epoch:  1300     LossContext: 0.20093258
    Epoch:  1400     LossContext: 0.20094338
    Epoch:  1499     LossContext: 0.20092660

Gradient descent adaptation time: 0 hours 1 mins 51 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 3.63334489
    Epoch:     1     LossContext: 3.61693883
    Epoch:     2     LossContext: 3.59859920
    Epoch:     3     LossContext: 3.61296415
    Epoch:   100     LossContext: 0.27877632
    Epoch:   200     LossContext: 0.27790949
    Epoch:   300     LossContext: 0.27790767
    Epoch:   400     LossContext: 0.27790979
    Epoch:   500     LossContext: 0.27790892
    Epoch:   600     LossContext: 0.27790976
    Epoch:   700     LossContext: 0.27791268
    Epoch:   800     LossContext: 0.27789918
    Epoch:   900     LossContext: 0.27790362
    Epoch:  1000     LossContext: 0.27790412
    Epoch:  1100     LossContext: 0.27790383
    Epoch:  1200     LossContext: 0.27789482
    Epoch:  1300     LossContext: 0.27788877
    Epoch:  1400     LossContext: 0.27789137
    Epoch:  1499     LossContext: 0.27787316

Gradient descent adaptation time: 0 hours 1 mins 51 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 5.01810122
    Epoch:     1     LossContext: 4.99883938
    Epoch:     2     LossContext: 4.97809649
    Epoch:     3     LossContext: 4.95831919
    Epoch:   100     LossContext: 0.58508873
    Epoch:   200     LossContext: 0.58484340
    Epoch:   300     LossContext: 0.58485121
    Epoch:   400     LossContext: 0.58480710
    Epoch:   500     LossContext: 0.58489507
    Epoch:   600     LossContext: 0.58484435
    Epoch:   700     LossContext: 0.58489174
    Epoch:   800     LossContext: 0.58495450
    Epoch:   900     LossContext: 0.58488852
    Epoch:  1000     LossContext: 0.58480281
    Epoch:  1100     LossContext: 0.58482373
    Epoch:  1200     LossContext: 0.58483589
    Epoch:  1300     LossContext: 0.58491772
    Epoch:  1400     LossContext: 0.58482218
    Epoch:  1499     LossContext: 0.58467764

Gradient descent adaptation time: 0 hours 1 mins 51 secs

Adapting to environment 4 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 1.31799531
    Epoch:     1     LossContext: 1.28924894
    Epoch:     2     LossContext: 1.26308262
    Epoch:     3     LossContext: 1.24241817
    Epoch:   100     LossContext: 1.21803164
    Epoch:   200     LossContext: 1.22513509
    Epoch:   300     LossContext: 1.23371720
    Epoch:   400     LossContext: 1.21571958
    Epoch:   500     LossContext: 1.19317520
    Epoch:   600     LossContext: 1.21929169
    Epoch:   700     LossContext: 1.21143341
    Epoch:   800     LossContext: 1.22779119
    Epoch:   900     LossContext: 1.24192524
    Epoch:  1000     LossContext: 1.22368288
    Epoch:  1100     LossContext: 1.20126998
    Epoch:  1200     LossContext: 1.19994605
    Epoch:  1300     LossContext: 1.20001101
    Epoch:  1400     LossContext: 1.20069933
    Epoch:  1499     LossContext: 1.20071614

Gradient descent adaptation time: 0 hours 1 mins 56 secs

Adapting to environment 5 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.61316848
    Epoch:     1     LossContext: 0.59548414
    Epoch:     2     LossContext: 0.57926661
    Epoch:     3     LossContext: 0.56719762
    Epoch:   100     LossContext: 0.51642317
    Epoch:   200     LossContext: 0.52305162
    Epoch:   300     LossContext: 0.51632220
    Epoch:   400     LossContext: 0.51582921
    Epoch:   500     LossContext: 0.51133394
    Epoch:   600     LossContext: 0.54450208
    Epoch:   700     LossContext: 0.51539642
    Epoch:   800     LossContext: 0.51848716
    Epoch:   900     LossContext: 0.54424411
    Epoch:  1000     LossContext: 0.51484263
    Epoch:  1100     LossContext: 0.52281266
    Epoch:  1200     LossContext: 0.51536542
    Epoch:  1300     LossContext: 0.51627839
    Epoch:  1400     LossContext: 0.50569767
    Epoch:  1499     LossContext: 0.52232498

Gradient descent adaptation time: 0 hours 1 mins 56 secs

Adapting to environment 6 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.69470263
    Epoch:     1     LossContext: 0.67202866
    Epoch:     2     LossContext: 0.65314132
    Epoch:     3     LossContext: 0.64501393
    Epoch:   100     LossContext: 0.63131446
    Epoch:   200     LossContext: 0.63013405
    Epoch:   300     LossContext: 0.62829125
    Epoch:   400     LossContext: 0.62632191
    Epoch:   500     LossContext: 0.62406367
    Epoch:   600     LossContext: 0.62015998
    Epoch:   700     LossContext: 0.61675501
    Epoch:   800     LossContext: 0.61279094
    Epoch:   900     LossContext: 0.60983169
    Epoch:  1000     LossContext: 0.60374224
    Epoch:  1100     LossContext: 0.60152155
    Epoch:  1200     LossContext: 0.59469593
    Epoch:  1300     LossContext: 0.59189177
    Epoch:  1400     LossContext: 0.58735210
    Epoch:  1499     LossContext: 0.58448052

Gradient descent adaptation time: 0 hours 1 mins 58 secs

Adapting to environment 7 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 2.33682847
    Epoch:     1     LossContext: 2.31229830
    Epoch:     2     LossContext: 2.29337692
    Epoch:     3     LossContext: 2.28457522
    Epoch:   100     LossContext: 2.29196739
    Epoch:   200     LossContext: 2.29382515
    Epoch:   300     LossContext: 2.29623342
    Epoch:   400     LossContext: 2.28873086
    Epoch:   500     LossContext: 2.29157853
    Epoch:   600     LossContext: 2.26670408
    Epoch:   700     LossContext: 2.27772641
    Epoch:   800     LossContext: 2.25359821
    Epoch:   900     LossContext: 2.26942921
    Epoch:  1000     LossContext: 2.25728464
    Epoch:  1100     LossContext: 2.25497580
    Epoch:  1200     LossContext: 2.22595453
    Epoch:  1300     LossContext: 2.23409271
    Epoch:  1400     LossContext: 2.22735715
    Epoch:  1499     LossContext: 2.20683336

Gradient descent adaptation time: 0 hours 2 mins 0 secs

Adapting to environment 8 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 3.30444503
    Epoch:     1     LossContext: 3.27163053
    Epoch:     2     LossContext: 3.24008060
    Epoch:     3     LossContext: 3.21892476
    Epoch:   100     LossContext: 0.32324168
    Epoch:   200     LossContext: 0.32297477
    Epoch:   300     LossContext: 0.32260424
    Epoch:   400     LossContext: 0.32395706
    Epoch:   500     LossContext: 0.32539475
    Epoch:   600     LossContext: 0.32267246
    Epoch:   700     LossContext: 0.32606757
    Epoch:   800     LossContext: 0.32452768
    Epoch:   900     LossContext: 0.32313341
    Epoch:  1000     LossContext: 0.32302061
    Epoch:  1100     LossContext: 0.31722108
    Epoch:  1200     LossContext: 0.32304066
    Epoch:  1300     LossContext: 0.32357612
    Epoch:  1400     LossContext: 0.32535738
    Epoch:  1499     LossContext: 0.32415682

Gradient descent adaptation time: 0 hours 1 mins 42 secs

Adapting to environment 9 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 3.11147714
    Epoch:     1     LossContext: 3.07861900
    Epoch:     2     LossContext: 3.04803586
    Epoch:     3     LossContext: 3.02523851
    Epoch:   100     LossContext: 0.16054925
    Epoch:   200     LossContext: 0.14961739
    Epoch:   300     LossContext: 0.14412111
    Epoch:   400     LossContext: 0.15235040
    Epoch:   500     LossContext: 0.15034452
    Epoch:   600     LossContext: 0.15228814
    Epoch:   700     LossContext: 0.14469317
    Epoch:   800     LossContext: 0.15215151
    Epoch:   900     LossContext: 0.15145054
    Epoch:  1000     LossContext: 0.15271357
    Epoch:  1100     LossContext: 0.15098667
    Epoch:  1200     LossContext: 0.15313150
    Epoch:  1300     LossContext: 0.15325862
    Epoch:  1400     LossContext: 0.14151773
    Epoch:  1499     LossContext: 0.15201482

Gradient descent adaptation time: 0 hours 1 mins 46 secs

Adapting to environment 10 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 2.66328001
    Epoch:     1     LossContext: 2.63128686
    Epoch:     2     LossContext: 2.60132575
    Epoch:     3     LossContext: 2.58617473
    Epoch:   100     LossContext: 2.21331525
    Epoch:   200     LossContext: 0.05586078
    Epoch:   300     LossContext: 0.05582514
    Epoch:   400     LossContext: 0.05646241
    Epoch:   500     LossContext: 0.05687016
    Epoch:   600     LossContext: 0.05600081
    Epoch:   700     LossContext: 0.05638592
    Epoch:   800     LossContext: 0.05590486
    Epoch:   900     LossContext: 0.05581998
    Epoch:  1000     LossContext: 0.05637611
    Epoch:  1100     LossContext: 0.05503363
    Epoch:  1200     LossContext: 0.05614716
    Epoch:  1300     LossContext: 0.05664010
    Epoch:  1400     LossContext: 0.05605916
    Epoch:  1499     LossContext: 0.05621136

Gradient descent adaptation time: 0 hours 1 mins 51 secs

Adapting to environment 11 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 1.73215771
    Epoch:     1     LossContext: 1.69999874
    Epoch:     2     LossContext: 1.67267776
    Epoch:     3     LossContext: 1.65726471
    Epoch:   100     LossContext: 1.63406610
    Epoch:   200     LossContext: 1.62540829
    Epoch:   300     LossContext: 1.62036371
    Epoch:   400     LossContext: 1.62257540
    Epoch:   500     LossContext: 1.62864459
    Epoch:   600     LossContext: 1.66994131
    Epoch:   700     LossContext: 1.62385774
    Epoch:   800     LossContext: 1.62028801
    Epoch:   900     LossContext: 1.61600327
    Epoch:  1000     LossContext: 1.62769043
    Epoch:  1100     LossContext: 1.67527723
    Epoch:  1200     LossContext: 1.61524630
    Epoch:  1300     LossContext: 1.66928649
    Epoch:  1400     LossContext: 1.61499250
    Epoch:  1499     LossContext: 1.61009395

Gradient descent adaptation time: 0 hours 1 mins 54 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./runs/06082024-113144/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 9
    Number of adaptation environments: 12
    Final length of the training trajectories: 20
    Length of the testing trajectories: 20
Test Score (OOD): 1799.3384

