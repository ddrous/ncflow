
############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.14
Available devices: [gpu(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: train
 Savepath: ./runs/04042024-165703/
 Seed: 2


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.14
Available devices: [gpu(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: test
 Savepath: ./runs/04042024-165703/
 Seed: 4


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.14
Available devices: [gpu(id=0)]
Run folder created successfuly: ./runs/04042024-165703/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 165709
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 165709
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 50000 


WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed


=== Beginning training with proximal alternating minimization ... ===
    Number of examples in a batch: 4
    Maximum number of steps per inner minimization: 20
    Maximum number of outer minimizations: 1000
    Maximum total number of training steps: 20000

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (21, 4, 11, 2) (11,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (21, 4, 11, 2) (11,)
    Outer Step:     0      LossTrajs: 2.42564392     ContextsNorm: 0.00000000     ValIndCrit: 2.36502194
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 1.61e-06
        -DiffCxt:  inf
    Outer Step:     1      LossTrajs: 1.81051958     ContextsNorm: 0.00130751     ValIndCrit: 1.99852061
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 1.24e-06
        -DiffCxt:  1.43e-03
    Outer Step:     2      LossTrajs: 1.55746019     ContextsNorm: 0.00256430     ValIndCrit: 1.84756911
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 1.10e-06
        -DiffCxt:  5.53e-04
    Outer Step:     3      LossTrajs: 1.47785676     ContextsNorm: 0.00416165     ValIndCrit: 1.83418655
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 1.41e-06
        -DiffCxt:  4.08e-04
    Outer Step:    25      LossTrajs: 0.12114388     ContextsNorm: 0.02586514     ValIndCrit: 0.16412359
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 6.16e-08
        -DiffCxt:  5.00e-08
    Outer Step:    50      LossTrajs: 0.10142140     ContextsNorm: 0.02594828     ValIndCrit: 0.11906123
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 6.24e-09
        -DiffCxt:  2.87e-08
    Outer Step:    75      LossTrajs: 0.09931958     ContextsNorm: 0.02594980     ValIndCrit: 0.11632626
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 3.47e-09
        -DiffCxt:  2.63e-08
    Outer Step:   100      LossTrajs: 0.09830894     ContextsNorm: 0.02605824     ValIndCrit: 0.11618038
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 4.44e-09
        -DiffCxt:  4.93e-08
    Outer Step:   125      LossTrajs: 0.06641571     ContextsNorm: 0.02721458     ValIndCrit: 0.08345141
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 1.74e-07
        -DiffCxt:  1.68e-06
    Outer Step:   150      LossTrajs: 0.06160191     ContextsNorm: 0.02681581     ValIndCrit: 0.07808471
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 1.41e-08
        -DiffCxt:  4.28e-08
    Outer Step:   175      LossTrajs: 0.05729798     ContextsNorm: 0.02784645     ValIndCrit: 0.07481433
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 1.30e-07
        -DiffCxt:  1.75e-07
    Outer Step:   200      LossTrajs: 0.04478018     ContextsNorm: 0.02831317     ValIndCrit: 0.07104044
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 2.40e-08
        -DiffCxt:  1.66e-07
    Outer Step:   225      LossTrajs: 0.03880067     ContextsNorm: 0.02932689     ValIndCrit: 0.06694858
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 3.96e-08
        -DiffCxt:  2.24e-07
    Outer Step:   250      LossTrajs: 0.03441998     ContextsNorm: 0.02870108     ValIndCrit: 0.05854896
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 3.35e-09
        -DiffCxt:  1.41e-07
    Outer Step:   275      LossTrajs: 0.07849992     ContextsNorm: 0.03123344     ValIndCrit: 0.08963274
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 2.26e-07
        -DiffCxt:  3.08e-06
    Outer Step:   300      LossTrajs: 0.15815137     ContextsNorm: 0.03365937     ValIndCrit: 0.16793224
        -NbInnerStepsNode:   20
        -NbInnerStepsCxt:   20
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 8.31e-07
        -DiffCxt:  4.79e-07
    Outer Step:   325      LossTrajs: 0.23118004     ContextsNorm: 0.03433094     ValIndCrit: 0.22873066
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 8.14e-12
        -DiffCxt:  3.02e-08
    Outer Step:   350      LossTrajs: 0.24050631     ContextsNorm: 0.03447375     ValIndCrit: 0.22818345
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 3.86e-14
        -DiffCxt:  1.28e-08
    Outer Step:   375      LossTrajs: 0.22272290     ContextsNorm: 0.03471689     ValIndCrit: 0.22729032
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 1.31e-16
        -DiffCxt:  1.35e-08
    Outer Step:   400      LossTrajs: 0.22489813     ContextsNorm: 0.03474997     ValIndCrit: 0.22701864
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 3.37e-17
        -DiffCxt:  4.23e-08
    Outer Step:   425      LossTrajs: 0.22215389     ContextsNorm: 0.03470659     ValIndCrit: 0.22775936
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 7.81e-16
        -DiffCxt:  1.61e-08
    Outer Step:   450      LossTrajs: 0.23057835     ContextsNorm: 0.03463987     ValIndCrit: 0.22819449
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 9.50e-18
        -DiffCxt:  2.84e-09
    Outer Step:   475      LossTrajs: 0.22259818     ContextsNorm: 0.03462459     ValIndCrit: 0.22812240
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 5.13e-17
        -DiffCxt:  2.33e-09
    Outer Step:   500      LossTrajs: 0.22403774     ContextsNorm: 0.03462776     ValIndCrit: 0.22806536
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 3.74e-19
        -DiffCxt:  5.00e-10
    Outer Step:   525      LossTrajs: 0.22573327     ContextsNorm: 0.03463034     ValIndCrit: 0.22801051
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 7.77e-19
        -DiffCxt:  5.33e-10
    Outer Step:   550      LossTrajs: 0.24394479     ContextsNorm: 0.03463285     ValIndCrit: 0.22794007
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 1.42e-18
        -DiffCxt:  5.66e-10
    Outer Step:   575      LossTrajs: 0.22418573     ContextsNorm: 0.03463296     ValIndCrit: 0.22788462
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 2.48e-18
        -DiffCxt:  1.86e-08
    Outer Step:   600      LossTrajs: 0.21786763     ContextsNorm: 0.03472365     ValIndCrit: 0.22650327
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 2.63e-17
        -DiffCxt:  1.54e-08
    Outer Step:   625      LossTrajs: 0.21706176     ContextsNorm: 0.03472418     ValIndCrit: 0.22592463
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 8.50e-19
        -DiffCxt:  7.11e-09
    Outer Step:   650      LossTrajs: 0.22190100     ContextsNorm: 0.03472626     ValIndCrit: 0.22591352
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 4.50e-11
        -DiffCxt:  8.68e-10
    Outer Step:   675      LossTrajs: 0.21974681     ContextsNorm: 0.03472779     ValIndCrit: 0.22586764
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 2.17e-13
        -DiffCxt:  4.17e-09
    Outer Step:   700      LossTrajs: 0.21952292     ContextsNorm: 0.03472946     ValIndCrit: 0.22581759
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 1.19e-15
        -DiffCxt:  5.43e-10
    Outer Step:   725      LossTrajs: 0.22425812     ContextsNorm: 0.03473204     ValIndCrit: 0.22577234
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 1.15e-17
        -DiffCxt:  4.67e-10
    Outer Step:   750      LossTrajs: 0.21081798     ContextsNorm: 0.03473379     ValIndCrit: 0.22573215
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 3.45e-15
        -DiffCxt:  9.16e-10
    Outer Step:   775      LossTrajs: 0.22916861     ContextsNorm: 0.03474025     ValIndCrit: 0.22569925
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 3.26e-13
        -DiffCxt:  1.26e-08
    Outer Step:   800      LossTrajs: 0.21335097     ContextsNorm: 0.03475523     ValIndCrit: 0.22562078
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 7.25e-15
        -DiffCxt:  5.10e-09
    Outer Step:   825      LossTrajs: 0.22163756     ContextsNorm: 0.03477182     ValIndCrit: 0.22576842
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 9.77e-17
        -DiffCxt:  3.82e-09
    Outer Step:   850      LossTrajs: 0.22789717     ContextsNorm: 0.03477723     ValIndCrit: 0.22572516
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 1.01e-14
        -DiffCxt:  1.27e-09
    Outer Step:   875      LossTrajs: 0.21433234     ContextsNorm: 0.03478087     ValIndCrit: 0.22567415
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 9.18e-14
        -DiffCxt:  5.77e-10
    Outer Step:   900      LossTrajs: 0.22006252     ContextsNorm: 0.03479118     ValIndCrit: 0.22561754
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 5.54e-16
        -DiffCxt:  3.48e-08
    Outer Step:   925      LossTrajs: 0.23355944     ContextsNorm: 0.03481402     ValIndCrit: 0.22454478
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 2.87e-12
        -DiffCxt:  8.52e-09
    Outer Step:   950      LossTrajs: 0.21913756     ContextsNorm: 0.03478182     ValIndCrit: 0.22472024
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 8.55e-13
        -DiffCxt:  3.18e-08
    Outer Step:   975      LossTrajs: 0.21902089     ContextsNorm: 0.03477801     ValIndCrit: 0.22468609
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 4.46e-15
        -DiffCxt:  8.99e-10
    Outer Step:   999      LossTrajs: 0.21898346     ContextsNorm: 0.03478139     ValIndCrit: 0.22465736
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-09
        -InnerToleranceCtx:  5.00e-08
        -DiffNode: 2.38e-15
        -DiffCxt:  1.10e-09

Total gradient descent training time: 0 hours 51 mins 31 secs
Environment weights at the end of the training: [0.04761905 0.04761905 0.04761905 0.04761905 0.04761905 0.04761905
 0.04761905 0.04761905 0.04761905 0.04761905 0.04761905 0.04761905
 0.04761905 0.04761905 0.04761905 0.04761905 0.04761905 0.04761905
 0.04761905 0.04761905 0.04761905]
WARNING: You did not provide a dataloader id. A new one has been generated: 174844
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 21
    Final length of the training trajectories: 11
    Length of the testing trajectories: 11
Test Score (In-Domain): 0.22465736


############# Inductive Bias Learning for Dynamical Systems #############

Jax version: 0.4.14
Available devices: [gpu(id=0)]
Running this script in ipython (Jupyter) session ? False
=== Parsed arguments to generate data ===
 Split: adapt
 Savepath: ./runs/04042024-165703/adapt/
 Seed: 6

==  Begining in-domain visualisation ... ==
    Environment id: 0
    Trajectory id: 3
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 11
    Length of the testing trajectories: 11
Testing finished. Figure saved in: ./runs/04042024-165703/results_in_domain.png
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 11, 2) (11,)
    Epoch:     0     LossContext: 1.20091021
    Epoch:     1     LossContext: 1.11095452
    Epoch:     2     LossContext: 1.02005863
    Epoch:     3     LossContext: 0.92944080
    Epoch:    25     LossContext: 0.06270214
    Epoch:    50     LossContext: 0.03755569
    Epoch:    75     LossContext: 0.03140969
    Epoch:   100     LossContext: 0.03120388
    Epoch:   125     LossContext: 0.03117864
    Epoch:   150     LossContext: 0.03117932
    Epoch:   175     LossContext: 0.03117661
    Epoch:   200     LossContext: 0.03117378
    Epoch:   225     LossContext: 0.03117139
    Epoch:   250     LossContext: 0.03117427
    Epoch:   275     LossContext: 0.03116692
    Epoch:   300     LossContext: 0.03116467
    Epoch:   325     LossContext: 0.03116518
    Epoch:   350     LossContext: 0.03116481
    Epoch:   375     LossContext: 0.03116052
    Epoch:   400     LossContext: 0.03116058
    Epoch:   425     LossContext: 0.03116172
    Epoch:   450     LossContext: 0.03115549
    Epoch:   475     LossContext: 0.03115145
    Epoch:   500     LossContext: 0.03115938
    Epoch:   525     LossContext: 0.03115924
    Epoch:   550     LossContext: 0.03115307
    Epoch:   575     LossContext: 0.03114908
    Epoch:   600     LossContext: 0.03114901
    Epoch:   625     LossContext: 0.03114709
    Epoch:   650     LossContext: 0.03114687
    Epoch:   675     LossContext: 0.03114761
    Epoch:   700     LossContext: 0.03114356
    Epoch:   725     LossContext: 0.03114189
    Epoch:   750     LossContext: 0.03114272
    Epoch:   775     LossContext: 0.03114016
    Epoch:   800     LossContext: 0.03113931
    Epoch:   825     LossContext: 0.03113712
    Epoch:   850     LossContext: 0.03113996
    Epoch:   875     LossContext: 0.03113285
    Epoch:   900     LossContext: 0.03113145
    Epoch:   925     LossContext: 0.03113313
    Epoch:   950     LossContext: 0.03113286
    Epoch:   975     LossContext: 0.03113052
    Epoch:  1000     LossContext: 0.03113115
    Epoch:  1025     LossContext: 0.03112979
    Epoch:  1050     LossContext: 0.03113256
    Epoch:  1075     LossContext: 0.03113190
    Epoch:  1100     LossContext: 0.03112383
    Epoch:  1125     LossContext: 0.03112369
    Epoch:  1150     LossContext: 0.03112574
    Epoch:  1175     LossContext: 0.03112136
    Epoch:  1200     LossContext: 0.03111516
    Epoch:  1225     LossContext: 0.03112063
    Epoch:  1250     LossContext: 0.03112066
    Epoch:  1275     LossContext: 0.03111520
    Epoch:  1300     LossContext: 0.03111725
    Epoch:  1325     LossContext: 0.03111604
    Epoch:  1350     LossContext: 0.03110798
    Epoch:  1375     LossContext: 0.03110817
    Epoch:  1400     LossContext: 0.03110845
    Epoch:  1425     LossContext: 0.03110943
    Epoch:  1450     LossContext: 0.03110471
    Epoch:  1475     LossContext: 0.03109964
    Epoch:  1499     LossContext: 0.03110300

Gradient descent adaptation time: 0 hours 2 mins 34 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.86691928
    Epoch:     1     LossContext: 0.75889450
    Epoch:     2     LossContext: 0.65068328
    Epoch:     3     LossContext: 0.54486942
    Epoch:    25     LossContext: 0.16614304
    Epoch:    50     LossContext: 0.13199024
    Epoch:    75     LossContext: 0.12944205
    Epoch:   100     LossContext: 0.12942845
    Epoch:   125     LossContext: 0.12947311
    Epoch:   150     LossContext: 0.12945390
    Epoch:   175     LossContext: 0.12945093
    Epoch:   200     LossContext: 0.12945193
    Epoch:   225     LossContext: 0.12945192
    Epoch:   250     LossContext: 0.12945206
    Epoch:   275     LossContext: 0.12945169
    Epoch:   300     LossContext: 0.12945165
    Epoch:   325     LossContext: 0.12945174
    Epoch:   350     LossContext: 0.12945120
    Epoch:   375     LossContext: 0.12945142
    Epoch:   400     LossContext: 0.12945102
    Epoch:   425     LossContext: 0.12945092
    Epoch:   450     LossContext: 0.12945151
    Epoch:   475     LossContext: 0.12945110
    Epoch:   500     LossContext: 0.12945117
    Epoch:   525     LossContext: 0.12945114
    Epoch:   550     LossContext: 0.12945119
    Epoch:   575     LossContext: 0.12945127
    Epoch:   600     LossContext: 0.12945092
    Epoch:   625     LossContext: 0.12945090
    Epoch:   650     LossContext: 0.12945057
    Epoch:   675     LossContext: 0.12945062
    Epoch:   700     LossContext: 0.12945090
    Epoch:   725     LossContext: 0.12945089
    Epoch:   750     LossContext: 0.12945023
    Epoch:   775     LossContext: 0.12945077
    Epoch:   800     LossContext: 0.12945078
    Epoch:   825     LossContext: 0.12945062
    Epoch:   850     LossContext: 0.12945049
    Epoch:   875     LossContext: 0.12945069
    Epoch:   900     LossContext: 0.12945065
    Epoch:   925     LossContext: 0.12945054
    Epoch:   950     LossContext: 0.12945031
    Epoch:   975     LossContext: 0.12945025
    Epoch:  1000     LossContext: 0.12945019
    Epoch:  1025     LossContext: 0.12945011
    Epoch:  1050     LossContext: 0.12945019
    Epoch:  1075     LossContext: 0.12944998
    Epoch:  1100     LossContext: 0.12945041
    Epoch:  1125     LossContext: 0.12945011
    Epoch:  1150     LossContext: 0.12944987
    Epoch:  1175     LossContext: 0.12944987
    Epoch:  1200     LossContext: 0.12945022
    Epoch:  1225     LossContext: 0.12944967
    Epoch:  1250     LossContext: 0.12944952
    Epoch:  1275     LossContext: 0.12944950
    Epoch:  1300     LossContext: 0.12944968
    Epoch:  1325     LossContext: 0.12944943
    Epoch:  1350     LossContext: 0.12944995
    Epoch:  1375     LossContext: 0.12944955
    Epoch:  1400     LossContext: 0.12944925
    Epoch:  1425     LossContext: 0.12944937
    Epoch:  1450     LossContext: 0.12944926
    Epoch:  1475     LossContext: 0.12944911
    Epoch:  1499     LossContext: 0.12944919

Gradient descent adaptation time: 0 hours 1 mins 32 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.00949597
    Epoch:     1     LossContext: 0.00574166
    Epoch:     2     LossContext: 0.00540667
    Epoch:     3     LossContext: 0.00479491
    Epoch:    25     LossContext: 0.00426486
    Epoch:    50     LossContext: 0.00415219
    Epoch:    75     LossContext: 0.00415334
    Epoch:   100     LossContext: 0.00415128
    Epoch:   125     LossContext: 0.00415112
    Epoch:   150     LossContext: 0.00415048
    Epoch:   175     LossContext: 0.00415100
    Epoch:   200     LossContext: 0.00415100
    Epoch:   225     LossContext: 0.00415089
    Epoch:   250     LossContext: 0.00415085
    Epoch:   275     LossContext: 0.00415078
    Epoch:   300     LossContext: 0.00415020
    Epoch:   325     LossContext: 0.00415066
    Epoch:   350     LossContext: 0.00415065
    Epoch:   375     LossContext: 0.00415059
    Epoch:   400     LossContext: 0.00415004
    Epoch:   425     LossContext: 0.00415058
    Epoch:   450     LossContext: 0.00415053
    Epoch:   475     LossContext: 0.00414994
    Epoch:   500     LossContext: 0.00415044
    Epoch:   525     LossContext: 0.00414991
    Epoch:   550     LossContext: 0.00414988
    Epoch:   575     LossContext: 0.00415039
    Epoch:   600     LossContext: 0.00415035
    Epoch:   625     LossContext: 0.00414975
    Epoch:   650     LossContext: 0.00415030
    Epoch:   675     LossContext: 0.00414972
    Epoch:   700     LossContext: 0.00415026
    Epoch:   725     LossContext: 0.00415019
    Epoch:   750     LossContext: 0.00415023
    Epoch:   775     LossContext: 0.00415021
    Epoch:   800     LossContext: 0.00414966
    Epoch:   825     LossContext: 0.00415014
    Epoch:   850     LossContext: 0.00415016
    Epoch:   875     LossContext: 0.00415020
    Epoch:   900     LossContext: 0.00415017
    Epoch:   925     LossContext: 0.00415016
    Epoch:   950     LossContext: 0.00415007
    Epoch:   975     LossContext: 0.00415010
    Epoch:  1000     LossContext: 0.00415011
    Epoch:  1025     LossContext: 0.00415008
    Epoch:  1050     LossContext: 0.00415005
    Epoch:  1075     LossContext: 0.00414951
    Epoch:  1100     LossContext: 0.00415005
    Epoch:  1125     LossContext: 0.00414947
    Epoch:  1150     LossContext: 0.00415004
    Epoch:  1175     LossContext: 0.00415003
    Epoch:  1200     LossContext: 0.00415000
    Epoch:  1225     LossContext: 0.00414990
    Epoch:  1250     LossContext: 0.00414995
    Epoch:  1275     LossContext: 0.00414994
    Epoch:  1300     LossContext: 0.00414936
    Epoch:  1325     LossContext: 0.00414987
    Epoch:  1350     LossContext: 0.00414930
    Epoch:  1375     LossContext: 0.00414979
    Epoch:  1400     LossContext: 0.00414933
    Epoch:  1425     LossContext: 0.00414931
    Epoch:  1450     LossContext: 0.00414929
    Epoch:  1475     LossContext: 0.00414927
    Epoch:  1499     LossContext: 0.00414979

Gradient descent adaptation time: 0 hours 1 mins 27 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.10512089
    Epoch:     1     LossContext: 0.06747460
    Epoch:     2     LossContext: 0.03844407
    Epoch:     3     LossContext: 0.01813357
    Epoch:    25     LossContext: 0.00448185
    Epoch:    50     LossContext: 0.00102916
    Epoch:    75     LossContext: 0.00083811
    Epoch:   100     LossContext: 0.00082685
    Epoch:   125     LossContext: 0.00082541
    Epoch:   150     LossContext: 0.00082500
    Epoch:   175     LossContext: 0.00082511
    Epoch:   200     LossContext: 0.00082496
    Epoch:   225     LossContext: 0.00082476
    Epoch:   250     LossContext: 0.00082476
    Epoch:   275     LossContext: 0.00082464
    Epoch:   300     LossContext: 0.00082463
    Epoch:   325     LossContext: 0.00082431
    Epoch:   350     LossContext: 0.00082439
    Epoch:   375     LossContext: 0.00082432
    Epoch:   400     LossContext: 0.00082413
    Epoch:   425     LossContext: 0.00082413
    Epoch:   450     LossContext: 0.00082420
    Epoch:   475     LossContext: 0.00082410
    Epoch:   500     LossContext: 0.00082404
    Epoch:   525     LossContext: 0.00082399
    Epoch:   550     LossContext: 0.00082385
    Epoch:   575     LossContext: 0.00082394
    Epoch:   600     LossContext: 0.00082371
    Epoch:   625     LossContext: 0.00082373
    Epoch:   650     LossContext: 0.00082354
    Epoch:   675     LossContext: 0.00082371
    Epoch:   700     LossContext: 0.00082359
    Epoch:   725     LossContext: 0.00082353
    Epoch:   750     LossContext: 0.00082355
    Epoch:   775     LossContext: 0.00082331
    Epoch:   800     LossContext: 0.00082343
    Epoch:   825     LossContext: 0.00082328
    Epoch:   850     LossContext: 0.00082326
    Epoch:   875     LossContext: 0.00082323
    Epoch:   900     LossContext: 0.00082328
    Epoch:   925     LossContext: 0.00082306
    Epoch:   950     LossContext: 0.00082323
    Epoch:   975     LossContext: 0.00082302
    Epoch:  1000     LossContext: 0.00082310
    Epoch:  1025     LossContext: 0.00082296
    Epoch:  1050     LossContext: 0.00082296
    Epoch:  1075     LossContext: 0.00082329
    Epoch:  1100     LossContext: 0.00082297
    Epoch:  1125     LossContext: 0.00082295
    Epoch:  1150     LossContext: 0.00082286
    Epoch:  1175     LossContext: 0.00082292
    Epoch:  1200     LossContext: 0.00082281
    Epoch:  1225     LossContext: 0.00082284
    Epoch:  1250     LossContext: 0.00082258
    Epoch:  1275     LossContext: 0.00082272
    Epoch:  1300     LossContext: 0.00082257
    Epoch:  1325     LossContext: 0.00082254
    Epoch:  1350     LossContext: 0.00082265
    Epoch:  1375     LossContext: 0.00082252
    Epoch:  1400     LossContext: 0.00082236
    Epoch:  1425     LossContext: 0.00082244
    Epoch:  1450     LossContext: 0.00082226
    Epoch:  1475     LossContext: 0.00082225
    Epoch:  1499     LossContext: 0.00082221

Gradient descent adaptation time: 0 hours 1 mins 32 secs

Adapting to environment 4 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 1.46718502
    Epoch:     1     LossContext: 1.31989622
    Epoch:     2     LossContext: 1.18158138
    Epoch:     3     LossContext: 1.05304945
    Epoch:    25     LossContext: 0.16741650
    Epoch:    50     LossContext: 0.15685369
    Epoch:    75     LossContext: 0.15581092
    Epoch:   100     LossContext: 0.15532430
    Epoch:   125     LossContext: 0.15532281
    Epoch:   150     LossContext: 0.15532754
    Epoch:   175     LossContext: 0.15532573
    Epoch:   200     LossContext: 0.15532468
    Epoch:   225     LossContext: 0.15532458
    Epoch:   250     LossContext: 0.15532601
    Epoch:   275     LossContext: 0.15532506
    Epoch:   300     LossContext: 0.15532473
    Epoch:   325     LossContext: 0.15532516
    Epoch:   350     LossContext: 0.15532513
    Epoch:   375     LossContext: 0.15532511
    Epoch:   400     LossContext: 0.15532517
    Epoch:   425     LossContext: 0.15532541
    Epoch:   450     LossContext: 0.15532555
    Epoch:   475     LossContext: 0.15532511
    Epoch:   500     LossContext: 0.15532416
    Epoch:   525     LossContext: 0.15532523
    Epoch:   550     LossContext: 0.15532523
    Epoch:   575     LossContext: 0.15532500
    Epoch:   600     LossContext: 0.15532526
    Epoch:   625     LossContext: 0.15532577
    Epoch:   650     LossContext: 0.15532532
    Epoch:   675     LossContext: 0.15532501
    Epoch:   700     LossContext: 0.15532471
    Epoch:   725     LossContext: 0.15532450
    Epoch:   750     LossContext: 0.15532529
    Epoch:   775     LossContext: 0.15532488
    Epoch:   800     LossContext: 0.15532504
    Epoch:   825     LossContext: 0.15532511
    Epoch:   850     LossContext: 0.15532508
    Epoch:   875     LossContext: 0.15532552
    Epoch:   900     LossContext: 0.15532562
    Epoch:   925     LossContext: 0.15532492
    Epoch:   950     LossContext: 0.15532495
    Epoch:   975     LossContext: 0.15532501
    Epoch:  1000     LossContext: 0.15532476
    Epoch:  1025     LossContext: 0.15532489
    Epoch:  1050     LossContext: 0.15532437
    Epoch:  1075     LossContext: 0.15532553
    Epoch:  1100     LossContext: 0.15532506
    Epoch:  1125     LossContext: 0.15532479
    Epoch:  1150     LossContext: 0.15532443
    Epoch:  1175     LossContext: 0.15532514
    Epoch:  1200     LossContext: 0.15532506
    Epoch:  1225     LossContext: 0.15532462
    Epoch:  1250     LossContext: 0.15532410
    Epoch:  1275     LossContext: 0.15532421
    Epoch:  1300     LossContext: 0.15532531
    Epoch:  1325     LossContext: 0.15532558
    Epoch:  1350     LossContext: 0.15532528
    Epoch:  1375     LossContext: 0.15532497
    Epoch:  1400     LossContext: 0.15532507
    Epoch:  1425     LossContext: 0.15532504
    Epoch:  1450     LossContext: 0.15532534
    Epoch:  1475     LossContext: 0.15532494
    Epoch:  1499     LossContext: 0.15532546

Gradient descent adaptation time: 0 hours 1 mins 43 secs

Adapting to environment 5 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 1.12949395
    Epoch:     1     LossContext: 1.01028442
    Epoch:     2     LossContext: 0.90098155
    Epoch:     3     LossContext: 0.80210882
    Epoch:    25     LossContext: 0.09653622
    Epoch:    50     LossContext: 0.01362720
    Epoch:    75     LossContext: 0.00200386
    Epoch:   100     LossContext: 0.00147486
    Epoch:   125     LossContext: 0.00147739
    Epoch:   150     LossContext: 0.00147369
    Epoch:   175     LossContext: 0.00147354
    Epoch:   200     LossContext: 0.00147352
    Epoch:   225     LossContext: 0.00147361
    Epoch:   250     LossContext: 0.00147346
    Epoch:   275     LossContext: 0.00147354
    Epoch:   300     LossContext: 0.00147344
    Epoch:   325     LossContext: 0.00147347
    Epoch:   350     LossContext: 0.00147344
    Epoch:   375     LossContext: 0.00147345
    Epoch:   400     LossContext: 0.00147338
    Epoch:   425     LossContext: 0.00147338
    Epoch:   450     LossContext: 0.00147331
    Epoch:   475     LossContext: 0.00147332
    Epoch:   500     LossContext: 0.00147337
    Epoch:   525     LossContext: 0.00147336
    Epoch:   550     LossContext: 0.00147331
    Epoch:   575     LossContext: 0.00147334
    Epoch:   600     LossContext: 0.00147320
    Epoch:   625     LossContext: 0.00147327
    Epoch:   650     LossContext: 0.00147318
    Epoch:   675     LossContext: 0.00147321
    Epoch:   700     LossContext: 0.00147318
    Epoch:   725     LossContext: 0.00147325
    Epoch:   750     LossContext: 0.00147315
    Epoch:   775     LossContext: 0.00147313
    Epoch:   800     LossContext: 0.00147313
    Epoch:   825     LossContext: 0.00147313
    Epoch:   850     LossContext: 0.00147320
    Epoch:   875     LossContext: 0.00147320
    Epoch:   900     LossContext: 0.00147318
    Epoch:   925     LossContext: 0.00147318
    Epoch:   950     LossContext: 0.00147318
    Epoch:   975     LossContext: 0.00147315
    Epoch:  1000     LossContext: 0.00147312
    Epoch:  1025     LossContext: 0.00147312
    Epoch:  1050     LossContext: 0.00147313
    Epoch:  1075     LossContext: 0.00147315
    Epoch:  1100     LossContext: 0.00147308
    Epoch:  1125     LossContext: 0.00147306
    Epoch:  1150     LossContext: 0.00147310
    Epoch:  1175     LossContext: 0.00147305
    Epoch:  1200     LossContext: 0.00147302
    Epoch:  1225     LossContext: 0.00147300
    Epoch:  1250     LossContext: 0.00147299
    Epoch:  1275     LossContext: 0.00147301
    Epoch:  1300     LossContext: 0.00147306
    Epoch:  1325     LossContext: 0.00147292
    Epoch:  1350     LossContext: 0.00147287
    Epoch:  1375     LossContext: 0.00147302
    Epoch:  1400     LossContext: 0.00147302
    Epoch:  1425     LossContext: 0.00147301
    Epoch:  1450     LossContext: 0.00147293
    Epoch:  1475     LossContext: 0.00147294
    Epoch:  1499     LossContext: 0.00147288

Gradient descent adaptation time: 0 hours 2 mins 39 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./runs/04042024-165703/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 21
    Number of adaptation environments: 6
    Final length of the training trajectories: 11
    Length of the testing trajectories: 11
Test Score (OOD): 0.053784937

==  Begining out-of-distribution visualisation ... ==
    Environment id: 0
    Trajectory id: 0
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 11
    Length of the testing trajectories: 11
Testing finished. Figure saved in: ./runs/04042024-165703/adapt/results_ood.png
