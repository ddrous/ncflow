
############# Neural Context Flow #############

Jax version: 0.4.28
Available devices: [cuda(id=0)]
Run folder created successfuly: ./runs/04082024-175436/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 175436
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 175436
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 310955 


WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed


=== Beginning training with proximal alternating minimization ... ===
    Number of examples in a batch: 16
    Maximum number of steps per inner minimization: 25
    Maximum number of outer minimizations: 250
    Maximum total number of training steps: 6250

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (5, 16, 10, 1024) (10,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (5, 16, 10, 1024) (10,)
    Outer Step:     0      LossTrajs: 1.28445923     ContextsNorm: 0.00000000     ValIndCrit: 1.23102593
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 4.44e-06
        -DiffCxt:  inf
    Outer Step:     1      LossTrajs: 0.86917901     ContextsNorm: 0.00016405     ValIndCrit: 0.85993004
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.91e-05
        -DiffCxt:  9.26e-03
    Outer Step:     2      LossTrajs: 0.80069876     ContextsNorm: 0.00009244     ValIndCrit: 0.78684157
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 6.18e-05
        -DiffCxt:  7.28e-02
    Outer Step:     3      LossTrajs: 0.55454904     ContextsNorm: 0.00013548     ValIndCrit: 0.53182626
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.25e-04
        -DiffCxt:  2.51e-02
    Outer Step:     5      LossTrajs: 0.37165445     ContextsNorm: 0.00317978     ValIndCrit: 0.36182529
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 3.79e-05
        -DiffCxt:  8.70e-04
    Outer Step:    10      LossTrajs: 0.06368906     ContextsNorm: 0.01802767     ValIndCrit: 0.06590610
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 6.34e-05
        -DiffCxt:  2.04e-05
    Outer Step:    15      LossTrajs: 0.03370065     ContextsNorm: 0.02623428     ValIndCrit: 0.03887650
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.36e-05
        -DiffCxt:  2.43e-06
    Outer Step:    20      LossTrajs: 0.01490760     ContextsNorm: 0.02829773     ValIndCrit: 0.01933924
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 6.32e-06
        -DiffCxt:  1.38e-07
    Outer Step:    25      LossTrajs: 0.00772319     ContextsNorm: 0.02795717     ValIndCrit: 0.01223287
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 2.46e-06
        -DiffCxt:  8.80e-08
    Outer Step:    30      LossTrajs: 0.00519903     ContextsNorm: 0.02761935     ValIndCrit: 0.00923455
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 9.33e-07
        -DiffCxt:  5.87e-08
    Outer Step:    35      LossTrajs: 0.00401235     ContextsNorm: 0.02727707     ValIndCrit: 0.00770690
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 5.16e-07
        -DiffCxt:  1.70e-07
    Outer Step:    40      LossTrajs: 0.00326814     ContextsNorm: 0.02694617     ValIndCrit: 0.00670610
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 3.41e-07
        -DiffCxt:  1.63e-07
    Outer Step:    45      LossTrajs: 0.00275266     ContextsNorm: 0.02667610     ValIndCrit: 0.00599365
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 2.27e-07
        -DiffCxt:  2.41e-08
    Outer Step:    50      LossTrajs: 0.00238126     ContextsNorm: 0.02666802     ValIndCrit: 0.00546044
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.68e-07
        -DiffCxt:  1.37e-07
    Outer Step:    55      LossTrajs: 0.00210316     ContextsNorm: 0.02650503     ValIndCrit: 0.00503303
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.22e-07
        -DiffCxt:  3.42e-08
    Outer Step:    60      LossTrajs: 0.00188596     ContextsNorm: 0.02616735     ValIndCrit: 0.00467929
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 9.71e-08
        -DiffCxt:  2.25e-07
    Outer Step:    65      LossTrajs: 0.00171312     ContextsNorm: 0.02604677     ValIndCrit: 0.00438590
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 8.11e-08
        -DiffCxt:  3.05e-07
    Outer Step:    70      LossTrajs: 0.00156966     ContextsNorm: 0.02587252     ValIndCrit: 0.00414033
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 6.27e-08
        -DiffCxt:  5.14e-08
    Outer Step:    75      LossTrajs: 0.00144976     ContextsNorm: 0.02554012     ValIndCrit: 0.00391842
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 5.28e-08
        -DiffCxt:  4.98e-08
    Outer Step:    80      LossTrajs: 0.00134774     ContextsNorm: 0.02549436     ValIndCrit: 0.00372908
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 4.42e-08
        -DiffCxt:  8.71e-08
    Outer Step:    85      LossTrajs: 0.00128385     ContextsNorm: 0.02551360     ValIndCrit: 0.00360462
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 7.07e-10
        -DiffCxt:  2.02e-09
    Outer Step:    90      LossTrajs: 0.00127419     ContextsNorm: 0.02536209     ValIndCrit: 0.00358726
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 7.38e-10
        -DiffCxt:  1.33e-09
    Outer Step:    95      LossTrajs: 0.00126409     ContextsNorm: 0.02524213     ValIndCrit: 0.00356750
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 7.62e-10
        -DiffCxt:  1.34e-09
    Outer Step:   100      LossTrajs: 0.00125372     ContextsNorm: 0.02511646     ValIndCrit: 0.00354709
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 7.86e-10
        -DiffCxt:  1.31e-09
    Outer Step:   105      LossTrajs: 0.00124309     ContextsNorm: 0.02500191     ValIndCrit: 0.00352614
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 8.12e-10
        -DiffCxt:  1.60e-09
    Outer Step:   110      LossTrajs: 0.00123223     ContextsNorm: 0.02488654     ValIndCrit: 0.00350377
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 8.37e-10
        -DiffCxt:  1.65e-09
    Outer Step:   115      LossTrajs: 0.00122109     ContextsNorm: 0.02478928     ValIndCrit: 0.00348069
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 8.61e-10
        -DiffCxt:  1.69e-09
    Outer Step:   120      LossTrajs: 0.00120960     ContextsNorm: 0.02469223     ValIndCrit: 0.00345722
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 8.88e-10
        -DiffCxt:  1.63e-09
    Outer Step:   125      LossTrajs: 0.00119799     ContextsNorm: 0.02458409     ValIndCrit: 0.00343153
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 9.15e-10
        -DiffCxt:  2.04e-09
    Outer Step:   130      LossTrajs: 0.00118605     ContextsNorm: 0.02449197     ValIndCrit: 0.00340739
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 9.38e-10
        -DiffCxt:  1.72e-09
    Outer Step:   135      LossTrajs: 0.00117392     ContextsNorm: 0.02439649     ValIndCrit: 0.00337997
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 9.64e-10
        -DiffCxt:  1.98e-09
    Outer Step:   140      LossTrajs: 0.00116150     ContextsNorm: 0.02430171     ValIndCrit: 0.00335230
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 9.84e-10
        -DiffCxt:  1.97e-09
    Outer Step:   145      LossTrajs: 0.00114890     ContextsNorm: 0.02422144     ValIndCrit: 0.00332459
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.01e-09
        -DiffCxt:  2.32e-09
    Outer Step:   150      LossTrajs: 0.00113619     ContextsNorm: 0.02414115     ValIndCrit: 0.00329477
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.03e-09
        -DiffCxt:  1.99e-09
    Outer Step:   155      LossTrajs: 0.00112324     ContextsNorm: 0.02405944     ValIndCrit: 0.00326589
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.06e-09
        -DiffCxt:  2.51e-09
    Outer Step:   160      LossTrajs: 0.00111012     ContextsNorm: 0.02396990     ValIndCrit: 0.00323481
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.08e-09
        -DiffCxt:  2.61e-09
    Outer Step:   165      LossTrajs: 0.00109685     ContextsNorm: 0.02388462     ValIndCrit: 0.00320336
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.10e-09
        -DiffCxt:  2.77e-09
    Outer Step:   170      LossTrajs: 0.00109160     ContextsNorm: 0.02383206     ValIndCrit: 0.00319143
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.20e-11
        -DiffCxt:  2.45e-10
    Outer Step:   175      LossTrajs: 0.00109007     ContextsNorm: 0.02379726     ValIndCrit: 0.00318831
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.27e-11
        -DiffCxt:  2.13e-10
    Outer Step:   180      LossTrajs: 0.00108861     ContextsNorm: 0.02376630     ValIndCrit: 0.00318441
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.33e-11
        -DiffCxt:  2.24e-10
    Outer Step:   185      LossTrajs: 0.00108708     ContextsNorm: 0.02373546     ValIndCrit: 0.00318156
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.39e-11
        -DiffCxt:  2.35e-10
    Outer Step:   190      LossTrajs: 0.00108555     ContextsNorm: 0.02370221     ValIndCrit: 0.00317723
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.45e-11
        -DiffCxt:  2.52e-10
    Outer Step:   195      LossTrajs: 0.00108388     ContextsNorm: 0.02367039     ValIndCrit: 0.00317331
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.52e-11
        -DiffCxt:  2.96e-10
    Outer Step:   200      LossTrajs: 0.00108218     ContextsNorm: 0.02363858     ValIndCrit: 0.00317012
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.59e-11
        -DiffCxt:  2.82e-10
    Outer Step:   205      LossTrajs: 0.00108028     ContextsNorm: 0.02360657     ValIndCrit: 0.00316458
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.67e-11
        -DiffCxt:  2.79e-10
    Outer Step:   210      LossTrajs: 0.00107853     ContextsNorm: 0.02357499     ValIndCrit: 0.00316018
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.75e-11
        -DiffCxt:  2.69e-10
    Outer Step:   215      LossTrajs: 0.00107663     ContextsNorm: 0.02353637     ValIndCrit: 0.00315553
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.81e-11
        -DiffCxt:  2.83e-10
    Outer Step:   220      LossTrajs: 0.00107468     ContextsNorm: 0.02351093     ValIndCrit: 0.00315062
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.88e-11
        -DiffCxt:  2.95e-10
    Outer Step:   225      LossTrajs: 0.00107263     ContextsNorm: 0.02347415     ValIndCrit: 0.00314558
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.97e-11
        -DiffCxt:  2.98e-10
    Outer Step:   230      LossTrajs: 0.00107047     ContextsNorm: 0.02343635     ValIndCrit: 0.00314025
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 2.05e-11
        -DiffCxt:  3.16e-10
    Outer Step:   235      LossTrajs: 0.00106830     ContextsNorm: 0.02340048     ValIndCrit: 0.00313543
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 2.15e-11
        -DiffCxt:  3.26e-10
    Outer Step:   240      LossTrajs: 0.00106611     ContextsNorm: 0.02336303     ValIndCrit: 0.00312957
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 2.25e-11
        -DiffCxt:  3.14e-10
    Outer Step:   245      LossTrajs: 0.00106381     ContextsNorm: 0.02332581     ValIndCrit: 0.00312383
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 2.31e-11
        -DiffCxt:  3.20e-10
    Outer Step:   249      LossTrajs: 0.00106190     ContextsNorm: 0.02330004     ValIndCrit: 0.00311860
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 2.36e-11
        -DiffCxt:  3.23e-10

Total gradient descent training time: 0 hours 18 mins 54 secs
Environment weights at the end of the training: [0.2 0.2 0.2 0.2 0.2]
WARNING: You did not provide a dataloader id. A new one has been generated: 181332
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 5
    Final length of the training trajectories: 10
    Length of the testing trajectories: 10
Test Score (In-Domain): 0.0031186014


Per-environment IND scores: [0.00502765 0.00350941 0.00264585 0.00223031 0.00217979]
==  Begining in-domain visualisation ... ==
    Environment id: 1
    Trajectory id: 29
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 10
    Length of the testing trajectories: 10
Testing finished. Figure saved in: ./runs/04082024-175436/results_in_domain.png
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 10, 1024) (10,)
    Epoch:     0     LossContext: 0.01331287
    Epoch:     1     LossContext: 0.01309516
    Epoch:     2     LossContext: 0.01286725
    Epoch:     3     LossContext: 0.01262917
    Epoch:    25     LossContext: 0.00574764
    Epoch:    50     LossContext: 0.00145775
    Epoch:    75     LossContext: 0.00137141
    Epoch:   100     LossContext: 0.00134420
    Epoch:   125     LossContext: 0.00134134
    Epoch:   150     LossContext: 0.00133847
    Epoch:   175     LossContext: 0.00133589
    Epoch:   200     LossContext: 0.00133345
    Epoch:   225     LossContext: 0.00133115
    Epoch:   250     LossContext: 0.00132894
    Epoch:   275     LossContext: 0.00132685
    Epoch:   300     LossContext: 0.00132483
    Epoch:   325     LossContext: 0.00132293
    Epoch:   350     LossContext: 0.00132111
    Epoch:   375     LossContext: 0.00131935
    Epoch:   400     LossContext: 0.00131763
    Epoch:   425     LossContext: 0.00131593
    Epoch:   450     LossContext: 0.00131424
    Epoch:   475     LossContext: 0.00131262
    Epoch:   500     LossContext: 0.00131106
    Epoch:   525     LossContext: 0.00130955
    Epoch:   550     LossContext: 0.00130804
    Epoch:   575     LossContext: 0.00130663
    Epoch:   600     LossContext: 0.00130524
    Epoch:   625     LossContext: 0.00130388
    Epoch:   650     LossContext: 0.00130258
    Epoch:   675     LossContext: 0.00130130
    Epoch:   700     LossContext: 0.00130003
    Epoch:   725     LossContext: 0.00129879
    Epoch:   750     LossContext: 0.00129760
    Epoch:   775     LossContext: 0.00129643
    Epoch:   800     LossContext: 0.00129527
    Epoch:   825     LossContext: 0.00129410
    Epoch:   850     LossContext: 0.00129296
    Epoch:   875     LossContext: 0.00129184
    Epoch:   900     LossContext: 0.00129074
    Epoch:   925     LossContext: 0.00128965
    Epoch:   950     LossContext: 0.00128858
    Epoch:   975     LossContext: 0.00128753
    Epoch:  1000     LossContext: 0.00128649
    Epoch:  1025     LossContext: 0.00128548
    Epoch:  1050     LossContext: 0.00128447
    Epoch:  1075     LossContext: 0.00128348
    Epoch:  1100     LossContext: 0.00128250
    Epoch:  1125     LossContext: 0.00128154
    Epoch:  1150     LossContext: 0.00128061
    Epoch:  1175     LossContext: 0.00127971
    Epoch:  1200     LossContext: 0.00127883
    Epoch:  1225     LossContext: 0.00127798
    Epoch:  1250     LossContext: 0.00127716
    Epoch:  1275     LossContext: 0.00127637
    Epoch:  1300     LossContext: 0.00127562
    Epoch:  1325     LossContext: 0.00127489
    Epoch:  1350     LossContext: 0.00127418
    Epoch:  1375     LossContext: 0.00127349
    Epoch:  1400     LossContext: 0.00127281
    Epoch:  1425     LossContext: 0.00127213
    Epoch:  1450     LossContext: 0.00127147
    Epoch:  1475     LossContext: 0.00127081
    Epoch:  1499     LossContext: 0.00127020

Gradient descent adaptation time: 0 hours 0 mins 10 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.00579767
    Epoch:     1     LossContext: 0.00566170
    Epoch:     2     LossContext: 0.00552022
    Epoch:     3     LossContext: 0.00537343
    Epoch:    25     LossContext: 0.00186187
    Epoch:    50     LossContext: 0.00092103
    Epoch:    75     LossContext: 0.00089797
    Epoch:   100     LossContext: 0.00089385
    Epoch:   125     LossContext: 0.00089193
    Epoch:   150     LossContext: 0.00089021
    Epoch:   175     LossContext: 0.00088861
    Epoch:   200     LossContext: 0.00088718
    Epoch:   225     LossContext: 0.00088591
    Epoch:   250     LossContext: 0.00088468
    Epoch:   275     LossContext: 0.00088347
    Epoch:   300     LossContext: 0.00088233
    Epoch:   325     LossContext: 0.00088121
    Epoch:   350     LossContext: 0.00088011
    Epoch:   375     LossContext: 0.00087911
    Epoch:   400     LossContext: 0.00087813
    Epoch:   425     LossContext: 0.00087722
    Epoch:   450     LossContext: 0.00087633
    Epoch:   475     LossContext: 0.00087546
    Epoch:   500     LossContext: 0.00087463
    Epoch:   525     LossContext: 0.00087388
    Epoch:   550     LossContext: 0.00087317
    Epoch:   575     LossContext: 0.00087246
    Epoch:   600     LossContext: 0.00087176
    Epoch:   625     LossContext: 0.00087106
    Epoch:   650     LossContext: 0.00087037
    Epoch:   675     LossContext: 0.00086970
    Epoch:   700     LossContext: 0.00086903
    Epoch:   725     LossContext: 0.00086841
    Epoch:   750     LossContext: 0.00086781
    Epoch:   775     LossContext: 0.00086724
    Epoch:   800     LossContext: 0.00086667
    Epoch:   825     LossContext: 0.00086612
    Epoch:   850     LossContext: 0.00086560
    Epoch:   875     LossContext: 0.00086510
    Epoch:   900     LossContext: 0.00086460
    Epoch:   925     LossContext: 0.00086412
    Epoch:   950     LossContext: 0.00086365
    Epoch:   975     LossContext: 0.00086319
    Epoch:  1000     LossContext: 0.00086273
    Epoch:  1025     LossContext: 0.00086228
    Epoch:  1050     LossContext: 0.00086187
    Epoch:  1075     LossContext: 0.00086146
    Epoch:  1100     LossContext: 0.00086105
    Epoch:  1125     LossContext: 0.00086064
    Epoch:  1150     LossContext: 0.00086024
    Epoch:  1175     LossContext: 0.00085986
    Epoch:  1200     LossContext: 0.00085950
    Epoch:  1225     LossContext: 0.00085917
    Epoch:  1250     LossContext: 0.00085884
    Epoch:  1275     LossContext: 0.00085853
    Epoch:  1300     LossContext: 0.00085823
    Epoch:  1325     LossContext: 0.00085793
    Epoch:  1350     LossContext: 0.00085763
    Epoch:  1375     LossContext: 0.00085735
    Epoch:  1400     LossContext: 0.00085707
    Epoch:  1425     LossContext: 0.00085680
    Epoch:  1450     LossContext: 0.00085655
    Epoch:  1475     LossContext: 0.00085629
    Epoch:  1499     LossContext: 0.00085605

Gradient descent adaptation time: 0 hours 0 mins 8 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.00266150
    Epoch:     1     LossContext: 0.00259082
    Epoch:     2     LossContext: 0.00251816
    Epoch:     3     LossContext: 0.00244379
    Epoch:    25     LossContext: 0.00107247
    Epoch:    50     LossContext: 0.00076822
    Epoch:    75     LossContext: 0.00076096
    Epoch:   100     LossContext: 0.00075368
    Epoch:   125     LossContext: 0.00074954
    Epoch:   150     LossContext: 0.00074642
    Epoch:   175     LossContext: 0.00074401
    Epoch:   200     LossContext: 0.00074204
    Epoch:   225     LossContext: 0.00074033
    Epoch:   250     LossContext: 0.00073885
    Epoch:   275     LossContext: 0.00073759
    Epoch:   300     LossContext: 0.00073639
    Epoch:   325     LossContext: 0.00073531
    Epoch:   350     LossContext: 0.00073435
    Epoch:   375     LossContext: 0.00073350
    Epoch:   400     LossContext: 0.00073268
    Epoch:   425     LossContext: 0.00073188
    Epoch:   450     LossContext: 0.00073110
    Epoch:   475     LossContext: 0.00073034
    Epoch:   500     LossContext: 0.00072965
    Epoch:   525     LossContext: 0.00072899
    Epoch:   550     LossContext: 0.00072835
    Epoch:   575     LossContext: 0.00072774
    Epoch:   600     LossContext: 0.00072714
    Epoch:   625     LossContext: 0.00072659
    Epoch:   650     LossContext: 0.00072605
    Epoch:   675     LossContext: 0.00072552
    Epoch:   700     LossContext: 0.00072502
    Epoch:   725     LossContext: 0.00072455
    Epoch:   750     LossContext: 0.00072411
    Epoch:   775     LossContext: 0.00072370
    Epoch:   800     LossContext: 0.00072331
    Epoch:   825     LossContext: 0.00072294
    Epoch:   850     LossContext: 0.00072258
    Epoch:   875     LossContext: 0.00072223
    Epoch:   900     LossContext: 0.00072189
    Epoch:   925     LossContext: 0.00072157
    Epoch:   950     LossContext: 0.00072126
    Epoch:   975     LossContext: 0.00072096
    Epoch:  1000     LossContext: 0.00072067
    Epoch:  1025     LossContext: 0.00072040
    Epoch:  1050     LossContext: 0.00072012
    Epoch:  1075     LossContext: 0.00071985
    Epoch:  1100     LossContext: 0.00071959
    Epoch:  1125     LossContext: 0.00071933
    Epoch:  1150     LossContext: 0.00071908
    Epoch:  1175     LossContext: 0.00071883
    Epoch:  1200     LossContext: 0.00071861
    Epoch:  1225     LossContext: 0.00071839
    Epoch:  1250     LossContext: 0.00071816
    Epoch:  1275     LossContext: 0.00071794
    Epoch:  1300     LossContext: 0.00071773
    Epoch:  1325     LossContext: 0.00071753
    Epoch:  1350     LossContext: 0.00071734
    Epoch:  1375     LossContext: 0.00071715
    Epoch:  1400     LossContext: 0.00071696
    Epoch:  1425     LossContext: 0.00071678
    Epoch:  1450     LossContext: 0.00071661
    Epoch:  1475     LossContext: 0.00071644
    Epoch:  1499     LossContext: 0.00071629

Gradient descent adaptation time: 0 hours 0 mins 8 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.00283728
    Epoch:     1     LossContext: 0.00278528
    Epoch:     2     LossContext: 0.00273144
    Epoch:     3     LossContext: 0.00267585
    Epoch:    25     LossContext: 0.00139109
    Epoch:    50     LossContext: 0.00086897
    Epoch:    75     LossContext: 0.00083183
    Epoch:   100     LossContext: 0.00082408
    Epoch:   125     LossContext: 0.00081722
    Epoch:   150     LossContext: 0.00081188
    Epoch:   175     LossContext: 0.00080750
    Epoch:   200     LossContext: 0.00080362
    Epoch:   225     LossContext: 0.00080016
    Epoch:   250     LossContext: 0.00079703
    Epoch:   275     LossContext: 0.00079411
    Epoch:   300     LossContext: 0.00079146
    Epoch:   325     LossContext: 0.00078904
    Epoch:   350     LossContext: 0.00078681
    Epoch:   375     LossContext: 0.00078473
    Epoch:   400     LossContext: 0.00078280
    Epoch:   425     LossContext: 0.00078098
    Epoch:   450     LossContext: 0.00077930
    Epoch:   475     LossContext: 0.00077769
    Epoch:   500     LossContext: 0.00077612
    Epoch:   525     LossContext: 0.00077463
    Epoch:   550     LossContext: 0.00077328
    Epoch:   575     LossContext: 0.00077203
    Epoch:   600     LossContext: 0.00077081
    Epoch:   625     LossContext: 0.00076968
    Epoch:   650     LossContext: 0.00076858
    Epoch:   675     LossContext: 0.00076753
    Epoch:   700     LossContext: 0.00076654
    Epoch:   725     LossContext: 0.00076557
    Epoch:   750     LossContext: 0.00076466
    Epoch:   775     LossContext: 0.00076381
    Epoch:   800     LossContext: 0.00076300
    Epoch:   825     LossContext: 0.00076224
    Epoch:   850     LossContext: 0.00076151
    Epoch:   875     LossContext: 0.00076079
    Epoch:   900     LossContext: 0.00076013
    Epoch:   925     LossContext: 0.00075951
    Epoch:   950     LossContext: 0.00075891
    Epoch:   975     LossContext: 0.00075833
    Epoch:  1000     LossContext: 0.00075778
    Epoch:  1025     LossContext: 0.00075724
    Epoch:  1050     LossContext: 0.00075674
    Epoch:  1075     LossContext: 0.00075625
    Epoch:  1100     LossContext: 0.00075579
    Epoch:  1125     LossContext: 0.00075536
    Epoch:  1150     LossContext: 0.00075494
    Epoch:  1175     LossContext: 0.00075455
    Epoch:  1200     LossContext: 0.00075419
    Epoch:  1225     LossContext: 0.00075386
    Epoch:  1250     LossContext: 0.00075355
    Epoch:  1275     LossContext: 0.00075326
    Epoch:  1300     LossContext: 0.00075299
    Epoch:  1325     LossContext: 0.00075273
    Epoch:  1350     LossContext: 0.00075248
    Epoch:  1375     LossContext: 0.00075225
    Epoch:  1400     LossContext: 0.00075202
    Epoch:  1425     LossContext: 0.00075181
    Epoch:  1450     LossContext: 0.00075159
    Epoch:  1475     LossContext: 0.00075139
    Epoch:  1499     LossContext: 0.00075120

Gradient descent adaptation time: 0 hours 0 mins 8 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./runs/04082024-175436/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 5
    Number of adaptation environments: 4
    Final length of the training trajectories: 10
    Length of the testing trajectories: 10
Test Score (OOD): 0.002968778

