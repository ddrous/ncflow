
############# Neural Context Flow #############

Jax version: 0.4.28
Available devices: [cuda(id=0)]
Run folder created successfuly: ./runs/04082024-161015/
Completed copied scripts 
WARNING: You did not provide a dataloader id. A new one has been generated: 161015
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: You did not provide a dataloader id. A new one has been generated: 161015
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


Total number of parameters in the model: 310955 


WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed


=== Beginning training with proximal alternating minimization ... ===
    Number of examples in a batch: 16
    Maximum number of steps per inner minimization: 25
    Maximum number of outer minimizations: 250
    Maximum total number of training steps: 6250

Compiling function "train_step" for neural ode ...
Shapes of elements in a batch: (5, 16, 10, 1024) (10,)

Compiling function "train_step" for context ...
Shapes of elements in a batch: (5, 16, 10, 1024) (10,)
    Outer Step:     0      LossTrajs: 2.68323374     ContextsNorm: 0.00000000     ValIndCrit: 2.57301188
        Saving best model so far ...
        -NbInnerStepsNode:    1
        -NbInnerStepsCxt:    1
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 4.78e-06
        -DiffCxt:  inf
    Outer Step:     1      LossTrajs: 0.85912263     ContextsNorm: 0.00017380     ValIndCrit: 0.84295064
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 2.21e-05
        -DiffCxt:  8.98e-03
    Outer Step:     2      LossTrajs: 0.79462743     ContextsNorm: 0.00012761     ValIndCrit: 0.77822524
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 6.55e-05
        -DiffCxt:  2.53e-02
    Outer Step:     3      LossTrajs: 0.55165762     ContextsNorm: 0.00019168     ValIndCrit: 0.53164732
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.29e-04
        -DiffCxt:  9.53e-03
    Outer Step:     5      LossTrajs: 0.38558403     ContextsNorm: 0.00196659     ValIndCrit: 0.38087460
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 3.06e-05
        -DiffCxt:  1.14e-03
    Outer Step:    10      LossTrajs: 0.06794846     ContextsNorm: 0.01163527     ValIndCrit: 0.07021876
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 3.68e-05
        -DiffCxt:  5.53e-05
    Outer Step:    15      LossTrajs: 0.03263701     ContextsNorm: 0.01773471     ValIndCrit: 0.03608885
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.47e-05
        -DiffCxt:  3.58e-06
    Outer Step:    20      LossTrajs: 0.01208943     ContextsNorm: 0.02124681     ValIndCrit: 0.01500956
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 4.86e-06
        -DiffCxt:  6.07e-07
    Outer Step:    25      LossTrajs: 0.00707009     ContextsNorm: 0.02268645     ValIndCrit: 0.00986839
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 2.10e-06
        -DiffCxt:  2.26e-07
    Outer Step:    30      LossTrajs: 0.00458503     ContextsNorm: 0.02301987     ValIndCrit: 0.00736448
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 9.55e-07
        -DiffCxt:  1.62e-07
    Outer Step:    35      LossTrajs: 0.00350442     ContextsNorm: 0.02305043     ValIndCrit: 0.00630982
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 4.74e-07
        -DiffCxt:  5.17e-08
    Outer Step:    40      LossTrajs: 0.00288242     ContextsNorm: 0.02281848     ValIndCrit: 0.00563093
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 3.17e-07
        -DiffCxt:  8.28e-07
    Outer Step:    45      LossTrajs: 0.00246279     ContextsNorm: 0.02265961     ValIndCrit: 0.00512992
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 2.03e-07
        -DiffCxt:  6.39e-08
    Outer Step:    50      LossTrajs: 0.00215735     ContextsNorm: 0.02240475     ValIndCrit: 0.00472136
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.55e-07
        -DiffCxt:  1.08e-07
    Outer Step:    55      LossTrajs: 0.00191890     ContextsNorm: 0.02216698     ValIndCrit: 0.00438023
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.20e-07
        -DiffCxt:  6.66e-08
    Outer Step:    60      LossTrajs: 0.00172606     ContextsNorm: 0.02188648     ValIndCrit: 0.00408646
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 9.66e-08
        -DiffCxt:  1.10e-07
    Outer Step:    65      LossTrajs: 0.00156668     ContextsNorm: 0.02163628     ValIndCrit: 0.00383973
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 7.76e-08
        -DiffCxt:  9.60e-08
    Outer Step:    70      LossTrajs: 0.00143720     ContextsNorm: 0.02161792     ValIndCrit: 0.00363295
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 7.16e-08
        -DiffCxt:  4.39e-07
    Outer Step:    75      LossTrajs: 0.00132494     ContextsNorm: 0.02156708     ValIndCrit: 0.00346814
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 5.88e-08
        -DiffCxt:  1.41e-07
    Outer Step:    80      LossTrajs: 0.00123316     ContextsNorm: 0.02133222     ValIndCrit: 0.00330700
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 5.58e-08
        -DiffCxt:  2.94e-07
    Outer Step:    85      LossTrajs: 0.00116837     ContextsNorm: 0.02117781     ValIndCrit: 0.00322358
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 7.13e-10
        -DiffCxt:  3.25e-09
    Outer Step:    90      LossTrajs: 0.00115937     ContextsNorm: 0.02104781     ValIndCrit: 0.00321112
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 7.44e-10
        -DiffCxt:  2.29e-09
    Outer Step:    95      LossTrajs: 0.00115005     ContextsNorm: 0.02092408     ValIndCrit: 0.00319674
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 7.69e-10
        -DiffCxt:  2.28e-09
    Outer Step:   100      LossTrajs: 0.00114040     ContextsNorm: 0.02080708     ValIndCrit: 0.00318214
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 7.93e-10
        -DiffCxt:  2.49e-09
    Outer Step:   105      LossTrajs: 0.00113048     ContextsNorm: 0.02070017     ValIndCrit: 0.00316819
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 8.14e-10
        -DiffCxt:  2.67e-09
    Outer Step:   110      LossTrajs: 0.00112030     ContextsNorm: 0.02061291     ValIndCrit: 0.00315279
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 8.39e-10
        -DiffCxt:  2.81e-09
    Outer Step:   115      LossTrajs: 0.00110985     ContextsNorm: 0.02050739     ValIndCrit: 0.00313780
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 8.66e-10
        -DiffCxt:  2.94e-09
    Outer Step:   120      LossTrajs: 0.00109924     ContextsNorm: 0.02040483     ValIndCrit: 0.00312081
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 8.85e-10
        -DiffCxt:  3.30e-09
    Outer Step:   125      LossTrajs: 0.00108840     ContextsNorm: 0.02030076     ValIndCrit: 0.00310575
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 9.14e-10
        -DiffCxt:  3.52e-09
    Outer Step:   130      LossTrajs: 0.00107732     ContextsNorm: 0.02021379     ValIndCrit: 0.00308843
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 9.39e-10
        -DiffCxt:  3.27e-09
    Outer Step:   135      LossTrajs: 0.00106607     ContextsNorm: 0.02013123     ValIndCrit: 0.00307188
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 9.58e-10
        -DiffCxt:  3.45e-09
    Outer Step:   140      LossTrajs: 0.00105454     ContextsNorm: 0.02004344     ValIndCrit: 0.00305360
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 9.87e-10
        -DiffCxt:  3.85e-09
    Outer Step:   145      LossTrajs: 0.00104297     ContextsNorm: 0.01995590     ValIndCrit: 0.00303448
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.01e-09
        -DiffCxt:  4.47e-09
    Outer Step:   150      LossTrajs: 0.00103108     ContextsNorm: 0.01987672     ValIndCrit: 0.00301775
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.03e-09
        -DiffCxt:  3.73e-09
    Outer Step:   155      LossTrajs: 0.00101913     ContextsNorm: 0.01979542     ValIndCrit: 0.00300012
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.04e-09
        -DiffCxt:  3.96e-09
    Outer Step:   160      LossTrajs: 0.00100711     ContextsNorm: 0.01971014     ValIndCrit: 0.00298056
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.06e-09
        -DiffCxt:  4.69e-09
    Outer Step:   165      LossTrajs: 0.00099496     ContextsNorm: 0.01964366     ValIndCrit: 0.00296179
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.07e-09
        -DiffCxt:  4.94e-09
    Outer Step:   170      LossTrajs: 0.00099007     ContextsNorm: 0.01959230     ValIndCrit: 0.00295520
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.20e-11
        -DiffCxt:  3.09e-10
    Outer Step:   175      LossTrajs: 0.00098877     ContextsNorm: 0.01956116     ValIndCrit: 0.00295335
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.25e-11
        -DiffCxt:  3.32e-10
    Outer Step:   180      LossTrajs: 0.00098741     ContextsNorm: 0.01952957     ValIndCrit: 0.00295132
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.30e-11
        -DiffCxt:  3.20e-10
    Outer Step:   185      LossTrajs: 0.00098602     ContextsNorm: 0.01950133     ValIndCrit: 0.00294952
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.36e-11
        -DiffCxt:  3.34e-10
    Outer Step:   190      LossTrajs: 0.00098457     ContextsNorm: 0.01947366     ValIndCrit: 0.00294635
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.42e-11
        -DiffCxt:  3.42e-10
    Outer Step:   195      LossTrajs: 0.00098298     ContextsNorm: 0.01943958     ValIndCrit: 0.00294357
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.48e-11
        -DiffCxt:  3.32e-10
    Outer Step:   200      LossTrajs: 0.00098144     ContextsNorm: 0.01940817     ValIndCrit: 0.00294200
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.53e-11
        -DiffCxt:  3.47e-10
    Outer Step:   205      LossTrajs: 0.00097980     ContextsNorm: 0.01937805     ValIndCrit: 0.00293890
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.60e-11
        -DiffCxt:  3.48e-10
    Outer Step:   210      LossTrajs: 0.00097816     ContextsNorm: 0.01933912     ValIndCrit: 0.00293791
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.67e-11
        -DiffCxt:  4.36e-10
    Outer Step:   215      LossTrajs: 0.00097650     ContextsNorm: 0.01930749     ValIndCrit: 0.00293453
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.74e-11
        -DiffCxt:  3.71e-10
    Outer Step:   220      LossTrajs: 0.00097471     ContextsNorm: 0.01927719     ValIndCrit: 0.00293117
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.80e-11
        -DiffCxt:  3.88e-10
    Outer Step:   225      LossTrajs: 0.00097286     ContextsNorm: 0.01924864     ValIndCrit: 0.00292852
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.87e-11
        -DiffCxt:  3.73e-10
    Outer Step:   230      LossTrajs: 0.00097098     ContextsNorm: 0.01922075     ValIndCrit: 0.00292613
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 1.95e-11
        -DiffCxt:  3.82e-10
    Outer Step:   235      LossTrajs: 0.00096905     ContextsNorm: 0.01919418     ValIndCrit: 0.00292220
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 2.01e-11
        -DiffCxt:  3.74e-10
    Outer Step:   240      LossTrajs: 0.00096705     ContextsNorm: 0.01916130     ValIndCrit: 0.00291930
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 2.12e-11
        -DiffCxt:  4.93e-10
    Outer Step:   245      LossTrajs: 0.00096503     ContextsNorm: 0.01912834     ValIndCrit: 0.00291543
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 2.16e-11
        -DiffCxt:  4.07e-10
    Outer Step:   249      LossTrajs: 0.00096331     ContextsNorm: 0.01910643     ValIndCrit: 0.00291314
        Saving best model so far ...
        -NbInnerStepsNode:   25
        -NbInnerStepsCxt:   25
        -InnerToleranceNode: 1.00e-16
        -InnerToleranceCtx:  1.00e-16
        -DiffNode: 2.21e-11
        -DiffCxt:  3.97e-10

Total gradient descent training time: 0 hours 18 mins 55 secs
Environment weights at the end of the training: [0.2 0.2 0.2 0.2 0.2]
WARNING: You did not provide a dataloader id. A new one has been generated: 162912
WARNING: Note that this id used to distuinguish between adaptations to different environments.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided, using time as seed
==  Begining in-domain testing ... ==
    Number of training environments: 5
    Final length of the training trajectories: 10
    Length of the testing trajectories: 10
Test Score (In-Domain): 0.0029131435


Per-environment IND scores: [0.00422071 0.00307289 0.0025025  0.00232617 0.00244345]
==  Begining in-domain visualisation ... ==
    Environment id: 0
    Trajectory id: 3
    Visualized dimensions: (0, 1)
    Final length of the training trajectories: 10
    Length of the testing trajectories: 10
Testing finished. Figure saved in: ./runs/04082024-161015/results_in_domain.png
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.


=== Beginning sequential adaptation ... ===
    Number of examples in a batch: 1
    Number of train steps per epoch: 1
    Number of training epochs: 1500
    Total number of training steps: 1500

Adapting to environment 0 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed

Compiling function "train_step" for context ...
Shapes of elements in a batch: (1, 1, 10, 1024) (10,)
    Epoch:     0     LossContext: 0.00801017
    Epoch:     1     LossContext: 0.00787489
    Epoch:     2     LossContext: 0.00773397
    Epoch:     3     LossContext: 0.00758754
    Epoch:    25     LossContext: 0.00372588
    Epoch:    50     LossContext: 0.00146266
    Epoch:    75     LossContext: 0.00129310
    Epoch:   100     LossContext: 0.00128487
    Epoch:   125     LossContext: 0.00127779
    Epoch:   150     LossContext: 0.00127279
    Epoch:   175     LossContext: 0.00126844
    Epoch:   200     LossContext: 0.00126468
    Epoch:   225     LossContext: 0.00126158
    Epoch:   250     LossContext: 0.00125877
    Epoch:   275     LossContext: 0.00125614
    Epoch:   300     LossContext: 0.00125385
    Epoch:   325     LossContext: 0.00125184
    Epoch:   350     LossContext: 0.00125000
    Epoch:   375     LossContext: 0.00124828
    Epoch:   400     LossContext: 0.00124665
    Epoch:   425     LossContext: 0.00124512
    Epoch:   450     LossContext: 0.00124365
    Epoch:   475     LossContext: 0.00124225
    Epoch:   500     LossContext: 0.00124096
    Epoch:   525     LossContext: 0.00123974
    Epoch:   550     LossContext: 0.00123853
    Epoch:   575     LossContext: 0.00123739
    Epoch:   600     LossContext: 0.00123629
    Epoch:   625     LossContext: 0.00123524
    Epoch:   650     LossContext: 0.00123424
    Epoch:   675     LossContext: 0.00123327
    Epoch:   700     LossContext: 0.00123233
    Epoch:   725     LossContext: 0.00123142
    Epoch:   750     LossContext: 0.00123054
    Epoch:   775     LossContext: 0.00122968
    Epoch:   800     LossContext: 0.00122887
    Epoch:   825     LossContext: 0.00122810
    Epoch:   850     LossContext: 0.00122735
    Epoch:   875     LossContext: 0.00122663
    Epoch:   900     LossContext: 0.00122591
    Epoch:   925     LossContext: 0.00122520
    Epoch:   950     LossContext: 0.00122450
    Epoch:   975     LossContext: 0.00122383
    Epoch:  1000     LossContext: 0.00122317
    Epoch:  1025     LossContext: 0.00122252
    Epoch:  1050     LossContext: 0.00122190
    Epoch:  1075     LossContext: 0.00122130
    Epoch:  1100     LossContext: 0.00122071
    Epoch:  1125     LossContext: 0.00122012
    Epoch:  1150     LossContext: 0.00121955
    Epoch:  1175     LossContext: 0.00121900
    Epoch:  1200     LossContext: 0.00121847
    Epoch:  1225     LossContext: 0.00121796
    Epoch:  1250     LossContext: 0.00121747
    Epoch:  1275     LossContext: 0.00121699
    Epoch:  1300     LossContext: 0.00121652
    Epoch:  1325     LossContext: 0.00121606
    Epoch:  1350     LossContext: 0.00121561
    Epoch:  1375     LossContext: 0.00121517
    Epoch:  1400     LossContext: 0.00121474
    Epoch:  1425     LossContext: 0.00121432
    Epoch:  1450     LossContext: 0.00121391
    Epoch:  1475     LossContext: 0.00121350
    Epoch:  1499     LossContext: 0.00121310

Gradient descent adaptation time: 0 hours 0 mins 10 secs

Adapting to environment 1 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.00243090
    Epoch:     1     LossContext: 0.00236778
    Epoch:     2     LossContext: 0.00230284
    Epoch:     3     LossContext: 0.00223628
    Epoch:    25     LossContext: 0.00100164
    Epoch:    50     LossContext: 0.00081625
    Epoch:    75     LossContext: 0.00079526
    Epoch:   100     LossContext: 0.00079329
    Epoch:   125     LossContext: 0.00079134
    Epoch:   150     LossContext: 0.00078975
    Epoch:   175     LossContext: 0.00078826
    Epoch:   200     LossContext: 0.00078688
    Epoch:   225     LossContext: 0.00078569
    Epoch:   250     LossContext: 0.00078457
    Epoch:   275     LossContext: 0.00078353
    Epoch:   300     LossContext: 0.00078265
    Epoch:   325     LossContext: 0.00078188
    Epoch:   350     LossContext: 0.00078116
    Epoch:   375     LossContext: 0.00078049
    Epoch:   400     LossContext: 0.00077984
    Epoch:   425     LossContext: 0.00077926
    Epoch:   450     LossContext: 0.00077874
    Epoch:   475     LossContext: 0.00077825
    Epoch:   500     LossContext: 0.00077776
    Epoch:   525     LossContext: 0.00077729
    Epoch:   550     LossContext: 0.00077681
    Epoch:   575     LossContext: 0.00077636
    Epoch:   600     LossContext: 0.00077594
    Epoch:   625     LossContext: 0.00077553
    Epoch:   650     LossContext: 0.00077515
    Epoch:   675     LossContext: 0.00077480
    Epoch:   700     LossContext: 0.00077450
    Epoch:   725     LossContext: 0.00077421
    Epoch:   750     LossContext: 0.00077393
    Epoch:   775     LossContext: 0.00077365
    Epoch:   800     LossContext: 0.00077338
    Epoch:   825     LossContext: 0.00077312
    Epoch:   850     LossContext: 0.00077288
    Epoch:   875     LossContext: 0.00077266
    Epoch:   900     LossContext: 0.00077245
    Epoch:   925     LossContext: 0.00077225
    Epoch:   950     LossContext: 0.00077205
    Epoch:   975     LossContext: 0.00077185
    Epoch:  1000     LossContext: 0.00077165
    Epoch:  1025     LossContext: 0.00077147
    Epoch:  1050     LossContext: 0.00077130
    Epoch:  1075     LossContext: 0.00077114
    Epoch:  1100     LossContext: 0.00077099
    Epoch:  1125     LossContext: 0.00077085
    Epoch:  1150     LossContext: 0.00077071
    Epoch:  1175     LossContext: 0.00077058
    Epoch:  1200     LossContext: 0.00077045
    Epoch:  1225     LossContext: 0.00077034
    Epoch:  1250     LossContext: 0.00077022
    Epoch:  1275     LossContext: 0.00077010
    Epoch:  1300     LossContext: 0.00076999
    Epoch:  1325     LossContext: 0.00076987
    Epoch:  1350     LossContext: 0.00076977
    Epoch:  1375     LossContext: 0.00076966
    Epoch:  1400     LossContext: 0.00076956
    Epoch:  1425     LossContext: 0.00076945
    Epoch:  1450     LossContext: 0.00076936
    Epoch:  1475     LossContext: 0.00076926
    Epoch:  1499     LossContext: 0.00076916

Gradient descent adaptation time: 0 hours 0 mins 8 secs

Adapting to environment 2 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.00099523
    Epoch:     1     LossContext: 0.00098199
    Epoch:     2     LossContext: 0.00096840
    Epoch:     3     LossContext: 0.00095452
    Epoch:    25     LossContext: 0.00070506
    Epoch:    50     LossContext: 0.00067597
    Epoch:    75     LossContext: 0.00067149
    Epoch:   100     LossContext: 0.00066855
    Epoch:   125     LossContext: 0.00066617
    Epoch:   150     LossContext: 0.00066407
    Epoch:   175     LossContext: 0.00066226
    Epoch:   200     LossContext: 0.00066081
    Epoch:   225     LossContext: 0.00065958
    Epoch:   250     LossContext: 0.00065852
    Epoch:   275     LossContext: 0.00065753
    Epoch:   300     LossContext: 0.00065663
    Epoch:   325     LossContext: 0.00065588
    Epoch:   350     LossContext: 0.00065531
    Epoch:   375     LossContext: 0.00065479
    Epoch:   400     LossContext: 0.00065431
    Epoch:   425     LossContext: 0.00065387
    Epoch:   450     LossContext: 0.00065349
    Epoch:   475     LossContext: 0.00065314
    Epoch:   500     LossContext: 0.00065280
    Epoch:   525     LossContext: 0.00065250
    Epoch:   550     LossContext: 0.00065221
    Epoch:   575     LossContext: 0.00065194
    Epoch:   600     LossContext: 0.00065167
    Epoch:   625     LossContext: 0.00065143
    Epoch:   650     LossContext: 0.00065120
    Epoch:   675     LossContext: 0.00065098
    Epoch:   700     LossContext: 0.00065077
    Epoch:   725     LossContext: 0.00065057
    Epoch:   750     LossContext: 0.00065039
    Epoch:   775     LossContext: 0.00065022
    Epoch:   800     LossContext: 0.00065006
    Epoch:   825     LossContext: 0.00064992
    Epoch:   850     LossContext: 0.00064977
    Epoch:   875     LossContext: 0.00064963
    Epoch:   900     LossContext: 0.00064950
    Epoch:   925     LossContext: 0.00064937
    Epoch:   950     LossContext: 0.00064925
    Epoch:   975     LossContext: 0.00064914
    Epoch:  1000     LossContext: 0.00064903
    Epoch:  1025     LossContext: 0.00064892
    Epoch:  1050     LossContext: 0.00064882
    Epoch:  1075     LossContext: 0.00064873
    Epoch:  1100     LossContext: 0.00064864
    Epoch:  1125     LossContext: 0.00064856
    Epoch:  1150     LossContext: 0.00064848
    Epoch:  1175     LossContext: 0.00064841
    Epoch:  1200     LossContext: 0.00064834
    Epoch:  1225     LossContext: 0.00064827
    Epoch:  1250     LossContext: 0.00064821
    Epoch:  1275     LossContext: 0.00064816
    Epoch:  1300     LossContext: 0.00064810
    Epoch:  1325     LossContext: 0.00064805
    Epoch:  1350     LossContext: 0.00064800
    Epoch:  1375     LossContext: 0.00064795
    Epoch:  1400     LossContext: 0.00064790
    Epoch:  1425     LossContext: 0.00064785
    Epoch:  1450     LossContext: 0.00064780
    Epoch:  1475     LossContext: 0.00064776
    Epoch:  1499     LossContext: 0.00064772

Gradient descent adaptation time: 0 hours 0 mins 8 secs

Adapting to environment 3 ...
WARNING: You are demanding a shuffled dataset but did not provide any keys for that.
WARNING: No key provided, using time as seed
WARNING: batch_size must be between 0 and nb_trajs_per_env. Setting batch_size to maximum.
WARNING: No key provided for the context initialization. Initializing at 0.
WARNING: No key provided, using time as seed
    Epoch:     0     LossContext: 0.00266159
    Epoch:     1     LossContext: 0.00259798
    Epoch:     2     LossContext: 0.00253269
    Epoch:     3     LossContext: 0.00246591
    Epoch:    25     LossContext: 0.00120533
    Epoch:    50     LossContext: 0.00085553
    Epoch:    75     LossContext: 0.00079994
    Epoch:   100     LossContext: 0.00079451
    Epoch:   125     LossContext: 0.00079139
    Epoch:   150     LossContext: 0.00078851
    Epoch:   175     LossContext: 0.00078588
    Epoch:   200     LossContext: 0.00078372
    Epoch:   225     LossContext: 0.00078182
    Epoch:   250     LossContext: 0.00078000
    Epoch:   275     LossContext: 0.00077823
    Epoch:   300     LossContext: 0.00077653
    Epoch:   325     LossContext: 0.00077498
    Epoch:   350     LossContext: 0.00077361
    Epoch:   375     LossContext: 0.00077237
    Epoch:   400     LossContext: 0.00077115
    Epoch:   425     LossContext: 0.00076997
    Epoch:   450     LossContext: 0.00076881
    Epoch:   475     LossContext: 0.00076769
    Epoch:   500     LossContext: 0.00076658
    Epoch:   525     LossContext: 0.00076552
    Epoch:   550     LossContext: 0.00076453
    Epoch:   575     LossContext: 0.00076356
    Epoch:   600     LossContext: 0.00076265
    Epoch:   625     LossContext: 0.00076183
    Epoch:   650     LossContext: 0.00076108
    Epoch:   675     LossContext: 0.00076035
    Epoch:   700     LossContext: 0.00075964
    Epoch:   725     LossContext: 0.00075897
    Epoch:   750     LossContext: 0.00075836
    Epoch:   775     LossContext: 0.00075778
    Epoch:   800     LossContext: 0.00075722
    Epoch:   825     LossContext: 0.00075667
    Epoch:   850     LossContext: 0.00075614
    Epoch:   875     LossContext: 0.00075563
    Epoch:   900     LossContext: 0.00075513
    Epoch:   925     LossContext: 0.00075464
    Epoch:   950     LossContext: 0.00075417
    Epoch:   975     LossContext: 0.00075372
    Epoch:  1000     LossContext: 0.00075327
    Epoch:  1025     LossContext: 0.00075284
    Epoch:  1050     LossContext: 0.00075244
    Epoch:  1075     LossContext: 0.00075205
    Epoch:  1100     LossContext: 0.00075167
    Epoch:  1125     LossContext: 0.00075129
    Epoch:  1150     LossContext: 0.00075092
    Epoch:  1175     LossContext: 0.00075056
    Epoch:  1200     LossContext: 0.00075021
    Epoch:  1225     LossContext: 0.00074987
    Epoch:  1250     LossContext: 0.00074953
    Epoch:  1275     LossContext: 0.00074921
    Epoch:  1300     LossContext: 0.00074889
    Epoch:  1325     LossContext: 0.00074859
    Epoch:  1350     LossContext: 0.00074829
    Epoch:  1375     LossContext: 0.00074801
    Epoch:  1400     LossContext: 0.00074774
    Epoch:  1425     LossContext: 0.00074747
    Epoch:  1450     LossContext: 0.00074721
    Epoch:  1475     LossContext: 0.00074696
    Epoch:  1499     LossContext: 0.00074672

Gradient descent adaptation time: 0 hours 0 mins 8 secs
WARNING: No key provided for the context initialization. Initializing at 0.

Saving adaptation parameters into ./runs/04082024-161015/adapt/ folder with id 170846 ...

==  Begining out-of-distribution testing ... ==
    Number of training environments: 5
    Number of adaptation environments: 4
    Final length of the training trajectories: 10
    Length of the testing trajectories: 10
Test Score (OOD): 0.002774254

